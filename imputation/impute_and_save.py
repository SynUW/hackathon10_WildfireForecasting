"""
åŸºäºSAITSçš„æ¨¡å‹ï¼Œå¯¹æŒ‡å®šå¹´ä»½çš„ç¼ºå¤±å€¼è¿›è¡Œæ’è¡¥ï¼Œå¹¶ä¿å­˜åˆ°æ–°çš„H5æ–‡ä»¶ä¸­
"""
import os
import sys
import argparse
import re
import numpy as np
import h5py
import torch

# Ensure project root on path
ROOT = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
if ROOT not in sys.path:
    sys.path.append(ROOT)

from dataload_year import TimeSeriesDataLoader
from imputation.saits import SAITS


def build_model(d_time: int, d_feature: int, device: torch.device) -> SAITS:
    model = SAITS(
        n_groups=1,
        n_group_inner_layers=1,
        d_time=d_time,
        d_feature=d_feature,
        d_model=d_feature,
        d_inner=d_feature,
        n_head=1,
        d_k=d_feature,
        d_v=d_feature,
        dropout=0.1,
        input_with_mask=True,
        param_sharing_strategy="between_group",
        MIT=True,
        device=str(device),
        diagonal_attention_mask=True,
    )
    return model.to(device)


def parse_years(s: str):
    years = []
    for part in s.split(','):
        if '-' in part:
            a, b = part.split('-')
            years.extend(list(range(int(a), int(b) + 1)))
        else:
            years.append(int(part))
    return years


def _parse_dataset_name_to_row_col(name: str):
    """è§£ææ•°æ®é›†åç§°ä¸º (row, col)ã€‚
    å…¼å®¹ä»¥ä¸‹å¸¸è§æ ·å¼ï¼š
      - "row_col"
      - "YYYYMMDD_row_col"
      - å«å¤šä¸ªæ•°å­—çš„åç§°ï¼Œå–æœ€åä¸¤ä¸ªæ•°å­—ä½œä¸º rowã€col
    å¤±è´¥åˆ™è¿”å› (None, None)ã€‚
    """
    # ç›´æ¥åŒ¹é… row_col æˆ– row-col
    m = re.match(r"^(\d+)[_-](\d+)$", name)
    if m:
        return int(m.group(1)), int(m.group(2))
    # åŒ¹é… ..._row_col
    nums = re.findall(r"\d+", name)
    if len(nums) >= 2:
        try:
            return int(nums[-2]), int(nums[-1])
        except Exception:
            return None, None
    return None, None


def main():
    parser = argparse.ArgumentParser(description="Impute missing values and save to a new H5 file")
    parser.add_argument("--h5_dir", type=str, default="/mnt/raid/zhengsen/wildfire_dataset/self_built_materials/full_datasets",  # year_datasets_h5_masked_10x
                        help="åŸå§‹æŒ‰å¹´H5æ•°æ®æ ¹ç›®å½•ã€‚è‹¥æä¾› --in_h5 å°†å¿½ç•¥æ­¤å‚æ•°")
    parser.add_argument("--in_h5", type=str, default="/mnt/raid/zhengsen/wildfire_dataset/self_built_materials/full_datasets/cache/samples_5563b8eced09.h5",
                        help="å¯é€‰ï¼šç›´æ¥åŸºäºå·²ç¼“å­˜çš„cache.h5è¿›è¡Œæ’è¡¥ï¼ˆé”®å½¢å¦‚ YYYYMMDD_row_colï¼Œæ•°æ®å½¢çŠ¶[39,L]ï¼‰")
    parser.add_argument("--years", type=str, default="2000-2024")
    parser.add_argument("--device", type=str, default="cuda:1")
    parser.add_argument("--batch_size", type=int, default=512)
    parser.add_argument("--num_workers", type=int, default=16)
    parser.add_argument("--ckpt", type=str, default="./runs_imputation/saits_best-39.pt")
    parser.add_argument("--out_h5", type=str, default="/mnt/raid/zhengsen/wildfire_dataset/self_built_materials/full_datasets/cache/imputed_cache_real.h5")
    parser.add_argument("--seed", type=int, default=42)
    # New: directly impute per-year H5 files and save to a mirrored folder
    parser.add_argument("--impute-year-files", action="store_true",
                        help="å¯¹åŸå§‹æŒ‰å¹´H5æ–‡ä»¶é€åƒç´ æ’è¡¥å¹¶ä¿å­˜ä¸ºæ–°ç›®å½•ä¸‹çš„åŒåH5æ–‡ä»¶")
    parser.add_argument("--out_dir", type=str, default="/mnt/raid/zhengsen/wildfire_dataset/self_built_materials/full_datasets_imputed",
                        help="ä¿å­˜æ’è¡¥åæŒ‰å¹´H5æ–‡ä»¶çš„ç›®å½•ï¼ˆä¸åŸæ–‡ä»¶åŒåï¼‰ï¼Œå¯ç”¨ --impute-year-files æ—¶å¿…å¡«")
    args = parser.parse_args()

    torch.manual_seed(args.seed)
    np.random.seed(args.seed)
    device = torch.device(args.device if torch.cuda.is_available() else "cpu")

    from torch.utils.data import Dataset, Subset, DataLoader as TorchDataLoader

    def is_imputed_cache_h5(h5_path: str) -> bool:
        """æ£€æµ‹æ˜¯å¦ä¸ºé€æ ·æœ¬æ•°æ®å¼cacheï¼ˆæ¯ä¸ªé”®æ˜¯ä¸€ä¸ªæ ·æœ¬ï¼Œå½¢å¦‚ YYYYMMDD_row_colï¼Œæ•°æ®ä¸º[39,L]ï¼‰ã€‚
        è‹¥æ˜¯æ ·æœ¬ç´¢å¼•cacheï¼ˆå¦‚ samples_*.h5ï¼‰ï¼Œåˆ™è¿”å›Falseã€‚"""
        try:
            with h5py.File(h5_path, 'r') as f:
                # å¿«é€Ÿè§„åˆ™ï¼šå­˜åœ¨å¤§é‡å½¢å¦‚åŒ…å«ä¸‹åˆ’çº¿çš„dataseté”®ï¼Œå¹¶ä¸”ä»»å–ä¸€ä¸ªdatasetæ˜¯2Dæ•°ç»„
                keys = list(f.keys())
                if len(keys) == 0:
                    return False
                # å¦‚æœæ˜¯æ ·æœ¬ç´¢å¼•cacheï¼Œå¸¸è§é¡¶å±‚é”®å°‘ä¸”éæ ·æœ¬åï¼›è¿™é‡Œå–å‰è‹¥å¹²å°è¯•
                sample_like = 0
                for k in keys[:50]:
                    if not isinstance(k, str):
                        continue
                    if '_' in k and k.split('_')[0].isdigit():
                        obj = f[k]
                        if isinstance(obj, h5py.Dataset) and obj.ndim == 2:
                            sample_like += 1
                return sample_like > 0
        except Exception:
            return False

    use_in_h5 = bool(args.in_h5) and os.path.isfile(args.in_h5) and is_imputed_cache_h5(args.in_h5)

    # ===== Path 1: Impute whole per-year H5 files and save to new folder =====
    if args.impute_year_files:
        assert args.out_dir, "--out_dir å¿…é¡»æä¾›ï¼Œç”¨äºä¿å­˜æ’è¡¥åçš„H5æ–‡ä»¶"
        os.makedirs(args.out_dir, exist_ok=True)

        # Discover year files
        year_files = []
        for fn in os.listdir(args.h5_dir):
            if fn.endswith('_year_dataset.h5'):
                try:
                    y = int(fn.split('_')[0])
                    year_files.append((y, os.path.join(args.h5_dir, fn)))
                except Exception:
                    continue
        year_files.sort(key=lambda x: x[0])

        # Filter years
        years_keep = set(parse_years(args.years))
        year_files = [(y, p) for (y, p) in year_files if (not years_keep) or (y in years_keep)]
        if not year_files:
            print(f"[WARN] æœªæ‰¾åˆ°å¹´ä»½æ–‡ä»¶: {args.h5_dir}, years={args.years}")

        # Build models cache by time length
        model_cache = {}

        # Load checkpoint once (state_dict)
        state = None
        if os.path.isfile(args.ckpt):
            ckpt = torch.load(args.ckpt, map_location=device)
            state = ckpt.get("model_state_dict", ckpt)
            print(f"Loaded checkpoint: {args.ckpt}")
        else:
            print(f"[WARN] Checkpoint not found: {args.ckpt}. Proceeding with randomly initialized model.")

        def get_model_for_T(T: int, C: int = 39) -> SAITS:
            # å§‹ç»ˆä½¿ç”¨ä¸checkpointä¸€è‡´çš„æ—¶é—´é•¿åº¦ï¼ˆ365ï¼‰æ¥æ„å»ºæ¨¡å‹ï¼Œé¿å…æƒé‡shapeä¸åŒ¹é…
            T_model = 365 if T >= 365 else T
            if T_model not in model_cache:
                m = build_model(d_time=T_model, d_feature=C, device=device)
                if state is not None:
                    # ä»¥strict=TrueåŠ è½½ï¼Œä¿è¯ä¸365é…ç½®å®Œå…¨ä¸€è‡´
                    m.load_state_dict(state)
                m.eval()
                model_cache[T_model] = m
            return model_cache[T_model]

        # Process each year file
        for y, src_path in year_files:
            with h5py.File(src_path, 'r') as fin:
                # Infer time length and channels
                T = int(fin.attrs.get('total_time_steps', 365))
                C = int(fin.attrs.get('total_channels', 39))
                model = get_model_for_T(T, C)

                # Prepare output file
                dst_path = os.path.join(args.out_dir, os.path.basename(src_path))
                with h5py.File(dst_path, 'w') as fout:
                    # Copy file-level attrs
                    for k, v in fin.attrs.items():
                        fout.attrs[k] = v
                    fout.attrs['imputed_from'] = src_path
                    fout.attrs['impute_ckpt'] = args.ckpt

                    # Collect dataset names (pixels)
                    pixel_keys = []
                    for k in fin.keys():
                        if '_' not in k:
                            continue
                        obj = fin[k]
                        if isinstance(obj, h5py.Dataset) and obj.ndim == 2:
                            pixel_keys.append(k)
                    # Batch process for speed
                    batch = []
                    meta = []
                    def flush_batch():
                        if not batch:
                            return
                        X = np.stack(batch, axis=0)  # [B, C, T]
                        X = torch.from_numpy(X).to(device=device, dtype=torch.float32)
                        X_orig = X.permute(0, 2, 1).contiguous()  # [B, T, C]
                        # è§‚æµ‹æ©ç ï¼š1 è¡¨ç¤ºåŸå§‹æœ‰æ•ˆå€¼ï¼›æ’é™¤ NaN/Inf/255/-9999ï¼Œå¹¶æŒ‰é€šé“å¤„ç†0å€¼æœ‰æ•ˆæ€§
                        observed_base = (
                            torch.isfinite(X_orig)
                            & (X_orig != 255)
                            & (X_orig != -9999)
                        )
                        # é€šé“0..12çš„0è§†ä¸ºæœ‰æ•ˆï¼Œå…¶ä½™é€šé“çš„0è§†ä¸ºæ— æ•ˆ
                        ch_idx = torch.arange(X_orig.shape[2], device=X_orig.device)
                        zero_ok = (ch_idx <= 12).view(1, 1, -1)
                        nonzero = (X_orig >= 1e-6)
                        observed = (observed_base & (zero_ok | nonzero)).to(torch.float32)
                        # å†æ¬¡ä¸isfiniteå¯¹é½ï¼Œæœç»è§‚æµ‹ä½éæœ‰é™
                        observed = (observed * torch.isfinite(X_orig).to(torch.float32)).clamp(0.0, 1.0)
                        # ç”¨whereæ›¿æ¢ç¼ºå¤±å€¼ä¸º0ï¼Œé¿å… NaN*0 ä»ä¸º NaN
                        X_masked = torch.where(observed > 0.5, X_orig, torch.zeros_like(X_orig))
                        # é¢å¤–å…œåº•ï¼šå°†ä»»ä½•æ®‹ä½™çš„NaN/Infè½¬ä¸º0
                        X_masked = torch.nan_to_num(X_masked, nan=0.0, posinf=0.0, neginf=0.0)
                        if T == 366:
                            # ä½¿ç”¨365å¤©æ¨¡å‹æ’è¡¥å‰365å¤©
                            X_orig_365 = X_masked[:, :365, :]
                            observed_365 = observed[:, :365, :]
                            inputs = {
                                "X": X_orig_365,
                                "missing_mask": observed_365,
                                "X_holdout": X_orig[:, :365, :],
                                "indicating_mask": torch.zeros_like(X_orig_365),
                            }
                            with torch.no_grad():
                                out = model(inputs, stage="test")
                                X_c_365 = out["imputed_data"]  # [B, 365, C]
                                X_c_365 = torch.nan_to_num(X_c_365, nan=0.0, posinf=0.0, neginf=0.0)
                                # ä¸¥æ ¼æ›¿æ¢ç¼ºå¤±ä½ç½®ä¸ºæ¨¡å‹è¾“å‡º
                                X_filled_365 = torch.where(observed_365 > 0.5, X_orig_365, X_c_365)
                            # å¤„ç†ç¬¬366å¤©ï¼šè‹¥æ— æ•ˆåˆ™ç”¨ç¬¬365å¤©çš„æ’å€¼ï¼ˆç›´æ¥æ‹·è´å‰ä¸€å¤©çš„å¡«å……å€¼ï¼‰ï¼Œå¦åˆ™ä¿ç•™åŸå€¼
                            obs366 = observed[:, 365, :]  # [B, C]
                            x366_orig = X_orig[:, 365, :]  # [B, C]
                            x365_final = X_filled_365[:, 364, :]  # [B, C]
                            x366_final = torch.where(obs366 > 0.5, x366_orig, x365_final)
                            X_filled = torch.cat([X_filled_365, x366_final.unsqueeze(1)], dim=1)  # [B, 366, C]
                        else:
                            inputs = {
                                "X": X_masked,
                                "missing_mask": observed,
                                "X_holdout": X_orig,
                                "indicating_mask": torch.zeros_like(X_orig),
                            }
                            with torch.no_grad():
                                out = model(inputs, stage="test")
                                X_c = out["imputed_data"]  # [B, T, C]
                                X_c = torch.nan_to_num(X_c, nan=0.0, posinf=0.0, neginf=0.0)
                                # ä¸¥æ ¼æ›¿æ¢ç¼ºå¤±ä½ç½®ä¸ºæ¨¡å‹è¾“å‡º
                                X_filled = torch.where(observed > 0.5, X_orig, X_c)

                        X_filled = torch.nan_to_num(X_filled, nan=0.0, posinf=0.0, neginf=0.0)
                        X_filled = X_filled.permute(0, 2, 1).contiguous().detach().cpu().numpy()  # [B, C, T]
                        # Write each datasetï¼›åç§°é‡‡ç”¨"row_col"ï¼Œè§£æå¤±è´¥åˆ™å›é€€åŸå
                        for i, rc in enumerate(meta):
                            row, col = rc
                            ds_name = None
                            if row is not None and col is not None:
                                ds_name = f"{row}_{col}"
                            else:
                                ds_name = pixel_keys[i] if i < len(pixel_keys) else f"pix_{i}"
                            if ds_name in fout:
                                del fout[ds_name]
                            fout.create_dataset(ds_name, data=X_filled[i], compression="gzip", compression_opts=4, shuffle=True)
                        batch.clear()
                        meta.clear()

                    for name in pixel_keys:
                        arr = fin[name][()]
                        # Ensure shape [C,T]
                        if arr.ndim != 2:
                            continue
                        if arr.shape[0] != C:
                            # transpose if stored [T,C]
                            if arr.shape[1] == C:
                                arr = arr.T
                            else:
                                continue
                        # Append to batchï¼›metaå­˜(row,col)
                        batch.append(arr.astype(np.float32))
                        meta.append(_parse_dataset_name_to_row_col(name))
                        if len(batch) >= args.batch_size:
                            flush_batch()
                    flush_batch()

            print(f"âœ… Year {y} imputed -> {dst_path}")

        print("ğŸ‰ All year files imputed and saved.")
        return

    # ===== Path 2/3: Original sampler-based or cache-based per-sample imputation =====
    if not use_in_h5:
        # åŸºäºåŸå§‹å¹´åº¦æ•°æ®æ„å»ºåŠ è½½å™¨
        ts_loader = TimeSeriesDataLoader(
            h5_dir=args.h5_dir,
            positive_ratio=1.0,
            pos_neg_ratio=999999,
            resample_each_epoch=False,
            lookback_seq=365,
            forecast_hor=7,
            verbose_sampling=False,
            enable_performance_optimizations=True,
        )

        # ç›®å‰å†…éƒ¨æ•°æ®é›†å·²æŒ‰å¹´ä»½åŠ è½½ï¼›æ­¤å¤„ç›´æ¥å–å…¨éƒ¨ç´¢å¼•
        indices = list(range(len(ts_loader.dataset)))
        subset = Subset(ts_loader.dataset, indices)
        loader = TorchDataLoader(
            subset,
            batch_size=args.batch_size,
            shuffle=False,
            num_workers=args.num_workers,
            pin_memory=True,
            persistent_workers=True,
            prefetch_factor=4,
            collate_fn=ts_loader.dataset.custom_collate_fn,
        )
    else:
        # åŸºäºå·²æœ‰çš„cache.h5æ„å»ºæ•°æ®é›†ï¼ˆé”®: YYYYMMDD_row_colï¼Œæ•°æ®: [39, L]ï¼‰
        if bool(args.in_h5) and os.path.isfile(args.in_h5) and not use_in_h5:
            print(f"[INFO] Detected non-imputed cache file '{args.in_h5}'. Falling back to original loader via --h5_dir.\n"
                  f"       æç¤ºï¼š--in_h5 ä»…æ”¯æŒé€æ ·æœ¬æ•°æ®å¼cacheï¼ˆæ¯ä¸ªæ ·æœ¬ä¸ºä¸€ä¸ªdatasetï¼Œå½¢å¦‚ YYYYMMDD_row_colï¼‰ã€‚")
            # å›é€€åˆ°åŸå§‹æ•°æ®åŠ è½½è·¯å¾„
            ts_loader = TimeSeriesDataLoader(
                h5_dir=args.h5_dir,
                positive_ratio=1.0,
                pos_neg_ratio=999999,
                resample_each_epoch=False,
                lookback_seq=365,
                forecast_hor=7,
                verbose_sampling=False,
                enable_performance_optimizations=True,
            )
            indices = list(range(len(ts_loader.dataset)))
            subset = Subset(ts_loader.dataset, indices)
            loader = TorchDataLoader(
                subset,
                batch_size=args.batch_size,
                shuffle=False,
                num_workers=args.num_workers,
                pin_memory=True,
                persistent_workers=True,
                prefetch_factor=4,
                collate_fn=ts_loader.dataset.custom_collate_fn,
            )
        else:
            years_set = set(parse_years(args.years))

        class H5ImputeDataset(Dataset):
            def __init__(self, h5_path: str, years_keep: set[int]):
                self.h5_path = h5_path
                with h5py.File(self.h5_path, 'r') as f:
                    self.keys = [k for k in f.keys() if isinstance(f[k], h5py.Dataset) and f[k].ndim == 2]

            def __len__(self):
                return len(self.keys)

            def __getitem__(self, idx):
                key = self.keys[idx]
                with h5py.File(self.h5_path, 'r') as f:
                    arr = f[key][()]
                if arr.ndim != 2:
                    raise RuntimeError("dataset should be 2-D")
                # è½¬ä¸º [C, L] çš„float32å¼ é‡
                if arr.shape[0] <= arr.shape[1]:
                    arr2 = arr
                else:
                    arr2 = arr.T
                x = torch.from_numpy(arr2).to(torch.float32)
                # è§£æ(row,col)
                row, col = _parse_dataset_name_to_row_col(key)
                meta = (row, col)
                return x, None, meta

        def h5_collate(batch):
            # batch: List[Tuple[x:[C,L] tensor, None, meta_tuple]] -> ([B,C,L], None, List[meta])
            xs = []
            metas = []
            for x, _, meta in batch:
                xs.append(x)
                metas.append(meta)
            xs = torch.stack(xs, dim=0)
            return xs, None, metas

            ds = H5ImputeDataset(args.in_h5, years_set)
            loader = TorchDataLoader(
                ds,
                batch_size=args.batch_size,
                shuffle=False,
                num_workers=args.num_workers,
                pin_memory=True,
                persistent_workers=(args.num_workers > 0),
                prefetch_factor=(4 if args.num_workers > 0 else None),
                collate_fn=h5_collate,
            )

    # Peek batch to infer shapes
    first_batch = next(iter(loader))
    past_batch, _, metadata_list = first_batch
    B0, C0, L0 = past_batch.shape  # [B, 39, L]
    # å§‹ç»ˆä½¿ç”¨365é•¿åº¦æ¨¡å‹ï¼ˆè‹¥L0>=365ï¼‰ï¼Œé¿å…ä¸checkpointä¸ä¸€è‡´
    d_time_model = 365 if L0 >= 365 else L0
    model = build_model(d_time=d_time_model, d_feature=C0, device=device)
    # Load checkpoint
    if not os.path.isfile(args.ckpt):
        print(f"[WARN] Checkpoint not found: {args.ckpt}. Proceeding with randomly initialized model.")
    else:
        ckpt = torch.load(args.ckpt, map_location=device)
        state = ckpt.get("model_state_dict", ckpt)
        model.load_state_dict(state)
        print(f"Loaded checkpoint: {args.ckpt}")

    # Prepare output H5
    os.makedirs(os.path.dirname(args.out_h5), exist_ok=True)
    h5f = h5py.File(args.out_h5, 'w')
    h5f.attrs['source_h5_dir'] = (args.in_h5 if use_in_h5 else args.h5_dir)
    h5f.attrs['years'] = args.years
    h5f.attrs['lookback_seq'] = 365
    h5f.attrs['note'] = 'Per-sample imputed past windows; dataset key = f"{date_int}_{row}_{col}"; data shape [39, L]'

    with torch.no_grad():
        sample_count = 0
        for batch in loader:
            # å…¼å®¹ä¸¤ç§æ•°æ®æºçš„batchç»“æ„
            if use_in_h5:
                past_batch, _, metadata_list = batch  # é»˜è®¤collateï¼špast_batch:[B,C,L], metadata: list[tuple]
            else:
                past_batch, _, metadata_list = batch

            X_orig = past_batch.to(device, non_blocking=True).permute(0, 2, 1).contiguous()  # [B,L,N]
            # print(torch.isfinite(X_orig))
            # print(torch.max(X_orig), torch.min(X_orig))
            # Build masks with zero handling: ch0..12: 0 is valid; others: 0 is invalid
            observed_base = (
                torch.isfinite(X_orig)
                & (X_orig != 255)
                & (X_orig != -9999)
            )
            ch_idx = torch.arange(X_orig.shape[2], device=X_orig.device)
            zero_ok = (ch_idx <= 12).view(1, 1, -1)
            nonzero = (X_orig >= 1e-6)
            observed = (observed_base & (zero_ok | nonzero)).to(torch.float32)
            observed = (observed * torch.isfinite(X_orig).to(torch.float32)).clamp(0.0, 1.0)
            # Mask original missing positions to zero before feeding model
            X_masked = torch.where(observed > 0.5, X_orig, torch.zeros_like(X_orig))
            X_masked = torch.nan_to_num(X_masked, nan=0.0, posinf=0.0, neginf=0.0)
            missing_mask = observed  # no artificial mask here
            indicating = torch.zeros_like(X_orig)
            inputs = {
                "X": X_masked,
                "missing_mask": missing_mask,
                "X_holdout": X_orig,
                "indicating_mask": indicating,
            }
            out = model(inputs, stage="test")
            X_c = out["imputed_data"]  # [B, L_used, N]
            X_c = torch.nan_to_num(X_c, nan=0.0, posinf=0.0, neginf=0.0)
            if L0 == 366 and d_time_model == 365:
                # å…ˆå¯¹å‰365å¤©è¿›è¡Œæ’è¡¥
                X_orig_365 = X_orig[:, :365, :]
                miss_365 = missing_mask[:, :365, :]
                X_c_365 = X_c[:, :365, :]
                X_c_365 = torch.nan_to_num(X_c_365, nan=0.0, posinf=0.0, neginf=0.0)
                X_filled_365 = torch.where(miss_365 > 0.5, X_orig_365, X_c_365)
                # ç¬¬366å¤©ï¼šç¼ºå¤±åˆ™ç”¨ç¬¬365å¤©çš„æ’è¡¥å€¼ï¼Œå¦åˆ™ä¿ç•™åŸå€¼
                obs366 = missing_mask[:, 365, :]
                x366_orig = X_orig[:, 365, :]
                x365_final = X_filled_365[:, 364, :]
                x366_final = torch.where(obs366 > 0.5, x366_orig, x365_final)
                X_filled = torch.cat([X_filled_365, x366_final.unsqueeze(1)], dim=1)
            else:
                # Replace only missing positions (excluding chan0 zeros)
                X_filled = torch.where(missing_mask > 0.5, X_orig, X_c)
            X_filled = torch.nan_to_num(X_filled, nan=0.0, posinf=0.0, neginf=0.0)
            # Save each sampleï¼Œè¾“å‡ºåç§°é‡‡ç”¨"row_col"
            for b in range(X_filled.shape[0]):
                row, col = metadata_list[b]
                if row is None or col is None:
                    key = f"sample_{b}"
                else:
                    key = f"{int(row)}_{int(col)}"
                data_np = X_filled[b].detach().cpu().numpy().T  # [N,L] -> save as [C,L]
                if key in h5f:
                    del h5f[key]
                h5f.create_dataset(key, data=data_np, compression="gzip")
                sample_count += 1
            if sample_count % 1000 == 0:
                print(f"Saved {sample_count} samples...")

    h5f.close()
    print(f"Done. Saved {sample_count} imputed samples to {args.out_h5}")


if __name__ == "__main__":
    main()

