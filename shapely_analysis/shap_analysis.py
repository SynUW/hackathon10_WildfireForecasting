#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
SHAPåˆ†æå·¥å…· - ç”Ÿæˆç±»ä¼¼SHAPæ‘˜è¦å›¾çš„å¯è§†åŒ–
"""

import torch
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Dict, List, Tuple, Optional
import os
import sys
from datetime import datetime
from tqdm import tqdm

# æ·»åŠ æ¨¡å‹è·¯å¾„
sys.path.insert(0, 'model_zoo')

class SHAPAnalyzer:
    """
    SHAPé£æ ¼çš„åˆ†æå™¨
    é€šè¿‡è®¡ç®—æ¯ä¸ªæ ·æœ¬ä¸­æ¯ä¸ªç‰¹å¾çš„è´¡çŒ®æ¥ç”Ÿæˆç±»ä¼¼SHAPæ‘˜è¦å›¾çš„å¯è§†åŒ–
    """
    
    def __init__(self, model_path: str, model_name: str, seq_len: int, pred_len: int, 
                 input_channels: int = 39, device: str = 'cuda'):
        """
        åˆå§‹åŒ–SHAPåˆ†æå™¨
        """
        self.model_path = model_path
        self.model_name = model_name
        self.seq_len = seq_len
        self.pred_len = pred_len
        self.input_channels = input_channels
        self.device = torch.device(device if torch.cuda.is_available() else 'cpu')
        
        # åŠ è½½æ¨¡å‹
        self.model = self._load_model()
        self.model.eval()
        
        # é©±åŠ¨å› ç´ åç§°ï¼ˆæ ¹æ®å®é™…æ•°æ®é¡ºåºï¼‰
        self.driver_names = [
            'Firms_Fire', 'Temperatu', 'U_Wind_10m', 'V_Wind_10m', 'Snow_Cover',
            'Total_Percipitation', 'Surface_Latent_heat', 'Downpoint', 'Surface_Pressure', 'Volumetric_soil_moisture1',
            'Volumetric_soil_moisture2', 'Volumetric_soil_moisture3', 'Volumetric_soil_moisture4', 'Aspect', 'DEM',
            'Hillshade', 'Slope', 'D2Infrastre', 'D2Water', 'LULC',
            'NDVI', 'EVI', 'Band1', 'Band2', 'Band3', 'Band7',
            'Band20_D', 'Band21_D', 'Band20_N', 'Band21_N', 'LST_Day',
            'Band29_D', 'Band31_D', 'Band32_D', 'LST_N', 'Band29_N',
            'Band31_N', 'Band32_N', 'LAI'
        ]
        
        # ç¡®ä¿é©±åŠ¨å› ç´ åç§°æ•°é‡åŒ¹é…
        if len(self.driver_names) < input_channels:
            self.driver_names.extend([f'Driver_{i}' for i in range(len(self.driver_names), input_channels)])
        elif len(self.driver_names) > input_channels:
            self.driver_names = self.driver_names[:input_channels]
    
    def _load_model(self) -> torch.nn.Module:
        """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹"""
        try:
            # åˆ›å»ºé…ç½®
            class Config:
                def __init__(self, model_name, seq_len, pred_len, input_channels):
                    self.task_name = 'long_term_forecast'
                    self.seq_len = seq_len
                    self.label_len = 0
                    self.pred_len = pred_len
                    self.enc_in = input_channels
                    self.dec_in = input_channels
                    self.c_out = 39
                    
                    # æ ¹æ®æ¨¡å‹åç§°è°ƒæ•´é…ç½®å‚æ•°
                    if model_name == 's_mamba':
                        self.d_model = 1024
                        self.n_heads = 16
                        self.e_layers = 2
                        self.d_layers = 1
                        self.d_ff = 2048  # ä¿®å¤ï¼šä½¿ç”¨2048è€Œä¸æ˜¯4096
                    else:
                        self.d_model = 512
                        self.n_heads = 8
                        self.e_layers = 2
                        self.d_layers = 1
                        self.d_ff = 2048
                    
                    self.dropout = 0.1
                    self.activation = 'gelu'
                    self.output_attention = False
                    self.factor = 5
                    self.moving_avg = 25
                    self.individual = False
                    self.features = 'M'
                    self.patch_len = 16
                    self.alpha = 0.5
                    self.beta = 0.5
                    self.top_k = 5
                    self.num_kernels = 6
                    self.d_state = 16
                    self.d_conv = 4
                    self.expand = 2
                    self.use_norm = True
                    self.distil = True
                    self.embed = 'timeF'
                    self.freq = 'd'
            
            config = Config(self.model_name, self.seq_len, self.pred_len, self.input_channels)
            
            # åŠ¨æ€å¯¼å…¥æ¨¡å‹
            module = __import__(f'model_zoo.{self.model_name}', fromlist=['Model'])
            Model = getattr(module, 'Model')
            
            # åˆ›å»ºæ¨¡å‹å®ä¾‹
            model = Model(config)
            
            # åŠ è½½æƒé‡
            checkpoint = torch.load(self.model_path, map_location=self.device)
            if 'model_state_dict' in checkpoint:
                model.load_state_dict(checkpoint['model_state_dict'])
            else:
                model.load_state_dict(checkpoint)
            
            model = model.to(self.device)
            print(f"âœ… æˆåŠŸåŠ è½½æ¨¡å‹: {self.model_name}")
            return model
            
        except Exception as e:
            print(f"âŒ åŠ è½½æ¨¡å‹å¤±è´¥: {e}")
            raise
    
    def generate_sample_data(self, num_samples: int = 100) -> Tuple[torch.Tensor, torch.Tensor]:
        """ç”Ÿæˆæ ·æœ¬æ•°æ®ç”¨äºSHAPåˆ†æï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰"""
        print(f"ğŸ“Š ç”Ÿæˆ{num_samples}ä¸ªæ ·æœ¬æ•°æ®...")
        
        # ä½¿ç”¨æ›´å¤§çš„æ‰¹é‡ç”Ÿæˆæ•°æ®
        batch_size = 128
        all_input_data = []
        all_predictions = []
        
        with tqdm(total=num_samples, desc="ç”Ÿæˆæ ·æœ¬æ•°æ®") as pbar:
            for i in range(0, num_samples, batch_size):
                current_batch_size = min(batch_size, num_samples - i)
                
                # ç”Ÿæˆéšæœºè¾“å…¥æ•°æ®
                batch_data = torch.randn(current_batch_size, self.seq_len, self.input_channels, device=self.device)
                
                # å½’ä¸€åŒ–åˆ°0-1èŒƒå›´ï¼ˆæ¨¡æ‹ŸçœŸå®æ•°æ®ï¼‰
                batch_data = torch.sigmoid(batch_data)
                
                # ç”Ÿæˆå¯¹åº”çš„é¢„æµ‹ç»“æœ
                with torch.no_grad():
                    batch_predictions = self.model(batch_data, None, None, None)
                
                all_input_data.append(batch_data)
                all_predictions.append(batch_predictions)
                
                pbar.update(current_batch_size)
        
        # åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡
        input_data = torch.cat(all_input_data, dim=0)
        predictions = torch.cat(all_predictions, dim=0)
        
        return input_data, predictions
    
    def calculate_shap_values(self, input_data: torch.Tensor, 
                            num_background: int = 50) -> Tuple[np.ndarray, np.ndarray]:
        """
        è®¡ç®—SHAPå€¼ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼Œä½¿ç”¨å‘é‡åŒ–å’Œå¹¶è¡ŒåŒ–ï¼‰
        
        Args:
            input_data: è¾“å…¥æ•°æ® [num_samples, seq_len, channels]
            num_background: èƒŒæ™¯æ ·æœ¬æ•°é‡
            
        Returns:
            shap_values: SHAPå€¼ [num_samples, seq_len, channels]
            feature_values: ç‰¹å¾å€¼ [num_samples, seq_len, channels]
        """
        print("ğŸ” è®¡ç®—SHAPå€¼ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰...")
        
        num_samples = input_data.shape[0]
        device = input_data.device
        
        # è®¡ç®—èƒŒæ™¯å¹³å‡å€¼
        background_samples = input_data[:num_background]
        background_mean = background_samples.mean(dim=0, keepdim=True)  # [1, seq_len, channels]
        
        # è®¡ç®—åŸºçº¿é¢„æµ‹
        with torch.no_grad():
            baseline_pred = self.model(background_samples, None, None, None).mean(dim=0, keepdim=True)
        
        # ä½¿ç”¨æ›´é«˜æ•ˆçš„SHAPè®¡ç®—æ–¹æ³•
        shap_values = self._calculate_shap_values_vectorized(
            input_data, background_mean, baseline_pred, num_samples
        )
        
        return shap_values, input_data.cpu().numpy()
    
    def _calculate_shap_values_vectorized(self, input_data: torch.Tensor, 
                                        background_mean: torch.Tensor,
                                        baseline_pred: torch.Tensor,
                                        num_samples: int) -> np.ndarray:
        """å‘é‡åŒ–è®¡ç®—SHAPå€¼"""
        
        # æ ¹æ®æ ·æœ¬æ•°é‡é€‰æ‹©æœ€ä¼˜æ–¹æ³•
        if num_samples <= 50:
            # å°æ ·æœ¬ï¼šä½¿ç”¨æ¢¯åº¦è¿‘ä¼¼ï¼ˆæœ€å¿«ï¼‰
            print("ğŸš€ ä½¿ç”¨æ¢¯åº¦è¿‘ä¼¼æ–¹æ³•ï¼ˆæœ€å¿«ï¼‰")
            shap_values = self._calculate_shap_gradient_approximation(
                input_data, background_mean, baseline_pred
            )
        elif num_samples <= 500:
            # ä¸­ç­‰æ ·æœ¬ï¼šä½¿ç”¨å¿«é€Ÿæ‰°åŠ¨æ–¹æ³•
            print("âš¡ ä½¿ç”¨å¿«é€Ÿæ‰°åŠ¨æ–¹æ³•ï¼ˆä¸­ç­‰é€Ÿåº¦ï¼‰")
            shap_values = self._calculate_shap_fast_perturbation(
                input_data, background_mean, baseline_pred, num_samples
            )
        else:
            # å¤§æ ·æœ¬ï¼šä½¿ç”¨æ‰¹é‡æ‰°åŠ¨ï¼ˆæœ€ç¨³å®šï¼‰
            print("ğŸ“¦ ä½¿ç”¨æ‰¹é‡æ‰°åŠ¨æ–¹æ³•ï¼ˆæœ€ç¨³å®šï¼‰")
            shap_values = self._calculate_shap_batch_perturbation(
                input_data, background_mean, baseline_pred, num_samples
            )
        
        return shap_values
    
    def _calculate_shap_fast_perturbation(self, input_data: torch.Tensor,
                                        background_mean: torch.Tensor,
                                        baseline_pred: torch.Tensor,
                                        num_samples: int) -> np.ndarray:
        """å¿«é€Ÿæ‰°åŠ¨è®¡ç®—SHAPå€¼ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰"""
        
        # ä½¿ç”¨æ›´å¤§çš„æ‰¹é‡å¤§å°
        batch_size = 128
        shap_values = np.zeros((num_samples, self.seq_len, self.input_channels))
        
        # è®¡ç®—æ€»æ‰¹æ¬¡æ•°
        total_batches = (num_samples + batch_size - 1) // batch_size
        
        print(f"âš¡ å¿«é€Ÿæ‰°åŠ¨æ–¹æ³•: {total_batches}ä¸ªæ‰¹æ¬¡")
        
        with tqdm(total=total_batches, desc="å¿«é€ŸSHAPè®¡ç®—") as pbar:
            for i in range(0, num_samples, batch_size):
                end_idx = min(i + batch_size, num_samples)
                batch_data = input_data[i:end_idx]
                batch_size_actual = batch_data.shape[0]
                
                # ä½¿ç”¨æ›´é«˜æ•ˆçš„æ‰°åŠ¨ç­–ç•¥
                shap_batch = self._compute_fast_shap_batch(
                    batch_data, background_mean, baseline_pred, batch_size_actual
                )
                
                shap_values[i:i+batch_size_actual] = shap_batch
                pbar.update(1)
        
        return shap_values
    
    def _compute_fast_shap_batch(self, batch_data: torch.Tensor,
                                background_mean: torch.Tensor,
                                baseline_pred: torch.Tensor,
                                batch_size: int) -> np.ndarray:
        """å¿«é€Ÿè®¡ç®—å•ä¸ªæ‰¹æ¬¡çš„SHAPå€¼"""
        
        shap_batch = np.zeros((batch_size, self.seq_len, self.input_channels))
        
        # åŸå§‹é¢„æµ‹
        with torch.no_grad():
            original_preds = self.model(batch_data, None, None, None)
        
        # å¯¹æ¯ä¸ªç‰¹å¾ä½ç½®è¿›è¡Œå¿«é€Ÿæ‰°åŠ¨
        for t in range(self.seq_len):
            for c in range(self.input_channels):
                # åˆ›å»ºæ‰°åŠ¨ç‰ˆæœ¬ï¼ˆåªæ‰°åŠ¨ä¸€ä¸ªç‰¹å¾ï¼‰
                perturbed = batch_data.clone()
                perturbed[:, t, c] = background_mean[0, t, c]
                
                # è®¡ç®—æ‰°åŠ¨é¢„æµ‹
                with torch.no_grad():
                    perturbed_preds = self.model(perturbed, None, None, None)
                
                # è®¡ç®—SHAPå€¼
                pred_diff = original_preds[:, 0, 0] - perturbed_preds[:, 0, 0]
                feature_diff = batch_data[:, t, c] - background_mean[0, t, c]
                
                shap_batch[:, t, c] = (feature_diff * pred_diff).cpu().numpy()
        
        return shap_batch
    
    def _smart_sample_indices(self, data: np.ndarray, max_points: int) -> np.ndarray:
        """æ™ºèƒ½é‡‡æ ·ç´¢å¼•ï¼Œä¿æŒæ•°æ®åˆ†å¸ƒ"""
        
        if len(data) <= max_points:
            return np.arange(len(data))
        
        # è®¡ç®—åˆ†ä½æ•°
        quantiles = np.percentile(data, np.linspace(0, 100, 10))
        
        # åœ¨æ¯ä¸ªåˆ†ä½æ•°åŒºé—´å†…é‡‡æ ·
        sampled_indices = []
        points_per_quantile = max_points // 10
        
        for i in range(len(quantiles) - 1):
            mask = (data >= quantiles[i]) & (data < quantiles[i + 1])
            if i == len(quantiles) - 2:  # æœ€åä¸€ä¸ªåŒºé—´åŒ…å«è¾¹ç•Œ
                mask = (data >= quantiles[i]) & (data <= quantiles[i + 1])
            
            quantile_indices = np.where(mask)[0]
            if len(quantile_indices) > 0:
                if len(quantile_indices) <= points_per_quantile:
                    sampled_indices.extend(quantile_indices)
                else:
                    # éšæœºé‡‡æ ·
                    sampled_indices.extend(
                        np.random.choice(quantile_indices, points_per_quantile, replace=False)
                    )
        
        return np.array(sampled_indices)
    
    def _calculate_shap_gradient_approximation(self, input_data: torch.Tensor,
                                             background_mean: torch.Tensor,
                                             baseline_pred: torch.Tensor) -> np.ndarray:
        """ä½¿ç”¨æ¢¯åº¦è¿‘ä¼¼è®¡ç®—SHAPå€¼ï¼ˆæœ€å¿«æ–¹æ³•ï¼‰"""
        
        input_data.requires_grad_(True)
        
        # å‰å‘ä¼ æ’­
        with torch.enable_grad():
            predictions = self.model(input_data, None, None, None)
            loss = torch.mean((predictions - baseline_pred) ** 2)
        
        # è®¡ç®—æ¢¯åº¦
        gradients = torch.autograd.grad(loss, input_data, create_graph=False)[0]
        
        # SHAPå€¼è¿‘ä¼¼ = ç‰¹å¾å€¼ * æ¢¯åº¦
        shap_values = (input_data - background_mean) * gradients
        
        return shap_values.detach().cpu().numpy()
    
    def _calculate_shap_batch_perturbation(self, input_data: torch.Tensor,
                                         background_mean: torch.Tensor,
                                         baseline_pred: torch.Tensor,
                                         num_samples: int) -> np.ndarray:
        """æ‰¹é‡æ‰°åŠ¨è®¡ç®—SHAPå€¼ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰"""
        
        batch_size = 64  # å¢å¤§æ‰¹é‡å¤§å°
        shap_values = np.zeros((num_samples, self.seq_len, self.input_channels))
        
        # è®¡ç®—æ€»æ‰¹æ¬¡æ•°
        total_batches = (num_samples + batch_size - 1) // batch_size
        total_features = self.seq_len * self.input_channels
        
        print(f"ğŸ“Š ä½¿ç”¨æ‰¹é‡æ‰°åŠ¨æ–¹æ³•: {total_batches}ä¸ªæ‰¹æ¬¡, {total_features}ä¸ªç‰¹å¾ä½ç½®")
        
        # ä½¿ç”¨è¿›åº¦æ¡
        with tqdm(total=total_batches, desc="è®¡ç®—SHAPå€¼") as pbar:
            for i in range(0, num_samples, batch_size):
                end_idx = min(i + batch_size, num_samples)
                batch_data = input_data[i:end_idx]
                batch_size_actual = batch_data.shape[0]
                
                # å‘é‡åŒ–åˆ›å»ºæ‰°åŠ¨æ•°æ®
                perturbed_data = self._create_perturbed_batch_vectorized(
                    batch_data, background_mean, batch_size_actual
                )
                
                # æ‰¹é‡é¢„æµ‹
                with torch.no_grad():
                    # åŸå§‹é¢„æµ‹
                    original_preds = self.model(batch_data, None, None, None)
                    
                    # æ‰°åŠ¨é¢„æµ‹
                    perturbed_preds = self.model(perturbed_data, None, None, None)
                
                # å‘é‡åŒ–è®¡ç®—SHAPå€¼
                self._compute_shap_values_vectorized(
                    shap_values, batch_data, background_mean, 
                    original_preds, perturbed_preds, 
                    i, batch_size_actual
                )
                
                pbar.update(1)
        
        return shap_values
    
    def _create_perturbed_batch_vectorized(self, batch_data: torch.Tensor,
                                         background_mean: torch.Tensor,
                                         batch_size: int) -> torch.Tensor:
        """å‘é‡åŒ–åˆ›å»ºæ‰°åŠ¨æ‰¹æ¬¡æ•°æ®"""
        
        # é¢„åˆ†é…å†…å­˜
        total_perturbations = self.seq_len * self.input_channels
        perturbed_batch = torch.zeros(
            batch_size * total_perturbations, 
            self.seq_len, 
            self.input_channels, 
            device=batch_data.device
        )
        
        # å¤åˆ¶åŸå§‹æ•°æ®åˆ°æ‰€æœ‰æ‰°åŠ¨ä½ç½®
        for i in range(total_perturbations):
            start_idx = i * batch_size
            end_idx = start_idx + batch_size
            perturbed_batch[start_idx:end_idx] = batch_data
        
        # å‘é‡åŒ–æ›¿æ¢ç‰¹å¾å€¼
        idx = 0
        for t in range(self.seq_len):
            for c in range(self.input_channels):
                start_idx = idx * batch_size
                end_idx = start_idx + batch_size
                perturbed_batch[start_idx:end_idx, t, c] = background_mean[0, t, c]
                idx += 1
        
        return perturbed_batch
    
    def _compute_shap_values_vectorized(self, shap_values: np.ndarray,
                                      batch_data: torch.Tensor,
                                      background_mean: torch.Tensor,
                                      original_preds: torch.Tensor,
                                      perturbed_preds: torch.Tensor,
                                      batch_start: int,
                                      batch_size: int):
        """å‘é‡åŒ–è®¡ç®—SHAPå€¼"""
        
        idx = 0
        for t in range(self.seq_len):
            for c in range(self.input_channels):
                # è®¡ç®—é¢„æµ‹å·®å¼‚
                pred_diff = (original_preds[:, 0, 0] - 
                           perturbed_preds[idx:idx+batch_size, 0, 0])
                
                # è®¡ç®—ç‰¹å¾å·®å¼‚
                feature_diff = batch_data[:, t, c] - background_mean[0, t, c]
                
                # SHAPå€¼
                shap_values[batch_start:batch_start+batch_size, t, c] = \
                    (feature_diff * pred_diff).cpu().numpy()
                
                idx += batch_size
    
    def create_shap_summary_plot(self, shap_values: np.ndarray, feature_values: np.ndarray,
                               save_path: str = None):
        """åˆ›å»ºSHAPæ‘˜è¦å›¾"""
        print("ğŸ“Š åˆ›å»ºSHAPæ‘˜è¦å›¾...")
        
        # è®¡ç®—æ¯ä¸ªç‰¹å¾çš„å¹³å‡ç»å¯¹SHAPå€¼
        feature_importance = np.mean(np.abs(shap_values), axis=(0, 1))  # [channels]
        
        # é€‰æ‹©æœ€é‡è¦çš„ç‰¹å¾ï¼ˆTop 20ï¼‰
        top_k = min(20, self.input_channels)
        top_indices = np.argsort(feature_importance)[-top_k:][::-1]
        top_features = [self.driver_names[i] for i in top_indices]
        
        # å‡†å¤‡æ•°æ®ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰
        plot_data = []
        max_points_per_feature = 1000  # é™åˆ¶æ¯ä¸ªç‰¹å¾çš„ç‚¹æ•°
        
        print(f"ğŸ“ˆ å¤„ç†{top_k}ä¸ªé‡è¦ç‰¹å¾çš„å¯è§†åŒ–æ•°æ®...")
        
        for i, feat_idx in enumerate(tqdm(top_indices, desc="å¤„ç†ç‰¹å¾æ•°æ®")):
            # è·å–è¯¥ç‰¹å¾çš„æ‰€æœ‰SHAPå€¼å’Œç‰¹å¾å€¼
            feat_shap = shap_values[:, :, feat_idx].flatten()  # [num_samples * seq_len]
            feat_values = feature_values[:, :, feat_idx].flatten()  # [num_samples * seq_len]
            
            # æ™ºèƒ½é‡‡æ ·ä»¥å‡å°‘ç‚¹çš„æ•°é‡
            if len(feat_shap) > max_points_per_feature:
                indices = self._smart_sample_indices(feat_shap, max_points_per_feature)
                feat_shap = feat_shap[indices]
                feat_values = feat_values[indices]
            
            # å‘é‡åŒ–æ·»åŠ åˆ°ç»˜å›¾æ•°æ®
            plot_data.extend([
                {
                    'Feature': top_features[i],
                    'SHAP': float(shap_val),
                    'Feature_Value': float(feat_val)
                }
                for shap_val, feat_val in zip(feat_shap, feat_values)
            ])
        
        # åˆ›å»ºDataFrame
        import pandas as pd
        df = pd.DataFrame(plot_data)
        
        # åˆ›å»ºSHAPæ‘˜è¦å›¾
        fig, ax = plt.subplots(figsize=(12, 10))
        
        # ä¸ºæ¯ä¸ªç‰¹å¾åˆ›å»ºæ°´å¹³ç‚¹å›¾
        y_positions = range(len(top_features))
        
        for i, feature in enumerate(top_features):
            feature_data = df[df['Feature'] == feature]
            
            # ç»˜åˆ¶ç‚¹
            scatter = ax.scatter(feature_data['SHAP'], [i] * len(feature_data), 
                               c=feature_data['Feature_Value'], 
                               cmap='viridis', alpha=0.6, s=20)
        
        # æ·»åŠ é›¶çº¿
        ax.axvline(x=0, color='gray', linestyle='--', alpha=0.5)
        
        # è®¾ç½®åæ ‡è½´
        ax.set_yticks(y_positions)
        ax.set_yticklabels(top_features)
        ax.set_xlabel('SHAP Value', fontsize=12)
        ax.set_title(f'SHAP Summary Plot - {self.model_name}', fontsize=14, fontweight='bold')
        
        # æ·»åŠ é¢œè‰²æ¡
        norm = plt.Normalize(0, 1)
        sm = plt.cm.ScalarMappable(cmap='viridis', norm=norm)
        sm.set_array([])
        cbar = plt.colorbar(sm, ax=ax)
        cbar.set_label('Feature Value', fontsize=10)
        
        # è°ƒæ•´å¸ƒå±€
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"ğŸ“Š SHAPæ‘˜è¦å›¾ä¿å­˜è‡³: {save_path}")
        
        plt.show()
    
    def create_feature_importance_plot(self, shap_values: np.ndarray, save_path: str = None):
        """åˆ›å»ºç‰¹å¾é‡è¦æ€§å›¾"""
        print("ğŸ“ˆ åˆ›å»ºç‰¹å¾é‡è¦æ€§å›¾...")
        
        # è®¡ç®—æ¯ä¸ªç‰¹å¾çš„å¹³å‡ç»å¯¹SHAPå€¼
        feature_importance = np.mean(np.abs(shap_values), axis=(0, 1))
        
        # é€‰æ‹©æœ€é‡è¦çš„ç‰¹å¾ï¼ˆTop 15ï¼‰
        top_k = min(15, self.input_channels)
        top_indices = np.argsort(feature_importance)[-top_k:][::-1]
        top_features = [self.driver_names[i] for i in top_indices]
        top_importance = feature_importance[top_indices]
        
        # åˆ›å»ºæ°´å¹³æ¡å½¢å›¾
        fig, ax = plt.subplots(figsize=(12, 8))
        
        bars = ax.barh(range(len(top_features)), top_importance, color='steelblue', alpha=0.7)
        
        # è®¾ç½®åæ ‡è½´
        ax.set_yticks(range(len(top_features)))
        ax.set_yticklabels(top_features)
        ax.set_xlabel('Mean |SHAP Value|', fontsize=12)
        ax.set_title(f'Feature Importance - {self.model_name}', fontsize=14, fontweight='bold')
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for i, (bar, importance) in enumerate(zip(bars, top_importance)):
            ax.text(bar.get_width() + 0.001, bar.get_y() + bar.get_height()/2, 
                   f'{importance:.4f}', va='center', fontsize=10)
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"ğŸ“ˆ ç‰¹å¾é‡è¦æ€§å›¾ä¿å­˜è‡³: {save_path}")
        
        plt.show()
    
    def generate_shap_report(self, shap_values: np.ndarray, feature_values: np.ndarray,
                           predictions: np.ndarray, save_path: str = None) -> str:
        """ç”ŸæˆSHAPåˆ†ææŠ¥å‘Š"""
        
        print("ğŸ“„ ç”ŸæˆSHAPåˆ†ææŠ¥å‘Š...")
        
        # è®¡ç®—æ¯ä¸ªç‰¹å¾çš„å¹³å‡ç»å¯¹SHAPå€¼ï¼ˆé‡è¦æ€§ï¼‰
        feature_importance = np.mean(np.abs(shap_values), axis=(0, 1))  # [channels]
        
        # è®¡ç®—æ¯ä¸ªç‰¹å¾çš„æ­£å‘å’Œè´Ÿå‘å½±å“åˆ†åˆ«
        positive_shap = np.where(shap_values > 0, shap_values, 0)
        negative_shap = np.where(shap_values < 0, shap_values, 0)
        
        # æ­£å‘å½±å“ï¼šæ‰€æœ‰æ­£SHAPå€¼çš„å¹³å‡
        positive_impact = np.mean(positive_shap, axis=(0, 1))  # [channels]
        # è´Ÿå‘å½±å“ï¼šæ‰€æœ‰è´ŸSHAPå€¼çš„å¹³å‡ï¼ˆå–ç»å¯¹å€¼ï¼‰
        negative_impact = np.abs(np.mean(negative_shap, axis=(0, 1)))  # [channels]
        
        # è®¡ç®—æ­£è´Ÿæ ·æœ¬æ¯”ä¾‹
        positive_ratio = np.sum(shap_values > 0, axis=(0, 1)) / shap_values.shape[0] / shap_values.shape[1]  # [channels]
        negative_ratio = np.sum(shap_values < 0, axis=(0, 1)) / shap_values.shape[0] / shap_values.shape[1]  # [channels]
        
        # ç»¼åˆå½±å“æ–¹å‘ï¼ˆç”¨äºæ’åºå’Œæ˜¾ç¤ºï¼‰
        feature_impact = positive_impact - negative_impact  # æ­£å€¼è¡¨ç¤ºæ­£å‘å½±å“ï¼Œè´Ÿå€¼è¡¨ç¤ºè´Ÿå‘å½±å“
        
        # è·å–æœ€é‡è¦çš„ç‰¹å¾ï¼ˆTop 20ï¼‰
        top_k = min(20, self.input_channels)
        top_indices = np.argsort(feature_importance)[-top_k:][::-1]
        top_features = [self.driver_names[i] for i in top_indices]
        top_importance = feature_importance[top_indices]
        top_impact = feature_impact[top_indices]
        
        # è®¡ç®—SHAPå€¼ç»Ÿè®¡ä¿¡æ¯
        shap_stats = {
            'mean': np.mean(shap_values),
            'std': np.std(shap_values),
            'min': np.min(shap_values),
            'max': np.max(shap_values),
            'positive_ratio': np.sum(shap_values > 0) / shap_values.size,
            'negative_ratio': np.sum(shap_values < 0) / shap_values.size
        }
        
        # ç”ŸæˆæŠ¥å‘Š
        report = f"""# SHAPåˆ†ææŠ¥å‘Š - {self.model_name}

## ğŸ“Š åˆ†ææ¦‚è¿°

- **æ¨¡å‹åç§°**: {self.model_name}
- **æ¨¡å‹è·¯å¾„**: {self.model_path}
- **è¾“å…¥åºåˆ—é•¿åº¦**: {self.seq_len}
- **é¢„æµ‹é•¿åº¦**: {self.pred_len}
- **è¾“å…¥é€šé“æ•°**: {self.input_channels}
- **æ ·æœ¬æ•°é‡**: {shap_values.shape[0]}
- **åˆ†ææ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## ğŸ” SHAPå€¼ç»Ÿè®¡ä¿¡æ¯

### æ•´ä½“ç»Ÿè®¡
- **å¹³å‡SHAPå€¼**: {shap_stats['mean']:.6f}
- **SHAPå€¼æ ‡å‡†å·®**: {shap_stats['std']:.6f}
- **SHAPå€¼èŒƒå›´**: [{shap_stats['min']:.6f}, {shap_stats['max']:.6f}]
- **æ­£å‘å½±å“æ¯”ä¾‹**: {shap_stats['positive_ratio']:.2%}
- **è´Ÿå‘å½±å“æ¯”ä¾‹**: {shap_stats['negative_ratio']:.2%}

## ğŸ† ç‰¹å¾é‡è¦æ€§æ’å (Top 20)

| æ’å | ç‰¹å¾åç§° | å¹³å‡ç»å¯¹SHAPå€¼ | æ­£å‘/è´Ÿå‘å½±å“ | å½±å“æ–¹å‘ (æ­£æ ·æœ¬æ¯”ä¾‹/è´Ÿæ ·æœ¬æ¯”ä¾‹) |
|------|----------|----------------|---------------|----------------------------------|
"""
        
        # æ·»åŠ ç‰¹å¾æ’åè¡¨æ ¼
        for i, (feature, importance, impact) in enumerate(zip(top_features, top_importance, top_impact)):
            feat_idx = top_indices[i]
            pos_impact = positive_impact[feat_idx]
            neg_impact = negative_impact[feat_idx]
            pos_ratio = positive_ratio[feat_idx]
            neg_ratio = negative_ratio[feat_idx]
            
            direction = "ğŸŸ¢ æ­£å‘" if impact > 0 else "ğŸ”´ è´Ÿå‘" if impact < 0 else "âšª ä¸­æ€§"
            report += f"| {i+1} | {feature} | {importance:.6f} | {pos_impact:.6f}/{neg_impact:.6f} | {direction} ({pos_ratio:.1%}/{neg_ratio:.1%}) |\n"
        
        report += f"""
## ğŸ“ˆ è¯¦ç»†ç‰¹å¾åˆ†æ

### æœ€é‡è¦çš„ç‰¹å¾åˆ†æ

"""
        
        # åˆ†æå‰5ä¸ªæœ€é‡è¦çš„ç‰¹å¾
        for i in range(min(5, len(top_features))):
            feat_idx = top_indices[i]
            feature = top_features[i]
            importance = top_importance[i]
            impact = top_impact[i]
            
            # è·å–è¯¥ç‰¹å¾çš„è¯¦ç»†ç»Ÿè®¡
            feat_shap = shap_values[:, :, feat_idx]
            feat_values = feature_values[:, :, feat_idx]
            
            # è®¡ç®—ç‰¹å¾å€¼ä¸SHAPå€¼çš„å…³ç³»
            high_val_mask = feat_values > np.median(feat_values)
            low_val_mask = feat_values <= np.median(feat_values)
            
            high_shap_mean = np.mean(feat_shap[high_val_mask])
            low_shap_mean = np.mean(feat_shap[low_val_mask])
            
            # è·å–è¯¥ç‰¹å¾çš„æ­£è´Ÿå½±å“è¯¦ç»†ä¿¡æ¯
            pos_impact = positive_impact[feat_idx]
            neg_impact = negative_impact[feat_idx]
            pos_ratio = positive_ratio[feat_idx]
            neg_ratio = negative_ratio[feat_idx]
            
            report += f"""#### {i+1}. {feature}

- **é‡è¦æ€§æ’å**: ç¬¬{i+1}ä½
- **å¹³å‡ç»å¯¹SHAPå€¼**: {importance:.6f}
- **æ­£å‘å½±å“å¼ºåº¦**: {pos_impact:.6f} (å {pos_ratio:.1%}æ ·æœ¬)
- **è´Ÿå‘å½±å“å¼ºåº¦**: {neg_impact:.6f} (å {neg_ratio:.1%}æ ·æœ¬)
- **ç»¼åˆå½±å“æ–¹å‘**: {'æ­£å‘' if impact > 0 else 'è´Ÿå‘' if impact < 0 else 'ä¸­æ€§'}
- **é«˜å€¼æ ·æœ¬å¹³å‡SHAP**: {high_shap_mean:.6f}
- **ä½å€¼æ ·æœ¬å¹³å‡SHAP**: {low_shap_mean:.6f}
- **ç‰¹å¾å€¼èŒƒå›´**: [{np.min(feat_values):.4f}, {np.max(feat_values):.4f}]

**åˆ†æè§£è¯»**: 
"""
            
            if impact > 0:
                report += f"{feature}å¯¹æ¨¡å‹é¢„æµ‹æœ‰æ­£å‘å½±å“ã€‚å½“{feature}å€¼è¾ƒé«˜æ—¶ï¼Œæ¨¡å‹å€¾å‘äºå¢åŠ é¢„æµ‹å€¼ã€‚"
            else:
                report += f"{feature}å¯¹æ¨¡å‹é¢„æµ‹æœ‰è´Ÿå‘å½±å“ã€‚å½“{feature}å€¼è¾ƒé«˜æ—¶ï¼Œæ¨¡å‹å€¾å‘äºå‡å°‘é¢„æµ‹å€¼ã€‚"
            
            if abs(high_shap_mean - low_shap_mean) > 0.01:
                report += f" ç‰¹å¾å€¼çš„é«˜ä½å¯¹SHAPå€¼æœ‰æ˜¾è‘—å½±å“ã€‚"
            else:
                report += f" ç‰¹å¾å€¼çš„é«˜ä½å¯¹SHAPå€¼å½±å“è¾ƒå°ã€‚"
            
            report += "\n\n"
        
        # æŒ‰ç±»åˆ«åˆ†æç‰¹å¾
        report += """## ğŸ—‚ï¸ æŒ‰ç±»åˆ«åˆ†æç‰¹å¾

### æ°”è±¡æ•°æ®ç‰¹å¾
"""
        
        weather_features = ['Temperatu', 'U_Wind_10r', 'V_Wind_10m', 'Snow_Cove', 'Total_Perci', 
                          'Surface_La', 'Downpoint', 'Surface_Pro']
        
        for feature in weather_features:
            if feature in self.driver_names:
                idx = self.driver_names.index(feature)
                importance = feature_importance[idx]
                impact = feature_impact[idx]
                pos_impact = positive_impact[idx]
                neg_impact = negative_impact[idx]
                pos_ratio = positive_ratio[idx]
                neg_ratio = negative_ratio[idx]
                rank = np.where(np.argsort(feature_importance)[::-1] == idx)[0][0] + 1
                report += f"- **{feature}**: æ’åç¬¬{rank}ä½ï¼Œé‡è¦æ€§{importance:.6f}ï¼Œæ­£å‘{pos_impact:.6f}({pos_ratio:.1%})ï¼Œè´Ÿå‘{neg_impact:.6f}({neg_ratio:.1%})\n"
        
        report += """
### åœŸå£¤æ•°æ®ç‰¹å¾
"""
        
        soil_features = ['Volumetric_1', 'Volumetric_2', 'Volumetric_3', 'Volumetric_4']
        
        for feature in soil_features:
            if feature in self.driver_names:
                idx = self.driver_names.index(feature)
                importance = feature_importance[idx]
                impact = feature_impact[idx]
                pos_impact = positive_impact[idx]
                neg_impact = negative_impact[idx]
                pos_ratio = positive_ratio[idx]
                neg_ratio = negative_ratio[idx]
                rank = np.where(np.argsort(feature_importance)[::-1] == idx)[0][0] + 1
                report += f"- **{feature}**: æ’åç¬¬{rank}ä½ï¼Œé‡è¦æ€§{importance:.6f}ï¼Œæ­£å‘{pos_impact:.6f}({pos_ratio:.1%})ï¼Œè´Ÿå‘{neg_impact:.6f}({neg_ratio:.1%})\n"
        
        report += """
### åœ°å½¢æ•°æ®ç‰¹å¾
"""
        
        terrain_features = ['Aspect', 'DEM', 'Hillshade', 'Slope', 'D2Infrastre', 'D2Water']
        
        for feature in terrain_features:
            if feature in self.driver_names:
                idx = self.driver_names.index(feature)
                importance = feature_importance[idx]
                impact = feature_impact[idx]
                pos_impact = positive_impact[idx]
                neg_impact = negative_impact[idx]
                pos_ratio = positive_ratio[idx]
                neg_ratio = negative_ratio[idx]
                rank = np.where(np.argsort(feature_importance)[::-1] == idx)[0][0] + 1
                report += f"- **{feature}**: æ’åç¬¬{rank}ä½ï¼Œé‡è¦æ€§{importance:.6f}ï¼Œæ­£å‘{pos_impact:.6f}({pos_ratio:.1%})ï¼Œè´Ÿå‘{neg_impact:.6f}({neg_ratio:.1%})\n"
        
        report += """
### å«æ˜Ÿæ•°æ®ç‰¹å¾
"""
        
        satellite_features = ['NDVI', 'EVI', 'Band1', 'Band2', 'Band3', 'Band7', 'LST_D', 'LST_N', 'LAI']
        
        for feature in satellite_features:
            if feature in self.driver_names:
                idx = self.driver_names.index(feature)
                importance = feature_importance[idx]
                impact = feature_impact[idx]
                pos_impact = positive_impact[idx]
                neg_impact = negative_impact[idx]
                pos_ratio = positive_ratio[idx]
                neg_ratio = negative_ratio[idx]
                rank = np.where(np.argsort(feature_importance)[::-1] == idx)[0][0] + 1
                report += f"- **{feature}**: æ’åç¬¬{rank}ä½ï¼Œé‡è¦æ€§{importance:.6f}ï¼Œæ­£å‘{pos_impact:.6f}({pos_ratio:.1%})ï¼Œè´Ÿå‘{neg_impact:.6f}({neg_ratio:.1%})\n"
        
        report += f"""
## ğŸ“Š é¢„æµ‹ç»“æœåˆ†æ

### é¢„æµ‹å€¼ç»Ÿè®¡
- **å¹³å‡é¢„æµ‹å€¼**: {np.mean(predictions):.6f}
- **é¢„æµ‹å€¼æ ‡å‡†å·®**: {np.std(predictions):.6f}
- **é¢„æµ‹å€¼èŒƒå›´**: [{np.min(predictions):.6f}, {np.max(predictions):.6f}]

### é¢„æµ‹åˆ†å¸ƒ
- **é«˜é¢„æµ‹å€¼æ ·æœ¬æ¯”ä¾‹**: {np.sum(predictions > np.median(predictions)) / len(predictions):.2%}
- **ä½é¢„æµ‹å€¼æ ·æœ¬æ¯”ä¾‹**: {np.sum(predictions <= np.median(predictions)) / len(predictions):.2%}

## ğŸ¯ å…³é”®å‘ç°

### 1. æœ€é‡è¦çš„é©±åŠ¨å› ç´ 
æ ¹æ®SHAPåˆ†æï¼Œä»¥ä¸‹ç‰¹å¾å¯¹æ¨¡å‹é¢„æµ‹å½±å“æœ€å¤§ï¼š
"""
        
        for i in range(min(3, len(top_features))):
            report += f"- **{top_features[i]}**: {top_importance[i]:.6f}\n"
        
        report += """
### 2. å½±å“æ–¹å‘åˆ†æ
- **æ­£å‘å½±å“ç‰¹å¾**: è¿™äº›ç‰¹å¾å€¼è¶Šé«˜ï¼Œæ¨¡å‹é¢„æµ‹å€¼è¶Šå¤§
- **è´Ÿå‘å½±å“ç‰¹å¾**: è¿™äº›ç‰¹å¾å€¼è¶Šé«˜ï¼Œæ¨¡å‹é¢„æµ‹å€¼è¶Šå°

### 3. ç‰¹å¾äº¤äº’æ¨¡å¼
é€šè¿‡SHAPå€¼åˆ†å¸ƒå¯ä»¥çœ‹å‡ºä¸åŒç‰¹å¾ä¹‹é—´çš„äº¤äº’å…³ç³»ï¼š
- æŸäº›ç‰¹å¾ç»„åˆå¯èƒ½äº§ç”ŸååŒæ•ˆåº”
- ä¸åŒç‰¹å¾å€¼èŒƒå›´å¯¹é¢„æµ‹çš„å½±å“ç¨‹åº¦ä¸åŒ

## ğŸ’¡ å»ºè®®ä¸æ”¹è¿›

### 1. æ•°æ®æ”¶é›†å»ºè®®
- é‡ç‚¹å…³æ³¨é«˜é‡è¦æ€§ç‰¹å¾çš„æµ‹é‡ç²¾åº¦
- è€ƒè™‘å¢åŠ å¯¹é‡è¦ç‰¹å¾çš„æ—¶é—´åˆ†è¾¨ç‡
- ä¼˜åŒ–æ•°æ®é¢„å¤„ç†æµç¨‹

### 2. æ¨¡å‹ä¼˜åŒ–å»ºè®®
- å¯ä»¥é’ˆå¯¹é«˜é‡è¦æ€§ç‰¹å¾è¿›è¡Œç‰¹å¾å·¥ç¨‹
- è€ƒè™‘ä½¿ç”¨ç‰¹å¾é€‰æ‹©æ–¹æ³•å‡å°‘å™ªå£°ç‰¹å¾
- ä¼˜åŒ–æ¨¡å‹æ¶æ„ä»¥æ›´å¥½åœ°åˆ©ç”¨é‡è¦ç‰¹å¾

### 3. åº”ç”¨å»ºè®®
- åœ¨å®é™…åº”ç”¨ä¸­ä¼˜å…ˆå…³æ³¨é«˜é‡è¦æ€§ç‰¹å¾
- å»ºç«‹åŸºäºSHAPå€¼çš„å¼‚å¸¸æ£€æµ‹æœºåˆ¶
- å®šæœŸæ›´æ–°SHAPåˆ†æä»¥ç›‘æ§æ¨¡å‹æ€§èƒ½å˜åŒ–

## ğŸ“‹ æŠ€æœ¯è¯´æ˜

### SHAPå€¼è®¡ç®—åŸç†
æœ¬åˆ†æä½¿ç”¨åŸºäºæ‰°åŠ¨çš„SHAPå€¼è®¡ç®—æ–¹æ³•ï¼š
1. ä½¿ç”¨èƒŒæ™¯æ ·æœ¬å»ºç«‹åŸºçº¿é¢„æµ‹
2. å¯¹æ¯ä¸ªç‰¹å¾è¿›è¡Œæ‰°åŠ¨ï¼ˆç”¨èƒŒæ™¯å¹³å‡å€¼æ›¿æ¢ï¼‰
3. è®¡ç®—é¢„æµ‹å·®å¼‚ä½œä¸ºSHAPå€¼
4. ç»Ÿè®¡æ‰€æœ‰æ ·æœ¬çš„SHAPå€¼åˆ†å¸ƒ

### æ­£è´Ÿå½±å“è®¡ç®—æ–¹æ³•
ä¸ºäº†é¿å…æ­£è´Ÿå½±å“æŠµæ¶ˆçš„é—®é¢˜ï¼Œæœ¬åˆ†æé‡‡ç”¨ä»¥ä¸‹æ–¹æ³•ï¼š
1. **æ­£å‘å½±å“**: è®¡ç®—æ‰€æœ‰æ­£SHAPå€¼çš„å¹³å‡å€¼ï¼Œè¡¨ç¤ºè¯¥ç‰¹å¾å¯¹é¢„æµ‹çš„æ­£å‘è´¡çŒ®å¼ºåº¦
2. **è´Ÿå‘å½±å“**: è®¡ç®—æ‰€æœ‰è´ŸSHAPå€¼çš„å¹³å‡å€¼çš„ç»å¯¹å€¼ï¼Œè¡¨ç¤ºè¯¥ç‰¹å¾å¯¹é¢„æµ‹çš„è´Ÿå‘è´¡çŒ®å¼ºåº¦
3. **æ­£è´Ÿæ ·æœ¬æ¯”ä¾‹**: ç»Ÿè®¡æ­£è´ŸSHAPå€¼åœ¨æ€»æ ·æœ¬ä¸­çš„å æ¯”
4. **ç»¼åˆå½±å“æ–¹å‘**: æ­£å‘å½±å“ - è´Ÿå‘å½±å“ï¼Œæ­£å€¼è¡¨ç¤ºæ•´ä½“æ­£å‘ï¼Œè´Ÿå€¼è¡¨ç¤ºæ•´ä½“è´Ÿå‘

### å¯è§†åŒ–è¯´æ˜
- **SHAPæ‘˜è¦å›¾**: æ˜¾ç¤ºæ¯ä¸ªç‰¹å¾çš„SHAPå€¼åˆ†å¸ƒå’Œç‰¹å¾å€¼å…³ç³»
- **ç‰¹å¾é‡è¦æ€§å›¾**: æŒ‰é‡è¦æ€§æ’åºæ˜¾ç¤ºå„ç‰¹å¾çš„å¹³å‡ç»å¯¹SHAPå€¼
- **é¢œè‰²ç¼–ç **: è“è‰²è¡¨ç¤ºä½ç‰¹å¾å€¼ï¼Œçº¢è‰²è¡¨ç¤ºé«˜ç‰¹å¾å€¼

---
*æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*
"""
        
        if save_path:
            with open(save_path, 'w', encoding='utf-8') as f:
                f.write(report)
            print(f"ğŸ“„ SHAPåˆ†ææŠ¥å‘Šä¿å­˜è‡³: {save_path}")
        
        return report
    
    def run_shap_analysis(self, output_dir: str = './shap_results', num_samples: int = 100):
        """è¿è¡Œå®Œæ•´çš„SHAPåˆ†æ"""
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(output_dir, exist_ok=True)
        
        print(f"ğŸš€ å¼€å§‹ {self.model_name} çš„SHAPåˆ†æ...")
        
        # ç”Ÿæˆæ ·æœ¬æ•°æ®
        input_data, predictions = self.generate_sample_data(num_samples)
        
        # è®¡ç®—SHAPå€¼
        shap_values, feature_values = self.calculate_shap_values(input_data)
        
        # åˆ›å»ºå¯è§†åŒ–
        self.create_shap_summary_plot(
            shap_values, feature_values,
            os.path.join(output_dir, f'{self.model_name}_shap_summary.png')
        )
        
        self.create_feature_importance_plot(
            shap_values,
            os.path.join(output_dir, f'{self.model_name}_feature_importance.png')
        )
        
        # ç”Ÿæˆåˆ†ææŠ¥å‘Š
        report = self.generate_shap_report(
            shap_values, feature_values, predictions.cpu().numpy(),
            os.path.join(output_dir, f'{self.model_name}_shap_analysis_report.md')
        )
        
        print(f"âœ… SHAPåˆ†æå®Œæˆï¼ç»“æœä¿å­˜åœ¨: {output_dir}")
        print(f"ğŸ“„ åˆ†ææŠ¥å‘Š: {os.path.join(output_dir, f'{self.model_name}_shap_analysis_report.md')}")
        
        return {
            'shap_values': shap_values,
            'feature_values': feature_values,
            'predictions': predictions.cpu().numpy(),
            'report': report
        }


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='SHAPåˆ†æå·¥å…·')
    parser.add_argument('--model_path', type=str, required=True, help='æ¨¡å‹æ–‡ä»¶è·¯å¾„(.pth)')
    parser.add_argument('--model_name', type=str, required=True, help='æ¨¡å‹åç§°')
    parser.add_argument('--seq_len', type=int, default=365, help='è¾“å…¥åºåˆ—é•¿åº¦')
    parser.add_argument('--pred_len', type=int, default=1, help='é¢„æµ‹é•¿åº¦')
    parser.add_argument('--input_channels', type=int, default=40, help='è¾“å…¥é€šé“æ•°')
    parser.add_argument('--output_dir', type=str, default='./shap_results', help='è¾“å‡ºç›®å½•')
    parser.add_argument('--num_samples', type=int, default=100, help='æ ·æœ¬æ•°é‡')
    parser.add_argument('--device', type=str, default='cuda', help='è®¡ç®—è®¾å¤‡')
    
    args = parser.parse_args()
    
    # åˆ›å»ºSHAPåˆ†æå™¨
    analyzer = SHAPAnalyzer(
        model_path=args.model_path,
        model_name=args.model_name,
        seq_len=args.seq_len,
        pred_len=args.pred_len,
        input_channels=args.input_channels,
        device=args.device
    )
    
    # è¿è¡Œåˆ†æ
    results = analyzer.run_shap_analysis(args.output_dir, args.num_samples)
    
    print("ğŸ‰ SHAPåˆ†æå®Œæˆï¼")


if __name__ == "__main__":
    main() 