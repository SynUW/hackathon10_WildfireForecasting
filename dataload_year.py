#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åŸºäºå¹´ä»½æ•°æ®çš„æ—¶é—´åºåˆ—æ•°æ®åŠ è½½å™¨ - æ— ç¼æ›¿æ¢ç‰ˆæœ¬
é€‚é…generate_whole_dataset.pyç”Ÿæˆçš„å¹´ä»½æ•°æ®ï¼Œä¸åŸdataload.pyæ¥å£å®Œå…¨ä¸€è‡´

æ•°æ®æ ¼å¼:
- æ–‡ä»¶å: {year}_year_dataset.h5
- æ•°æ®é›†å: {row}_{col} (åƒç´ åæ ‡)
- æ•°æ®å½¢çŠ¶: (channels, time_steps) å…¶ä¸­ channels=39, time_steps=365/366
- é€šé“0: FIRMSæ•°æ®ï¼Œç”¨äºæ­£è´Ÿæ ·æœ¬åˆ¤æ–­

ç‰¹æ€§:
- æ­£æ ·æœ¬: FIRMS >= min_fire_threshold çš„æŸä¸€å¤©
- è´Ÿæ ·æœ¬: å…¨å±€æ— ç«æ—¥æœŸçš„éšæœºåƒç´ ä½ç½®
- æ”¯æŒè·¨å¹´ä»½æ•°æ®åŠ è½½ï¼ˆå†å²/æœªæ¥æ•°æ®ï¼‰
- æ™ºèƒ½é‡‡æ ·ç¼“å­˜æœºåˆ¶ï¼Œé¿å…é‡å¤è®¡ç®—
- ä¸åŸdataload.pyå®Œå…¨å…¼å®¹çš„æ¥å£
"""

import os
import h5py
import torch
import numpy as np
from torch.utils.data import Dataset, DataLoader
from tqdm import tqdm
import logging
import hashlib
import pickle
import random
import time
from datetime import datetime, timedelta
from collections import defaultdict
import glob
import json
import gc
from concurrent.futures import ThreadPoolExecutor, as_completed

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class YearTimeSeriesPixelDataset(Dataset):
    """
    åŸºäºå¹´ä»½æ•°æ®çš„æ—¶é—´åºåˆ—åƒç´ æ•°æ®é›†
    
    ä¸åŸTimeSeriesPixelDatasetå®Œå…¨å…¼å®¹çš„æ¥å£ï¼Œä½†ä½¿ç”¨æ–°çš„å¹´ä»½æ•°æ®æº
    """
    
    def __init__(self, h5_dir, years=None, return_metadata=True, 
                 positive_ratio=1.0, pos_neg_ratio=1.0, 
                 resample_each_epoch=False, epoch_seed=None, verbose_sampling=True,
                 lookback_seq=365, forecast_hor=7, min_fire_threshold=0.001,
                 cache_dir=None, force_resample=False):
        """
        åˆå§‹åŒ–å¹´ä»½æ—¶é—´åºåˆ—åƒç´ æ•°æ®é›†
        
        Args:
            h5_dir: å¹´ä»½æ•°æ®H5æ–‡ä»¶ç›®å½•
            years: è¦åŠ è½½çš„å¹´ä»½åˆ—è¡¨ï¼ŒNoneè¡¨ç¤ºåŠ è½½æ‰€æœ‰å¹´ä»½
            return_metadata: æ˜¯å¦è¿”å›å…ƒæ•°æ®ï¼ˆæ—¥æœŸã€åæ ‡ç­‰ï¼‰
            positive_ratio: æ­£æ ·æœ¬ä½¿ç”¨æ¯”ä¾‹ (0.0-1.0)
            pos_neg_ratio: æ­£è´Ÿæ ·æœ¬æ¯”ä¾‹ï¼Œå³è´Ÿæ ·æœ¬æ•° = æ­£æ ·æœ¬æ•° Ã— pos_neg_ratio
            resample_each_epoch: æ˜¯å¦åœ¨æ¯ä¸ªepoché‡æ–°è¿›è¡Œæ ·æœ¬æŠ½æ ·
            epoch_seed: å½“å‰epochçš„éšæœºç§å­
            verbose_sampling: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†é‡‡æ ·ä¿¡æ¯
            lookback_seq: å†å²æ—¶é—´é•¿åº¦ï¼ˆå¤©ï¼‰
            forecast_hor: æœªæ¥æ—¶é—´é•¿åº¦ï¼ˆå¤©ï¼‰
            min_fire_threshold: FIRMSé˜ˆå€¼ï¼Œ>=è¯¥å€¼è®¤ä¸ºæ˜¯æ­£æ ·æœ¬
            cache_dir: é‡‡æ ·ç¼“å­˜ç›®å½•ï¼ŒNoneè¡¨ç¤ºä½¿ç”¨h5_dir/cache
            force_resample: æ˜¯å¦å¼ºåˆ¶é‡æ–°é‡‡æ ·ï¼Œå¿½ç•¥ç¼“å­˜
        """
        self.h5_dir = h5_dir
        self.years = years
        self.return_metadata = return_metadata
        self.positive_ratio = positive_ratio
        self.pos_neg_ratio = pos_neg_ratio
        self.resample_each_epoch = resample_each_epoch
        self.epoch_seed = epoch_seed
        self.verbose_sampling = verbose_sampling
        self.lookback_seq = lookback_seq
        self.forecast_hor = forecast_hor
        self.min_fire_threshold = min_fire_threshold
        self.force_resample = force_resample
        
        # ç¼“å­˜é…ç½®
        self.cache_dir = cache_dir if cache_dir else os.path.join(h5_dir, 'cache')
        os.makedirs(self.cache_dir, exist_ok=True)
        
        # è·å–å¹´ä»½æ–‡ä»¶åˆ—è¡¨
        self.year_files = self._get_year_files()
        
        # æ„å»ºæˆ–åŠ è½½é‡‡æ ·ç»“æœ
        self.full_sample_index = []  # å­˜å‚¨æ‰€æœ‰æ ·æœ¬çš„å®Œæ•´ç´¢å¼•
        self.sample_index = []  # å½“å‰ä½¿ç”¨çš„æ ·æœ¬ç´¢å¼•
        self.dataset_info = {}  # å­˜å‚¨æ•°æ®é›†ä¿¡æ¯
        
        self._build_or_load_samples()
        
        # åˆå§‹çš„æ ·æœ¬æ¯”ä¾‹ç­›é€‰
        if positive_ratio < 1.0 or pos_neg_ratio != 1.0:
            if resample_each_epoch:
                if self.verbose_sampling:
                    logger.info("å¯ç”¨æ¯epoché‡æ–°æŠ½æ ·æ¨¡å¼")
                # ä¿å­˜å®Œæ•´ç´¢å¼•å¹¶è¿›è¡Œåˆå§‹æŠ½æ ·
                self.full_sample_index = self.sample_index.copy()
                self._apply_sample_ratio_filtering()
            else:
                # ä¼ ç»Ÿæ¨¡å¼ï¼šä¸€æ¬¡æ€§æŠ½æ ·
                self._apply_sample_ratio_filtering()
        
        logger.info(f"å¹´ä»½æ•°æ®é›†åˆå§‹åŒ–å®Œæˆï¼Œå…± {len(self.sample_index)} ä¸ªæ ·æœ¬")
        logger.info(f"æ­£æ ·æœ¬ä½¿ç”¨æ¯”ä¾‹: {positive_ratio:.2f}, æ­£è´Ÿæ ·æœ¬æ¯”ä¾‹: 1:{pos_neg_ratio:.2f}")
        if resample_each_epoch:
            logger.info(f"å¯ç”¨æ¯epoché‡æ–°æŠ½æ ·ï¼Œæ€»æ ·æœ¬æ± : {len(self.full_sample_index)} ä¸ªæ ·æœ¬")
    
    def _get_year_files(self):
        """è·å–å¹´ä»½H5æ–‡ä»¶åˆ—è¡¨"""
        year_files = {}
        
        # æŸ¥æ‰¾æ‰€æœ‰å¹´ä»½æ–‡ä»¶
        for filename in os.listdir(self.h5_dir):
            if filename.endswith('_year_dataset.h5'):
                try:
                    year = int(filename.split('_')[0])
                    if self.years is None or year in self.years:
                        year_files[year] = os.path.join(self.h5_dir, filename)
                except ValueError:
                    continue
        
        if not year_files:
            raise ValueError(f"æœªæ‰¾åˆ°å¹´ä»½æ•°æ®æ–‡ä»¶ï¼Œç›®å½•: {self.h5_dir}")
        
        logger.info(f"æ‰¾åˆ° {len(year_files)} ä¸ªå¹´ä»½æ•°æ®æ–‡ä»¶: {sorted(year_files.keys())}")
        return year_files
    
    def _get_cache_filename(self):
        """ç”Ÿæˆç¼“å­˜æ–‡ä»¶å"""
        # æ ¹æ®å…³é”®å‚æ•°ç”Ÿæˆå”¯ä¸€æ ‡è¯†
        cache_params = {
            'h5_dir': self.h5_dir,
            'years': sorted(self.year_files.keys()),
            'lookback_seq': self.lookback_seq,
            'forecast_hor': self.forecast_hor,
            'min_fire_threshold': self.min_fire_threshold,
            'version': '1.0'  # ç‰ˆæœ¬å·ï¼Œç”¨äºç¼“å­˜æ ¼å¼å˜æ›´
        }
        
        # ç”Ÿæˆå‚æ•°å“ˆå¸Œ
        param_str = json.dumps(cache_params, sort_keys=True)
        param_hash = hashlib.md5(param_str.encode()).hexdigest()[:12]
        
        return os.path.join(self.cache_dir, f"samples_{param_hash}.h5")
    
    def _check_cache_validity(self, cache_file):
        """æ£€æŸ¥ç¼“å­˜æ–‡ä»¶æ˜¯å¦æœ‰æ•ˆ"""
        if self.force_resample:
            return False
        
        if not os.path.exists(cache_file):
            return False
        
        try:
            with h5py.File(cache_file, 'r') as f:
                # æ£€æŸ¥ç¼“å­˜å‚æ•°
                cached_params = json.loads(f.attrs.get('cache_params', '{}'))
                current_params = {
                    'h5_dir': self.h5_dir,
                    'years': sorted(self.year_files.keys()),
                    'lookback_seq': self.lookback_seq,
                    'forecast_hor': self.forecast_hor,
                    'min_fire_threshold': self.min_fire_threshold,
                    'version': '1.0'
                }
                
                if cached_params != current_params:
                    return False
                
                # æ£€æŸ¥æºæ•°æ®æ–‡ä»¶æ˜¯å¦æœ‰å˜åŒ–
                cached_file_mtimes = json.loads(f.attrs.get('source_file_mtimes', '{}'))
                current_file_mtimes = {}
                
                for year, file_path in self.year_files.items():
                    if os.path.exists(file_path):
                        current_file_mtimes[str(year)] = os.path.getmtime(file_path)
                
                if cached_file_mtimes != current_file_mtimes:
                    return False
                
                return True
                
        except Exception as e:
            logger.warning(f"æ£€æŸ¥ç¼“å­˜æ–‡ä»¶å¤±è´¥: {e}")
            return False
    
    def _process_year_for_no_fire_days(self, year_file_info):
        """å¤„ç†å•ä¸ªå¹´ä»½çš„æ— ç«æ—¥æœŸæŸ¥æ‰¾"""
        year, file_path = year_file_info
        year_no_fire_days = []
        
        try:
            with h5py.File(file_path, 'r') as f:
                year_days = int(f.attrs.get('total_time_steps', 365))
                
                # ä¸€æ¬¡æ€§åŠ è½½æ‰€æœ‰åƒç´ çš„FIRMSæ•°æ®
                pixel_names = [name for name in f.keys() if '_' in name]
                
                # ğŸ” æ·»åŠ è°ƒè¯•ä¿¡æ¯
                logger.info(f"å¹´ä»½ {year}: æ‰¾åˆ° {len(pixel_names)} ä¸ªåƒç´ æ•°æ®é›†ï¼ˆæŸ¥æ‰¾æ— ç«æ—¥æœŸï¼‰")
                if len(pixel_names) == 0:
                    logger.warning(f"å¹´ä»½ {year} æ–‡ä»¶ {file_path} æ²¡æœ‰æ‰¾åˆ°åƒç´ æ•°æ®é›†")
                    all_keys = list(f.keys())
                    logger.warning(f"æ–‡ä»¶ä¸­çš„æ‰€æœ‰é”®: {all_keys[:10]}...")  # æ˜¾ç¤ºå‰10ä¸ª
                    return year_no_fire_days
                
                if not pixel_names:
                    return year_no_fire_days
                
                # æ‰¹é‡åŠ è½½FIRMSæ•°æ®
                firms_data_list = []
                for pixel_name in pixel_names:
                    try:
                        pixel_data = f[pixel_name]
                        firms_channel = pixel_data[0, :]  # FIRMSé€šé“æ•°æ®
                        firms_data_list.append(firms_channel)
                    except Exception as e:
                        logger.warning(f"åŠ è½½åƒç´  {pixel_name} å¤±è´¥: {e}")
                        continue
                
                if not firms_data_list:
                    return year_no_fire_days
                
                # è½¬æ¢ä¸ºnumpyæ•°ç»„è¿›è¡Œå‘é‡åŒ–è®¡ç®—
                # shape: (num_pixels, time_steps)
                firms_array = np.array(firms_data_list)
                
                # å‘é‡åŒ–è®¡ç®—æ¯å¤©çš„æœ€å¤§FIRMSå€¼
                daily_max_firms = np.nanmax(firms_array, axis=0)
                
                # ğŸ” æ·»åŠ è°ƒè¯•ä¿¡æ¯
                overall_max_firms = np.nanmax(daily_max_firms)
                overall_min_firms = np.nanmin(daily_max_firms)
                logger.info(f"å¹´ä»½ {year}: æ¯æ—¥æœ€å¤§FIRMSå€¼èŒƒå›´: {overall_min_firms:.2f}-{overall_max_firms:.2f}, é˜ˆå€¼: {self.min_fire_threshold}")
                
                # æ‰¾å‡ºæ— ç«æ—¥æœŸï¼ˆæœ€å¤§FIRMSå€¼å°äºé˜ˆå€¼çš„å¤©ï¼‰
                no_fire_days = np.where(daily_max_firms < self.min_fire_threshold)[0]
                
                # ğŸ” æ·»åŠ è°ƒè¯•ä¿¡æ¯
                logger.info(f"å¹´ä»½ {year}: æ‰¾åˆ° {len(no_fire_days)} ä¸ªæ— ç«æ—¥æœŸï¼ˆå…± {len(daily_max_firms)} å¤©ï¼‰")
                
                # æ„å»ºç»“æœ
                start_date = datetime(year, 1, 1)
                for day_idx in no_fire_days:
                    actual_date = start_date + timedelta(days=int(day_idx))
                    year_no_fire_days.append({
                        'year': year,
                        'day_of_year': int(day_idx),
                        'date': actual_date,
                        'file_path': file_path
                    })
                    
        except Exception as e:
            logger.error(f"å¤„ç†å¹´ä»½ {year} æ—¶å‡ºé”™: {e}")
        
        # ğŸ” æ·»åŠ æ±‡æ€»ä¿¡æ¯
        logger.info(f"å¹´ä»½ {year}: æ€»å…±æ‰¾åˆ° {len(year_no_fire_days)} ä¸ªæ— ç«æ—¥æœŸ")
        
        return year_no_fire_days

    def _find_global_no_fire_days(self):
        """æŸ¥æ‰¾å…¨å±€æ— ç«æ—¥æœŸ - ä¼˜åŒ–ç‰ˆæœ¬"""
        logger.info("æ­£åœ¨æŸ¥æ‰¾å…¨å±€æ— ç«æ—¥æœŸ...")
        
        global_no_fire_days = []
        
        # ä½¿ç”¨çº¿ç¨‹æ± è€Œä¸æ˜¯è¿›ç¨‹æ± ï¼Œé¿å…pickleé—®é¢˜
        from concurrent.futures import ThreadPoolExecutor, as_completed
        
        # å¹¶è¡Œå¤„ç†æ‰€æœ‰å¹´ä»½ (é™åˆ¶å¹¶è¡Œæ•°é¿å…å†…å­˜è¿‡è½½)
        max_workers = min(4, len(self.year_files), os.cpu_count() or 1)
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = [
                executor.submit(self._process_year_for_no_fire_days, (year, file_path))
                for year, file_path in self.year_files.items()
            ]
            
            for future in tqdm(as_completed(futures), total=len(futures), desc="æ‰«æå¹´ä»½"):
                try:
                    year_results = future.result()
                    global_no_fire_days.extend(year_results)
                except Exception as e:
                    logger.error(f"å¤„ç†å¹´ä»½æ—¶å‡ºé”™: {e}")
        
        logger.info(f"æ‰¾åˆ° {len(global_no_fire_days)} ä¸ªå…¨å±€æ— ç«æ—¥æœŸ")
        return global_no_fire_days
    
    def _process_year_for_positive_samples(self, year_file_info):
        """å¤„ç†å•ä¸ªå¹´ä»½çš„æ­£æ ·æœ¬æŸ¥æ‰¾"""
        year, file_path = year_file_info
        year_positive_samples = []
        
        try:
            with h5py.File(file_path, 'r') as f:
                year_days = int(f.attrs.get('total_time_steps', 365))
                start_date = datetime(year, 1, 1)
                
                # è·å–æ‰€æœ‰åƒç´ æ•°æ®é›†åç§°
                pixel_names = [name for name in f.keys() if '_' in name]
                
                # ğŸ” æ·»åŠ è°ƒè¯•ä¿¡æ¯
                logger.info(f"å¹´ä»½ {year}: æ‰¾åˆ° {len(pixel_names)} ä¸ªåƒç´ æ•°æ®é›†")
                if len(pixel_names) == 0:
                    logger.warning(f"å¹´ä»½ {year} æ–‡ä»¶ {file_path} æ²¡æœ‰æ‰¾åˆ°åƒç´ æ•°æ®é›†")
                    all_keys = list(f.keys())
                    logger.warning(f"æ–‡ä»¶ä¸­çš„æ‰€æœ‰é”®: {all_keys[:10]}...")  # æ˜¾ç¤ºå‰10ä¸ª
                
                for pixel_name in pixel_names:
                    try:
                        row, col = map(int, pixel_name.split('_'))
                        pixel_data = f[pixel_name]
                        firms_channel = pixel_data[0, :]  # FIRMSé€šé“æ•°æ®
                        
                        # ğŸ” æ·»åŠ è°ƒè¯•ä¿¡æ¯
                        max_firms = np.nanmax(firms_channel)
                        min_firms = np.nanmin(firms_channel)
                        
                        # å‘é‡åŒ–æŸ¥æ‰¾è¶…è¿‡é˜ˆå€¼çš„å¤©æ•°
                        fire_days = np.where(firms_channel >= self.min_fire_threshold)[0]
                        
                        # ğŸ” å¦‚æœæ‰¾åˆ°ç«ç¾å¤©ï¼Œæ·»åŠ è°ƒè¯•ä¿¡æ¯
                        if len(fire_days) > 0:
                            logger.info(f"åƒç´  {pixel_name}: æ‰¾åˆ° {len(fire_days)} ä¸ªç«ç¾å¤©ï¼ŒFIRMSèŒƒå›´: {min_firms:.2f}-{max_firms:.2f}")
                        
                        # ä¸ºæ¯ä¸ªç«ç¾å¤©åˆ›å»ºæ­£æ ·æœ¬
                        for day_idx in fire_days:
                            actual_date = start_date + timedelta(days=int(day_idx))
                            year_positive_samples.append({
                                'year': year,
                                'day_of_year': int(day_idx),
                                'date': actual_date,
                                'pixel_row': row,
                                'pixel_col': col,
                                'firms_value': float(firms_channel[day_idx]),
                                'file_path': file_path
                            })
                            
                    except ValueError:
                        # è·³è¿‡éåƒç´ æ•°æ®é›†
                        continue
                    except Exception as e:
                        logger.warning(f"å¤„ç†åƒç´  {pixel_name} æ—¶å‡ºé”™: {e}")
                        continue
                        
        except Exception as e:
            logger.error(f"å¤„ç†å¹´ä»½ {year} æ—¶å‡ºé”™: {e}")
        
        # ğŸ” æ·»åŠ æ±‡æ€»ä¿¡æ¯
        logger.info(f"å¹´ä»½ {year}: æ€»å…±æ‰¾åˆ° {len(year_positive_samples)} ä¸ªæ­£æ ·æœ¬")
        
        return year_positive_samples

    def _find_positive_samples(self):
        """æŸ¥æ‰¾æ­£æ ·æœ¬ - ä¼˜åŒ–ç‰ˆæœ¬"""
        logger.info("æ­£åœ¨æŸ¥æ‰¾æ­£æ ·æœ¬...")
        
        positive_samples = []
        
        # ä½¿ç”¨çº¿ç¨‹æ± è€Œä¸æ˜¯è¿›ç¨‹æ± ï¼Œé¿å…pickleé—®é¢˜
        from concurrent.futures import ThreadPoolExecutor, as_completed
        
        # å¹¶è¡Œå¤„ç†æ‰€æœ‰å¹´ä»½ (é™åˆ¶å¹¶è¡Œæ•°é¿å…å†…å­˜è¿‡è½½)
        max_workers = min(4, len(self.year_files), os.cpu_count() or 1)
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = [
                executor.submit(self._process_year_for_positive_samples, (year, file_path))
                for year, file_path in self.year_files.items()
            ]
            
            for future in tqdm(as_completed(futures), total=len(futures), desc="æ‰«ææ­£æ ·æœ¬"):
                try:
                    year_results = future.result()
                    positive_samples.extend(year_results)
                except Exception as e:
                    logger.error(f"å¤„ç†å¹´ä»½æ—¶å‡ºé”™: {e}")
        
        logger.info(f"æ‰¾åˆ° {len(positive_samples)} ä¸ªæ­£æ ·æœ¬")
        return positive_samples
    
    def _generate_negative_samples(self, global_no_fire_days, num_negative_samples):
        """ç”Ÿæˆè´Ÿæ ·æœ¬ - ä¼˜åŒ–ç‰ˆæœ¬"""
        logger.info(f"æ­£åœ¨ç”Ÿæˆ {num_negative_samples} ä¸ªè´Ÿæ ·æœ¬...")
        
        if not global_no_fire_days:
            logger.warning("æ²¡æœ‰å…¨å±€æ— ç«æ—¥æœŸï¼Œæ— æ³•ç”Ÿæˆè´Ÿæ ·æœ¬")
            return []
        
        negative_samples = []
        
        # é¢„å…ˆç¼“å­˜æ¯ä¸ªå¹´ä»½çš„åƒç´ åˆ—è¡¨ï¼Œé¿å…é‡å¤è¯»å–
        year_pixels_cache = {}
        
        logger.info("é¢„ç¼“å­˜å¹´ä»½åƒç´ ä¿¡æ¯...")
        for year, file_path in tqdm(self.year_files.items(), desc="ç¼“å­˜åƒç´ ä¿¡æ¯"):
            try:
                with h5py.File(file_path, 'r') as f:
                    pixels = []
                    for dataset_name in f.keys():
                        if '_' in dataset_name:
                            try:
                                row, col = map(int, dataset_name.split('_'))
                                pixels.append((row, col))
                            except ValueError:
                                continue
                    
                    if pixels:
                        year_pixels_cache[year] = pixels
                        
            except Exception as e:
                logger.warning(f"ç¼“å­˜å¹´ä»½ {year} åƒç´ ä¿¡æ¯æ—¶å‡ºé”™: {e}")
                continue
        
        if not year_pixels_cache:
            logger.warning("æ²¡æœ‰å¯ç”¨çš„åƒç´ ä½ç½®ï¼Œæ— æ³•ç”Ÿæˆè´Ÿæ ·æœ¬")
            return []
        
        # æ„å»ºå¯ç”¨çš„æ— ç«æ—¥æœŸåˆ—è¡¨ï¼ˆåªåŒ…å«æœ‰åƒç´ æ•°æ®çš„å¹´ä»½ï¼‰
        valid_no_fire_days = []
        for day_info in global_no_fire_days:
            if day_info['year'] in year_pixels_cache:
                valid_no_fire_days.append(day_info)
        
        if not valid_no_fire_days:
            logger.warning("æ²¡æœ‰æœ‰æ•ˆçš„æ— ç«æ—¥æœŸï¼Œæ— æ³•ç”Ÿæˆè´Ÿæ ·æœ¬")
            return []
        
        # éšæœºé‡‡æ ·è´Ÿæ ·æœ¬
        random.seed(42)  # ç¡®ä¿å¯é‡å¤æ€§
        
        logger.info(f"ä» {len(valid_no_fire_days)} ä¸ªæœ‰æ•ˆæ— ç«æ—¥æœŸä¸­éšæœºé‡‡æ ·...")
        for _ in tqdm(range(num_negative_samples), desc="ç”Ÿæˆè´Ÿæ ·æœ¬"):
            # éšæœºé€‰æ‹©ä¸€ä¸ªæ— ç«æ—¥æœŸ
            day_info = random.choice(valid_no_fire_days)
            
            # éšæœºé€‰æ‹©ä¸€ä¸ªåƒç´ ä½ç½®
            available_pixels = year_pixels_cache[day_info['year']]
            pixel_row, pixel_col = random.choice(available_pixels)
            
            negative_samples.append({
                'year': day_info['year'],
                'day_of_year': day_info['day_of_year'],
                'date': day_info['date'],
                'pixel_row': pixel_row,
                'pixel_col': pixel_col,
                'firms_value': 0.0,  # è´Ÿæ ·æœ¬FIRMSå€¼ä¸º0
                'file_path': day_info['file_path']
            })
        
        logger.info(f"ç”Ÿæˆ {len(negative_samples)} ä¸ªè´Ÿæ ·æœ¬")
        return negative_samples
    
    def _sample_data_with_cache(self):
        """é‡‡æ ·æ•°æ®ï¼Œä½¿ç”¨ç¼“å­˜æœºåˆ¶"""
        cache_file = self._get_cache_filename()
        
        # æ£€æŸ¥ç¼“å­˜æ˜¯å¦æœ‰æ•ˆ
        if self._check_cache_validity(cache_file):
            logger.info(f"ğŸš€ ä½¿ç”¨ç¼“å­˜æ–‡ä»¶: {cache_file}")
            return self._load_samples_from_cache(cache_file)
        
        # ç¼“å­˜æ— æ•ˆï¼Œé‡æ–°é‡‡æ ·
        logger.info("ğŸ’¾ ç¼“å­˜æ— æ•ˆï¼Œå¼€å§‹é‡æ–°é‡‡æ ·...")
        
        # è®°å½•å¼€å§‹æ—¶é—´
        start_time = time.time()
        
        # æŸ¥æ‰¾å…¨å±€æ— ç«æ—¥æœŸ
        logger.info("ğŸ” æ­¥éª¤1: æŸ¥æ‰¾å…¨å±€æ— ç«æ—¥æœŸ...")
        step_start = time.time()
        global_no_fire_days = self._find_global_no_fire_days()
        step_duration = time.time() - step_start
        logger.info(f"âœ… å®Œæˆï¼Œè€—æ—¶: {step_duration:.2f}ç§’")
        
        # æŸ¥æ‰¾æ­£æ ·æœ¬
        logger.info("ğŸ” æ­¥éª¤2: æŸ¥æ‰¾æ­£æ ·æœ¬...")
        step_start = time.time()
        positive_samples = self._find_positive_samples()
        step_duration = time.time() - step_start
        logger.info(f"âœ… å®Œæˆï¼Œè€—æ—¶: {step_duration:.2f}ç§’")
        
        if not positive_samples:
            raise ValueError("æœªæ‰¾åˆ°ä»»ä½•æ­£æ ·æœ¬")
        
        # ç”Ÿæˆè´Ÿæ ·æœ¬
        logger.info("ğŸ” æ­¥éª¤3: ç”Ÿæˆè´Ÿæ ·æœ¬...")
        step_start = time.time()
        num_negative_samples = len(positive_samples) * 2  # é»˜è®¤2å€è´Ÿæ ·æœ¬
        negative_samples = self._generate_negative_samples(global_no_fire_days, num_negative_samples)
        step_duration = time.time() - step_start
        logger.info(f"âœ… å®Œæˆï¼Œè€—æ—¶: {step_duration:.2f}ç§’")
        
        # ä¿å­˜åˆ°ç¼“å­˜
        logger.info("ğŸ’¾ æ­¥éª¤4: ä¿å­˜åˆ°ç¼“å­˜...")
        step_start = time.time()
        self._save_samples_to_cache(cache_file, positive_samples, negative_samples, global_no_fire_days)
        step_duration = time.time() - step_start
        logger.info(f"âœ… å®Œæˆï¼Œè€—æ—¶: {step_duration:.2f}ç§’")
        
        total_duration = time.time() - start_time
        logger.info(f"ğŸ‰ é‡‡æ ·å®Œæˆï¼æ€»è€—æ—¶: {total_duration:.2f}ç§’")
        logger.info(f"ğŸ“Š é‡‡æ ·ç»“æœ: {len(positive_samples)} æ­£æ ·æœ¬, {len(negative_samples)} è´Ÿæ ·æœ¬")
        
        return positive_samples, negative_samples
    
    def _load_samples_from_cache(self, cache_file):
        """ä»ç¼“å­˜åŠ è½½æ ·æœ¬ - ä¼˜åŒ–ç‰ˆæœ¬"""
        import time
        start_time = time.time()
        
        try:
            with h5py.File(cache_file, 'r') as f:
                positive_samples = []
                negative_samples = []
                
                # æ‰¹é‡åŠ è½½æ­£æ ·æœ¬æ•°æ®
                if 'positive_samples' in f:
                    pos_group = f['positive_samples']
                    # ä¸€æ¬¡æ€§è¯»å–æ‰€æœ‰æ•°ç»„
                    pos_years = pos_group['year'][:]
                    pos_days = pos_group['day_of_year'][:]
                    pos_dates = pos_group['date'][:]
                    pos_rows = pos_group['pixel_row'][:]
                    pos_cols = pos_group['pixel_col'][:]
                    pos_firms = pos_group['firms_value'][:]
                    
                    # å‘é‡åŒ–å¤„ç† - é¢„è§£ç å­—ç¬¦ä¸²
                    decoded_dates = [pos_dates[i].decode() for i in range(len(pos_dates))]
                    for i in range(len(pos_years)):
                        year = int(pos_years[i])
                        positive_samples.append({
                            'year': year,
                            'day_of_year': int(pos_days[i]),
                            'date': datetime.strptime(decoded_dates[i], '%Y-%m-%d'),
                            'pixel_row': int(pos_rows[i]),
                            'pixel_col': int(pos_cols[i]),
                            'firms_value': float(pos_firms[i]),
                            'file_path': self.year_files[year]
                        })
                
                # æ‰¹é‡åŠ è½½è´Ÿæ ·æœ¬æ•°æ®
                if 'negative_samples' in f:
                    neg_group = f['negative_samples']
                    # ä¸€æ¬¡æ€§è¯»å–æ‰€æœ‰æ•°ç»„
                    neg_years = neg_group['year'][:]
                    neg_days = neg_group['day_of_year'][:]
                    neg_dates = neg_group['date'][:]
                    neg_rows = neg_group['pixel_row'][:]
                    neg_cols = neg_group['pixel_col'][:]
                    neg_firms = neg_group['firms_value'][:]
                    
                    # å‘é‡åŒ–å¤„ç† - é¢„è§£ç å­—ç¬¦ä¸²
                    decoded_dates = [neg_dates[i].decode() for i in range(len(neg_dates))]
                    for i in range(len(neg_years)):
                        year = int(neg_years[i])
                        negative_samples.append({
                            'year': year,
                            'day_of_year': int(neg_days[i]),
                            'date': datetime.strptime(decoded_dates[i], '%Y-%m-%d'),
                            'pixel_row': int(neg_rows[i]),
                            'pixel_col': int(neg_cols[i]),
                            'firms_value': float(neg_firms[i]),
                            'file_path': self.year_files[year]
                        })
                
                load_time = time.time() - start_time
                logger.info(f"ä»ç¼“å­˜åŠ è½½æ ·æœ¬: {len(positive_samples)} æ­£æ ·æœ¬, {len(negative_samples)} è´Ÿæ ·æœ¬ (è€—æ—¶: {load_time:.2f}ç§’)")
                return positive_samples, negative_samples
                
        except Exception as e:
            logger.error(f"ä»ç¼“å­˜åŠ è½½æ ·æœ¬å¤±è´¥: {e}")
            # åˆ é™¤æ— æ•ˆç¼“å­˜
            try:
                os.remove(cache_file)
            except:
                pass
            
            # é‡æ–°é‡‡æ ·
            return self._sample_data_with_cache()
    
    def _save_samples_to_cache(self, cache_file, positive_samples, negative_samples, global_no_fire_days):
        """ä¿å­˜æ ·æœ¬åˆ°ç¼“å­˜"""
        try:
            with h5py.File(cache_file, 'w') as f:
                # ä¿å­˜ç¼“å­˜å‚æ•°
                cache_params = {
                    'h5_dir': self.h5_dir,
                    'years': sorted(self.year_files.keys()),
                    'lookback_seq': self.lookback_seq,
                    'forecast_hor': self.forecast_hor,
                    'min_fire_threshold': self.min_fire_threshold,
                    'version': '1.0'
                }
                f.attrs['cache_params'] = json.dumps(cache_params)
                
                # ä¿å­˜æºæ–‡ä»¶ä¿®æ”¹æ—¶é—´
                source_file_mtimes = {}
                for year, file_path in self.year_files.items():
                    if os.path.exists(file_path):
                        source_file_mtimes[str(year)] = os.path.getmtime(file_path)
                f.attrs['source_file_mtimes'] = json.dumps(source_file_mtimes)
                
                # ä¿å­˜å…ƒæ•°æ®
                f.attrs['total_positive'] = len(positive_samples)
                f.attrs['total_negative'] = len(negative_samples)
                f.attrs['global_no_fire_days'] = len(global_no_fire_days)
                f.attrs['creation_time'] = datetime.now().isoformat()
                
                # ä¿å­˜æ­£æ ·æœ¬
                if positive_samples:
                    pos_group = f.create_group('positive_samples')
                    pos_group.create_dataset('year', data=[s['year'] for s in positive_samples])
                    pos_group.create_dataset('day_of_year', data=[s['day_of_year'] for s in positive_samples])
                    pos_group.create_dataset('date', data=[s['date'].strftime('%Y-%m-%d').encode() for s in positive_samples])
                    pos_group.create_dataset('pixel_row', data=[s['pixel_row'] for s in positive_samples])
                    pos_group.create_dataset('pixel_col', data=[s['pixel_col'] for s in positive_samples])
                    pos_group.create_dataset('firms_value', data=[s['firms_value'] for s in positive_samples])
                
                # ä¿å­˜è´Ÿæ ·æœ¬
                if negative_samples:
                    neg_group = f.create_group('negative_samples')
                    neg_group.create_dataset('year', data=[s['year'] for s in negative_samples])
                    neg_group.create_dataset('day_of_year', data=[s['day_of_year'] for s in negative_samples])
                    neg_group.create_dataset('date', data=[s['date'].strftime('%Y-%m-%d').encode() for s in negative_samples])
                    neg_group.create_dataset('pixel_row', data=[s['pixel_row'] for s in negative_samples])
                    neg_group.create_dataset('pixel_col', data=[s['pixel_col'] for s in negative_samples])
                    neg_group.create_dataset('firms_value', data=[s['firms_value'] for s in negative_samples])
                
            logger.info(f"æ ·æœ¬å·²ä¿å­˜åˆ°ç¼“å­˜: {cache_file}")
            
        except Exception as e:
            logger.error(f"ä¿å­˜æ ·æœ¬åˆ°ç¼“å­˜å¤±è´¥: {e}")
            # åˆ é™¤æŸåçš„ç¼“å­˜æ–‡ä»¶
            try:
                os.remove(cache_file)
            except:
                pass
    
    def _build_or_load_samples(self):
        """æ„å»ºæˆ–åŠ è½½æ ·æœ¬ç´¢å¼•"""
        positive_samples, negative_samples = self._sample_data_with_cache()
        
        # åˆå¹¶æ ·æœ¬å¹¶æ„å»ºç´¢å¼•
        all_samples = positive_samples + negative_samples
        
        self.sample_index = []
        self.dataset_info = {}
        
        # æ‰¹é‡å¤„ç†æ ·æœ¬ï¼Œå‡å°‘é‡å¤è®¡ç®—
        valid_samples = []
        for sample in all_samples:
            # æ£€æŸ¥æ ·æœ¬æ˜¯å¦æœ‰æ•ˆï¼ˆå†å²å’Œæœªæ¥æ•°æ®å……è¶³ï¼‰
            if self._is_sample_valid(sample):
                valid_samples.append(sample)
        
        # æ‰¹é‡æ„å»ºæ ·æœ¬ç´¢å¼•
        for sample in valid_samples:
            pixel_row = sample['pixel_row']
            pixel_col = sample['pixel_col']
            date_obj = sample['date']
            
            sample_metadata = {
                'year': sample['year'],
                'day_of_year': sample['day_of_year'],
                'date': date_obj,
                'date_int': int(date_obj.strftime('%Y%m%d')),
                'pixel_row': pixel_row,
                'pixel_col': pixel_col,
                'firms_value': sample['firms_value'],
                'row': pixel_row,  # å…¼å®¹åŸæ¥å£
                'col': pixel_col,  # å…¼å®¹åŸæ¥å£
                'sample_type': 'positive' if sample['firms_value'] >= self.min_fire_threshold else 'negative'
            }
            
            self.sample_index.append((sample['file_path'], f"{pixel_row}_{pixel_col}", sample_metadata))
        
        # æ„å»ºæ•°æ®é›†ä¿¡æ¯
        for year, file_path in self.year_files.items():
            try:
                with h5py.File(file_path, 'r') as f:
                    self.dataset_info[file_path] = {
                        'year': year,
                        'total_time_steps': int(f.attrs.get('total_time_steps', 365)),
                        'total_channels': int(f.attrs.get('total_channels', 39)),
                        'past_days': self.lookback_seq,
                        'future_days': self.forecast_hor
                    }
            except Exception as e:
                logger.warning(f"è¯»å–æ•°æ®é›†ä¿¡æ¯å¤±è´¥: {file_path}, {e}")
    
    def _is_sample_valid(self, sample):
        """æ£€æŸ¥æ ·æœ¬æ˜¯å¦æœ‰æ•ˆï¼ˆå†å²å’Œæœªæ¥æ•°æ®å……è¶³ï¼‰"""
        target_date = sample['date']
        
        # æ£€æŸ¥å†å²æ•°æ®
        history_start = target_date - timedelta(days=self.lookback_seq - 1)
        if history_start.year < min(self.year_files.keys()):
            return False
        
        # æ£€æŸ¥æœªæ¥æ•°æ®
        future_end = target_date + timedelta(days=self.forecast_hor - 1)
        if future_end.year > max(self.year_files.keys()):
            return False
        
        return True
    
    def _load_pixel_data_for_date_range(self, pixel_row, pixel_col, start_date, end_date):
        """åŠ è½½æŒ‡å®šåƒç´ åœ¨æŒ‡å®šæ—¥æœŸèŒƒå›´å†…çš„æ•°æ®"""
        data_segments = []
        
        current_date = start_date
        while current_date <= end_date:
            year = current_date.year
            
            if year not in self.year_files:
                # ç”¨NaNå¡«å……ç¼ºå¤±å¹´ä»½
                data_segments.append(np.full((39, 1), np.nan))
                current_date += timedelta(days=1)
                continue
            
            try:
                with h5py.File(self.year_files[year], 'r') as f:
                    dataset_name = f"{pixel_row}_{pixel_col}"
                    
                    if dataset_name not in f:
                        # åƒç´ ä¸å­˜åœ¨ï¼Œç”¨NaNå¡«å……
                        data_segments.append(np.full((39, 1), np.nan))
                        current_date += timedelta(days=1)
                        continue
                    
                    pixel_data = f[dataset_name][:]  # shape: (channels, time_steps)
                    
                    # è®¡ç®—åœ¨å¹´ä»½å†…çš„ç´¢å¼•
                    year_start = datetime(year, 1, 1)
                    day_of_year = (current_date - year_start).days
                    
                    if day_of_year >= pixel_data.shape[1]:
                        # è¶…å‡ºå¹´ä»½èŒƒå›´ï¼Œç”¨NaNå¡«å……
                        data_segments.append(np.full((39, 1), np.nan))
                    else:
                        # æå–å½“å¤©æ•°æ®
                        day_data = pixel_data[:, day_of_year:day_of_year+1]
                        data_segments.append(day_data)
                        
            except Exception as e:
                logger.warning(f"åŠ è½½åƒç´ æ•°æ®å¤±è´¥: {pixel_row}_{pixel_col}, {current_date}, {e}")
                data_segments.append(np.full((39, 1), np.nan))
            
            current_date += timedelta(days=1)
        
        # åˆå¹¶æ‰€æœ‰æ•°æ®æ®µ
        if data_segments:
            return np.concatenate(data_segments, axis=1)
        else:
            return np.full((39, 0), np.nan)
    
    def __len__(self):
        """è¿”å›æ ·æœ¬æ•°é‡"""
        return len(self.sample_index)
    
    def __getitem__(self, idx):
        """
        è·å–å•ä¸ªæ ·æœ¬
        
        Returns:
            - past_data: (channels, past_time_steps) 
            - future_data: (channels, future_time_steps)
            - metadata (å¯é€‰): [date_int, row, col]
        """
        file_path, dataset_key, metadata = self.sample_index[idx]
        
        try:
            target_date = metadata['date']
            pixel_row = metadata['pixel_row']
            pixel_col = metadata['pixel_col']
            
            # è®¡ç®—å†å²æ•°æ®èŒƒå›´
            history_start = target_date - timedelta(days=self.lookback_seq - 1)
            history_end = target_date - timedelta(days=1)
            
            # è®¡ç®—æœªæ¥æ•°æ®èŒƒå›´ï¼ˆåŒ…å«å½“å¤©ï¼‰
            future_start = target_date
            future_end = target_date + timedelta(days=self.forecast_hor - 1)
            
            # åŠ è½½å†å²æ•°æ®
            past_data = self._load_pixel_data_for_date_range(pixel_row, pixel_col, history_start, history_end)
            
            # åŠ è½½æœªæ¥æ•°æ®
            future_data = self._load_pixel_data_for_date_range(pixel_row, pixel_col, future_start, future_end)
            
            # æ£€æŸ¥æ•°æ®ç»´åº¦å¹¶å¤„ç†ä¸åŒ¹é…æƒ…å†µ
            if past_data.shape[1] != self.lookback_seq:
                # å¤„ç†å†å²æ•°æ®é•¿åº¦ä¸åŒ¹é…ï¼ˆå¯èƒ½æ˜¯é—°å¹´/å¹³å¹´å¯¼è‡´ï¼‰
                if past_data.shape[1] < self.lookback_seq:
                    # æ•°æ®ä¸è¶³ï¼Œåœ¨å¼€å¤´å¡«å……
                    padding_days = self.lookback_seq - past_data.shape[1]
                    padding = np.zeros((39, padding_days))
                    past_data = np.concatenate([padding, past_data], axis=1)
                    logger.debug(f"å†å²æ•°æ®é•¿åº¦ä¸è¶³: æœŸæœ›{self.lookback_seq}, å®é™…{past_data.shape[1]-padding_days}, å·²å¡«å……{padding_days}å¤©")
                else:
                    # æ•°æ®è¿‡å¤šï¼Œæˆªå–æœ€è¿‘çš„éƒ¨åˆ†
                    past_data = past_data[:, -self.lookback_seq:]
                    logger.debug(f"å†å²æ•°æ®è¿‡é•¿: æœŸæœ›{self.lookback_seq}, å®é™…{past_data.shape[1]}, å·²æˆªå–æœ€è¿‘{self.lookback_seq}å¤©")
            
            if future_data.shape[1] != self.forecast_hor:
                # å¤„ç†æœªæ¥æ•°æ®é•¿åº¦ä¸åŒ¹é…
                if future_data.shape[1] < self.forecast_hor:
                    # æ•°æ®ä¸è¶³ï¼Œåœ¨æœ«å°¾å¡«å……
                    padding_days = self.forecast_hor - future_data.shape[1]
                    padding = np.zeros((39, padding_days))
                    future_data = np.concatenate([future_data, padding], axis=1)
                    logger.debug(f"æœªæ¥æ•°æ®é•¿åº¦ä¸è¶³: æœŸæœ›{self.forecast_hor}, å®é™…{future_data.shape[1]-padding_days}, å·²å¡«å……{padding_days}å¤©")
                else:
                    # æ•°æ®è¿‡å¤šï¼Œæˆªå–å‰é¢éƒ¨åˆ†
                    future_data = future_data[:, :self.forecast_hor]
                    logger.debug(f"æœªæ¥æ•°æ®è¿‡é•¿: æœŸæœ›{self.forecast_hor}, å®é™…{future_data.shape[1]}, å·²æˆªå–å‰{self.forecast_hor}å¤©")
            
            # å¤„ç†NaNå€¼
            past_data = np.nan_to_num(past_data, nan=0.0, posinf=0.0, neginf=0.0)
            future_data = np.nan_to_num(future_data, nan=0.0, posinf=0.0, neginf=0.0)
            
            # è½¬æ¢ä¸ºtorch tensor
            past_data = torch.from_numpy(past_data).float()
            future_data = torch.from_numpy(future_data).float()
            
            # æœ€ç»ˆæ£€æŸ¥
            if torch.isnan(past_data).any() or torch.isinf(past_data).any():
                past_data = torch.nan_to_num(past_data, nan=0.0, posinf=0.0, neginf=0.0)
            
            if torch.isnan(future_data).any() or torch.isinf(future_data).any():
                future_data = torch.nan_to_num(future_data, nan=0.0, posinf=0.0, neginf=0.0)
            
            if self.return_metadata:
                # è¿”å›ç®€åŒ–çš„metadataæ ¼å¼: [æ—¥æœŸ, xåæ ‡, yåæ ‡]
                simplified_metadata = [metadata['date_int'], metadata['row'], metadata['col']]
                return past_data, future_data, simplified_metadata
            else:
                return past_data, future_data
                
        except Exception as e:
            logger.debug(f"è·å–æ ·æœ¬å¤±è´¥: {dataset_key}, {e}")
            raise e
    
    def custom_collate_fn(self, batch):
        """
        è‡ªå®šä¹‰çš„collateå‡½æ•°ï¼Œä¸åŸdataload.pyå®Œå…¨ä¸€è‡´
        """
        if self.return_metadata:
            past_data_list, future_data_list, metadata_list = zip(*batch)
        else:
            past_data_list, future_data_list = zip(*batch)
        
        # æ£€æŸ¥æ‰€æœ‰tensorçš„å½¢çŠ¶
        past_shapes = [data.shape for data in past_data_list]
        future_shapes = [data.shape for data in future_data_list]
        
        # æ£€æŸ¥pastæ•°æ®å½¢çŠ¶ä¸€è‡´æ€§
        if len(set(past_shapes)) > 1:
            raise ValueError(f"Pastæ•°æ®å½¢çŠ¶ä¸ä¸€è‡´: {set(past_shapes)}")
        
        # æ£€æŸ¥futureæ•°æ®å½¢çŠ¶ä¸€è‡´æ€§
        if len(set(future_shapes)) > 1:
            raise ValueError(f"Futureæ•°æ®å½¢çŠ¶ä¸ä¸€è‡´: {set(future_shapes)}")
        
        # ç›´æ¥å †å ï¼Œå½¢çŠ¶å¿…é¡»ä¸€è‡´
        past_batch = torch.stack(past_data_list, dim=0)
        future_batch = torch.stack(future_data_list, dim=0)
        
        if self.return_metadata:
            return past_batch, future_batch, metadata_list
        else:
            return past_batch, future_batch
    
    def resample_for_epoch(self, epoch_seed):
        """
        ä¸ºæ–°çš„epoché‡æ–°è¿›è¡Œæ ·æœ¬æŠ½æ ·
        """
        if not self.resample_each_epoch:
            return
        
        if not hasattr(self, 'full_sample_index') or not self.full_sample_index:
            logger.warning("æ— æ³•é‡æ–°æŠ½æ ·ï¼šæ²¡æœ‰å®Œæ•´æ ·æœ¬ç´¢å¼•")
            return
        
        self.epoch_seed = epoch_seed
        
        # ä½¿ç”¨å®Œæ•´æ ·æœ¬ç´¢å¼•é‡æ–°æŠ½æ ·
        self.sample_index = self.full_sample_index.copy()
        
        # ä½¿ç”¨æ–°çš„éšæœºç§å­é‡æ–°æŠ½æ ·
        self._apply_sample_ratio_filtering(seed=epoch_seed)
    
    def _apply_sample_ratio_filtering(self, seed=None):
        """åº”ç”¨æ­£è´Ÿæ ·æœ¬æ¯”ä¾‹ç­›é€‰"""
        if seed is None:
            seed = 42
        
        if self.verbose_sampling:
            logger.info(f"å¼€å§‹åº”ç”¨æ­£è´Ÿæ ·æœ¬æ¯”ä¾‹ç­›é€‰... (éšæœºç§å­: {seed})")
        
        # åˆ†ç¦»æ­£æ ·æœ¬å’Œè´Ÿæ ·æœ¬
        positive_samples = []
        negative_samples = []
        
        for idx, (file_path, dataset_key, metadata) in enumerate(self.sample_index):
            if metadata['firms_value'] >= self.min_fire_threshold:
                positive_samples.append(idx)
            else:
                negative_samples.append(idx)
        
        positive_count = len(positive_samples)
        negative_count = len(negative_samples)
        
        if self.verbose_sampling:
            logger.info(f"åŸå§‹æ ·æœ¬ç»Ÿè®¡: æ­£æ ·æœ¬ {positive_count} ä¸ª, è´Ÿæ ·æœ¬ {negative_count} ä¸ª")
        
        # è®¡ç®—éœ€è¦ä¿ç•™çš„æ ·æœ¬æ•°
        retained_positive_count = int(positive_count * self.positive_ratio)
        retained_negative_count = int(retained_positive_count * self.pos_neg_ratio)
        
        # ç¡®ä¿ä¸è¶…è¿‡å¯ç”¨æ•°é‡
        retained_negative_count = min(retained_negative_count, negative_count)
        
        if self.verbose_sampling:
            logger.info(f"è®¡åˆ’ä¿ç•™: æ­£æ ·æœ¬ {retained_positive_count} ä¸ª, è´Ÿæ ·æœ¬ {retained_negative_count} ä¸ª")
        
        # éšæœºæŠ½æ ·
        random.seed(seed)
        
        selected_positive_indices = random.sample(positive_samples, retained_positive_count) if retained_positive_count < len(positive_samples) else positive_samples
        selected_negative_indices = random.sample(negative_samples, retained_negative_count) if retained_negative_count < len(negative_samples) else negative_samples
        
        # åˆå¹¶å¹¶é‡å»ºç´¢å¼•
        selected_indices = selected_positive_indices + selected_negative_indices
        
        new_sample_index = []
        for idx in selected_indices:
            new_sample_index.append(self.sample_index[idx])
        
        self.sample_index = new_sample_index
        
        if self.verbose_sampling:
            logger.info(f"æ ·æœ¬ç­›é€‰å®Œæˆ: æ­£æ ·æœ¬ {len(selected_positive_indices)} ä¸ª, è´Ÿæ ·æœ¬ {len(selected_negative_indices)} ä¸ª")
    
    def get_current_sample_stats(self):
        """è·å–å½“å‰æ ·æœ¬çš„ç»Ÿè®¡ä¿¡æ¯"""
        positive_count = 0
        negative_count = 0
        
        for _, _, metadata in self.sample_index:
            if metadata['firms_value'] >= self.min_fire_threshold:
                positive_count += 1
            else:
                negative_count += 1
        
        return {
            'total_samples': len(self.sample_index),
            'positive_samples': positive_count,
            'negative_samples': negative_count,
            'positive_ratio': positive_count / len(self.sample_index) if len(self.sample_index) > 0 else 0,
            'pos_neg_ratio': negative_count / positive_count if positive_count > 0 else 0
        }
    
    def get_statistics(self):
        """è·å–æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            'total_samples': len(self.sample_index),
            'years': set(),
            'firms_values': set(),
            'sample_types': set(),
            'files': len(self.year_files)
        }
        
        for file_path, dataset_key, metadata in self.sample_index:
            stats['years'].add(metadata['year'])
            stats['firms_values'].add(metadata['firms_value'])
            stats['sample_types'].add(metadata['sample_type'])
        
        # è½¬æ¢ä¸ºåˆ—è¡¨å¹¶æ’åº
        stats['years'] = sorted(list(stats['years']))
        stats['firms_values'] = sorted(list(stats['firms_values']))
        stats['sample_types'] = sorted(list(stats['sample_types']))
        
        return stats
    
    def get_dataset_info(self):
        """è·å–æ•°æ®é›†ä¿¡æ¯"""
        return self.dataset_info


class YearTimeSeriesDataLoader:
    """
    å¹´ä»½æ—¶é—´åºåˆ—æ•°æ®åŠ è½½å™¨ - ä¸åŸTimeSeriesDataLoaderå®Œå…¨å…¼å®¹
    """
    
    def __init__(self, h5_dir, positive_ratio=1.0, pos_neg_ratio=1.0, 
                 resample_each_epoch=False, verbose_sampling=True,
                 lookback_seq=365, forecast_hor=7, min_fire_threshold=0.001,
                 cache_dir=None, force_resample=False, **kwargs):
        """
        åˆå§‹åŒ–å¹´ä»½æ—¶é—´åºåˆ—æ•°æ®åŠ è½½å™¨
        
        Args:
            h5_dir: å¹´ä»½æ•°æ®H5æ–‡ä»¶ç›®å½•
            positive_ratio: æ­£æ ·æœ¬ä½¿ç”¨æ¯”ä¾‹
            pos_neg_ratio: æ­£è´Ÿæ ·æœ¬æ¯”ä¾‹
            resample_each_epoch: æ˜¯å¦åœ¨æ¯ä¸ªepoché‡æ–°æŠ½æ ·
            verbose_sampling: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†é‡‡æ ·ä¿¡æ¯
            lookback_seq: å†å²æ—¶é—´é•¿åº¦
            forecast_hor: æœªæ¥æ—¶é—´é•¿åº¦
            min_fire_threshold: FIRMSé˜ˆå€¼
            cache_dir: ç¼“å­˜ç›®å½•
            force_resample: æ˜¯å¦å¼ºåˆ¶é‡æ–°é‡‡æ ·
            **kwargs: å…¶ä»–å‚æ•°ï¼ˆä¸ºäº†å…¼å®¹æ€§ï¼‰
        """
        self.h5_dir = h5_dir
        self.positive_ratio = positive_ratio
        self.pos_neg_ratio = pos_neg_ratio
        self.resample_each_epoch = resample_each_epoch
        self.verbose_sampling = verbose_sampling
        self.lookback_seq = lookback_seq
        self.forecast_hor = forecast_hor
        self.min_fire_threshold = min_fire_threshold
        self.cache_dir = cache_dir
        self.force_resample = force_resample
        
        # åˆ›å»ºæ•°æ®é›†
        self.dataset = YearTimeSeriesPixelDataset(
            h5_dir=h5_dir,
            positive_ratio=positive_ratio,
            pos_neg_ratio=pos_neg_ratio,
            resample_each_epoch=resample_each_epoch,
            verbose_sampling=verbose_sampling,
            lookback_seq=lookback_seq,
            forecast_hor=forecast_hor,
            min_fire_threshold=min_fire_threshold,
            cache_dir=cache_dir,
            force_resample=force_resample
        )
        
        logger.info(f"å¹´ä»½æ—¶é—´åºåˆ—æ•°æ®åŠ è½½å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def get_year_based_split(self, train_years, val_years, test_years, test_full_years=None):
        """
        åŸºäºå¹´ä»½è¿›è¡Œæ•°æ®åˆ’åˆ† - ä¸åŸæ¥å£å®Œå…¨ä¸€è‡´
        """
        train_indices = []
        val_indices = []
        test_indices = []
        
        for idx, (file_path, dataset_key, metadata) in enumerate(self.dataset.sample_index):
            year = metadata['year']
            
            if year in train_years:
                train_indices.append(idx)
            elif year in val_years:
                val_indices.append(idx)
            elif year in test_years:
                test_indices.append(idx)
        
        if self.verbose_sampling:
            logger.info(f"å¹´ä»½åˆ’åˆ†ç»“æœ:")
            logger.info(f"  è®­ç»ƒé›†: {len(train_indices)} æ ·æœ¬ (å¹´ä»½: {train_years})")
            logger.info(f"  éªŒè¯é›†: {len(val_indices)} æ ·æœ¬ (å¹´ä»½: {val_years})")
            logger.info(f"  æµ‹è¯•é›†: {len(test_indices)} æ ·æœ¬ (å¹´ä»½: {test_years})")
        
        # å¦‚æœéœ€è¦å®Œæ•´æ•°æ®æµ‹è¯•é›†
        if test_full_years is not None:
            full_dataset = YearTimeSeriesPixelDataset(
                h5_dir=self.h5_dir,
                years=test_full_years,
                positive_ratio=1.0,
                pos_neg_ratio=999999,  # ä½¿ç”¨æ‰€æœ‰è´Ÿæ ·æœ¬
                resample_each_epoch=False,
                verbose_sampling=self.verbose_sampling,
                lookback_seq=self.lookback_seq,
                forecast_hor=self.forecast_hor,
                min_fire_threshold=self.min_fire_threshold,
                cache_dir=self.cache_dir,
                force_resample=self.force_resample
            )
            
            test_full_indices = list(range(len(full_dataset)))
            logger.info(f"å®Œæ•´æ•°æ®æµ‹è¯•é›†: {len(test_full_indices)} æ ·æœ¬ (å¹´ä»½: {test_full_years})")
            
            return train_indices, val_indices, test_indices, test_full_indices, full_dataset
        
        return train_indices, val_indices, test_indices


class YearFullDatasetLoader:
    """
    å¹´ä»½å®Œæ•´æ•°æ®é›†åŠ è½½å™¨ - ä¸åŸFullDatasetLoaderå®Œå…¨å…¼å®¹
    """
    
    def __init__(self, h5_dir, years=None, return_metadata=True, 
                 lookback_seq=365, forecast_hor=7, min_fire_threshold=0.001,
                 cache_dir=None):
        """
        åˆå§‹åŒ–å¹´ä»½å®Œæ•´æ•°æ®é›†åŠ è½½å™¨
        """
        self.h5_dir = h5_dir
        self.years = years
        self.return_metadata = return_metadata
        self.lookback_seq = lookback_seq
        self.forecast_hor = forecast_hor
        self.min_fire_threshold = min_fire_threshold
        self.cache_dir = cache_dir
        
        # åˆ›å»ºåŸºç¡€æ•°æ®é›†ï¼Œä¸è¿›è¡Œä»»ä½•é‡‡æ ·
        self.dataset = YearTimeSeriesPixelDataset(
            h5_dir=h5_dir,
            years=years,
            return_metadata=return_metadata,
            positive_ratio=1.0,
            pos_neg_ratio=999999,  # ä½¿ç”¨æ‰€æœ‰è´Ÿæ ·æœ¬
            resample_each_epoch=False,
            verbose_sampling=True,
            lookback_seq=lookback_seq,
            forecast_hor=forecast_hor,
            min_fire_threshold=min_fire_threshold,
            cache_dir=cache_dir,
            force_resample=False
        )
        
        logger.info(f"å¹´ä»½å®Œæ•´æ•°æ®é›†åŠ è½½å™¨åˆå§‹åŒ–å®Œæˆï¼Œå…± {len(self.dataset)} ä¸ªæ ·æœ¬")
        
        # è·å–ç»Ÿè®¡ä¿¡æ¯
        stats = self.dataset.get_statistics()
        logger.info(f"æ•°æ®é›†ç»Ÿè®¡: {stats}")
    
    def create_dataloader(self, batch_size=32, shuffle=False, num_workers=4, worker_init_fn=None, **dataloader_kwargs):
        """
        åˆ›å»ºPyTorch DataLoader
        """
        return DataLoader(
            self.dataset,
            batch_size=batch_size,
            shuffle=shuffle,
            num_workers=num_workers,
            collate_fn=self.dataset.custom_collate_fn,
            worker_init_fn=worker_init_fn,
            **dataloader_kwargs
        )


# ä¸ºäº†å®Œå…¨å…¼å®¹åŸæ¥å£ï¼Œåˆ›å»ºåˆ«å
TimeSeriesDataLoader = YearTimeSeriesDataLoader
TimeSeriesPixelDataset = YearTimeSeriesPixelDataset
FullDatasetLoader = YearFullDatasetLoader


# æµ‹è¯•å‡½æ•°
def test_year_dataloader():
    """æµ‹è¯•å¹´ä»½æ•°æ®åŠ è½½å™¨ - ä¼˜åŒ–ç‰ˆæœ¬"""
    
    # æµ‹è¯•å‚æ•°
    h5_dir = "/mnt/raid/zhengsen/wildfire_dataset/self_built_materials/year_datasets_h5"
    
    try:
        logger.info("ğŸš€ å¼€å§‹æµ‹è¯•å¹´ä»½æ•°æ®åŠ è½½å™¨...")
        
        # æµ‹è¯•ç¼“å­˜å’Œé‡‡æ ·ä¼˜åŒ–
        logger.info("ğŸ“Š æµ‹è¯•1: ç¼“å­˜å’Œé‡‡æ ·ä¼˜åŒ–...")
        start_time = time.time()
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        data_loader = YearTimeSeriesDataLoader(
            h5_dir=h5_dir,
            positive_ratio=0.1,
            pos_neg_ratio=2.0,
            lookback_seq=365,
            forecast_hor=7,
            min_fire_threshold=1.0,
            verbose_sampling=True,
            force_resample=False  # ä½¿ç”¨ç¼“å­˜
        )
        
        init_time = time.time() - start_time
        logger.info(f"â° æ•°æ®åŠ è½½å™¨åˆå§‹åŒ–è€—æ—¶: {init_time:.2f}ç§’")
        
        # æµ‹è¯•å¹´ä»½åˆ’åˆ†
        logger.info("ğŸ“Š æµ‹è¯•2: å¹´ä»½åˆ’åˆ†...")
        train_indices, val_indices, test_indices = data_loader.get_year_based_split(
            train_years=[2021, 2022],
            val_years=[2023],
            test_years=[2024]
        )
        
        logger.info(f"âœ… è®­ç»ƒé›†: {len(train_indices)} æ ·æœ¬")
        logger.info(f"âœ… éªŒè¯é›†: {len(val_indices)} æ ·æœ¬")
        logger.info(f"âœ… æµ‹è¯•é›†: {len(test_indices)} æ ·æœ¬")
        
        # æµ‹è¯•æ•°æ®åŠ è½½æ€§èƒ½
        logger.info("ğŸ“Š æµ‹è¯•3: æ•°æ®åŠ è½½æ€§èƒ½...")
        if train_indices:
            sample_idx = train_indices[0]
            
            # æµ‹è¯•å•ä¸ªæ ·æœ¬åŠ è½½æ—¶é—´
            start_time = time.time()
            past_data, future_data, metadata = data_loader.dataset[sample_idx]
            sample_time = time.time() - start_time
            
            logger.info(f"â° å•æ ·æœ¬åŠ è½½è€—æ—¶: {sample_time:.4f}ç§’")
            logger.info(f"âœ… æ ·æœ¬å½¢çŠ¶: past={past_data.shape}, future={future_data.shape}")
            logger.info(f"âœ… å…ƒæ•°æ®: {metadata}")
            
            # æ£€æŸ¥æ•°æ®è´¨é‡
            logger.info("ğŸ“Š æµ‹è¯•4: æ•°æ®è´¨é‡æ£€æŸ¥...")
            logger.info(f"âœ… Pastæ•°æ®èŒƒå›´: [{past_data.min():.4f}, {past_data.max():.4f}]")
            logger.info(f"âœ… Futureæ•°æ®èŒƒå›´: [{future_data.min():.4f}, {future_data.max():.4f}]")
            logger.info(f"âœ… NaNæ£€æŸ¥: Past={torch.isnan(past_data).sum()}, Future={torch.isnan(future_data).sum()}")
        
        # æµ‹è¯•DataLoaderæ‰¹å¤„ç†æ€§èƒ½
        logger.info("ğŸ“Š æµ‹è¯•5: æ‰¹å¤„ç†æ€§èƒ½...")
        from torch.utils.data import Subset
        
        test_sample_size = min(100, len(train_indices))
        train_dataset = Subset(data_loader.dataset, train_indices[:test_sample_size])
        train_loader = DataLoader(
            train_dataset,
            batch_size=8,
            shuffle=True,
            collate_fn=data_loader.dataset.custom_collate_fn,
            num_workers=2
        )
        
        start_time = time.time()
        for i, batch in enumerate(train_loader):
            past_batch, future_batch, metadata_batch = batch
            if i == 0:
                logger.info(f"âœ… æ‰¹æ¬¡å½¢çŠ¶: past={past_batch.shape}, future={future_batch.shape}, metadata={len(metadata_batch)}")
            if i >= 4:  # åªæµ‹è¯•å‰5ä¸ªæ‰¹æ¬¡
                break
        
        batch_time = time.time() - start_time
        logger.info(f"â° æ‰¹å¤„ç†è€—æ—¶: {batch_time:.2f}ç§’ (5ä¸ªæ‰¹æ¬¡)")
        
        # æµ‹è¯•æ ·æœ¬ç»Ÿè®¡
        logger.info("ğŸ“Š æµ‹è¯•6: æ ·æœ¬ç»Ÿè®¡...")
        stats = data_loader.dataset.get_current_sample_stats()
        logger.info(f"âœ… æ ·æœ¬ç»Ÿè®¡: {stats}")
        
        # æµ‹è¯•ç¬¬äºŒæ¬¡åˆå§‹åŒ–ï¼ˆç¼“å­˜æ•ˆæœï¼‰
        logger.info("ğŸ“Š æµ‹è¯•7: ç¼“å­˜æ•ˆæœ...")
        start_time = time.time()
        
        data_loader2 = YearTimeSeriesDataLoader(
            h5_dir=h5_dir,
            positive_ratio=0.1,
            pos_neg_ratio=2.0,
            lookback_seq=365,
            forecast_hor=7,
            min_fire_threshold=1.0,
            verbose_sampling=True,
            force_resample=False
        )
        
        cache_time = time.time() - start_time
        logger.info(f"â° ç¼“å­˜åŠ è½½è€—æ—¶: {cache_time:.2f}ç§’")
        logger.info(f"ğŸš€ åŠ é€Ÿæ¯”: {init_time/cache_time:.1f}x")
        
        logger.info("ğŸ‰ æ‰€æœ‰æµ‹è¯•å®Œæˆï¼")
        
    except Exception as e:
        logger.error(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        raise


if __name__ == "__main__":
    # è¿è¡Œæµ‹è¯•
    test_year_dataloader() 