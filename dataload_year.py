#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åŸºäºå¹´ä»½æ•°æ®çš„æ—¶é—´åºåˆ—æ•°æ®åŠ è½½å™¨ - æ€§èƒ½ä¼˜åŒ–ç‰ˆæœ¬
é€‚é…generate_whole_dataset.pyç”Ÿæˆçš„å¹´ä»½æ•°æ®ï¼Œä¸åŸdataload.pyæ¥å£å®Œå…¨ä¸€è‡´

æ•°æ®æ ¼å¼:
- æ–‡ä»¶å: {year}_year_dataset.h5
- æ•°æ®é›†å: {row}_{col} (åƒç´ åæ ‡)
- æ•°æ®å½¢çŠ¶: (channels, time_steps) å…¶ä¸­ channels=39, time_steps=365/366
- é€šé“0: FIRMSæ•°æ®ï¼Œç”¨äºæ­£è´Ÿæ ·æœ¬åˆ¤æ–­

ç‰¹æ€§:
- æ­£æ ·æœ¬: FIRMS >= min_fire_threshold çš„æŸä¸€å¤©
- è´Ÿæ ·æœ¬: å…¨å±€æ— ç«æ—¥æœŸçš„éšæœºåƒç´ ä½ç½®
- æ”¯æŒè·¨å¹´ä»½æ•°æ®åŠ è½½ï¼ˆå†å²/æœªæ¥æ•°æ®ï¼‰
- æ™ºèƒ½é‡‡æ ·ç¼“å­˜æœºåˆ¶ï¼Œé¿å…é‡å¤è®¡ç®—
- ä¸åŸdataload.pyå®Œå…¨å…¼å®¹çš„æ¥å£

æ€§èƒ½ä¼˜åŒ–:
- æ–‡ä»¶å¥æŸ„ç¼“å­˜å’Œé‡ç”¨
- æ•°æ®å†…å­˜ç¼“å­˜
- æ‰¹é‡æ•°æ®åŠ è½½
- å‘é‡åŒ–æ“ä½œ
- å‡å°‘ç£ç›˜I/Oæ“ä½œ
"""

import os
import h5py
import torch
import numpy as np
from torch.utils.data import Dataset, DataLoader
from tqdm import tqdm
import logging
import hashlib
import pickle
import random
import time
from datetime import datetime, timedelta
from collections import defaultdict, OrderedDict
import glob
import json
import gc
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading
import weakref
from functools import lru_cache

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# =============================================================================
# æ€§èƒ½ä¼˜åŒ–ç»„ä»¶
# =============================================================================

class FileHandleManager:
    """
    æ–‡ä»¶å¥æŸ„ç®¡ç†å™¨ - ä½¿ç”¨LRUç¼“å­˜ä¼˜åŒ–æ–‡ä»¶æ‰“å¼€/å…³é—­
    """
    
    def __init__(self, max_handles=50):
        self.max_handles = max_handles
        self.handles = OrderedDict()
        self.lock = threading.Lock()
        
    def get_handle(self, file_path):
        """è·å–æ–‡ä»¶å¥æŸ„ï¼Œè‡ªåŠ¨ç®¡ç†LRUç¼“å­˜"""
        with self.lock:
            if file_path in self.handles:
                # ç§»åˆ°æœ€åï¼ˆæœ€è¿‘ä½¿ç”¨ï¼‰
                handle = self.handles.pop(file_path)
                self.handles[file_path] = handle
                return handle
            
            # å¦‚æœç¼“å­˜æ»¡äº†ï¼Œå…³é—­æœ€æ—§çš„æ–‡ä»¶
            if len(self.handles) >= self.max_handles:
                oldest_path, oldest_handle = self.handles.popitem(last=False)
                try:
                    oldest_handle.close()
                except:
                    pass
            
            # æ‰“å¼€æ–°æ–‡ä»¶
            try:
                handle = h5py.File(file_path, 'r')
                self.handles[file_path] = handle
                return handle
            except Exception as e:
                logger.error(f"æ— æ³•æ‰“å¼€æ–‡ä»¶ {file_path}: {e}")
                return None
    
    def close_all(self):
        """å…³é—­æ‰€æœ‰æ–‡ä»¶å¥æŸ„"""
        with self.lock:
            for handle in self.handles.values():
                try:
                    handle.close()
                except:
                    pass
            self.handles.clear()
    
    def __del__(self):
        self.close_all()


class DataCache:
    """
    æ•°æ®ç¼“å­˜ç®¡ç†å™¨ - æ™ºèƒ½å†…å­˜ç¼“å­˜
    """
    
    def __init__(self, max_size_mb=1024):
        self.max_size_bytes = max_size_mb * 1024 * 1024
        self.cache = OrderedDict()
        self.current_size = 0
        self.lock = threading.Lock()
        
    def get(self, key):
        """è·å–ç¼“å­˜æ•°æ®"""
        with self.lock:
            if key in self.cache:
                # ç§»åˆ°æœ€åï¼ˆæœ€è¿‘ä½¿ç”¨ï¼‰
                data = self.cache.pop(key)
                self.cache[key] = data
                return data
            return None
    
    def put(self, key, data):
        """å­˜å‚¨æ•°æ®åˆ°ç¼“å­˜"""
        data_size = data.nbytes if hasattr(data, 'nbytes') else len(str(data))
        
        with self.lock:
            # å¦‚æœæ•°æ®å¤ªå¤§ï¼Œä¸ç¼“å­˜
            if data_size > self.max_size_bytes * 0.5:
                return
            
            # æ¸…ç†ç©ºé—´
            while self.current_size + data_size > self.max_size_bytes and self.cache:
                oldest_key, oldest_data = self.cache.popitem(last=False)
                old_size = oldest_data.nbytes if hasattr(oldest_data, 'nbytes') else len(str(oldest_data))
                self.current_size -= old_size
            
            # æ·»åŠ æ–°æ•°æ®
            self.cache[key] = data
            self.current_size += data_size
    
    def clear(self):
        """æ¸…ç©ºç¼“å­˜"""
        with self.lock:
            self.cache.clear()
            self.current_size = 0
    
    def get_stats(self):
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        with self.lock:
            return {
                'entries': len(self.cache),
                'size_mb': self.current_size / (1024 * 1024),
                'max_size_mb': self.max_size_bytes / (1024 * 1024)
            }


# å…¨å±€å•ä¾‹å®ä¾‹
_file_handle_manager = None
_data_cache = None

def get_file_handle_manager():
    """è·å–å…¨å±€æ–‡ä»¶å¥æŸ„ç®¡ç†å™¨"""
    global _file_handle_manager
    if _file_handle_manager is None:
        _file_handle_manager = FileHandleManager(max_handles=50)
    return _file_handle_manager

def get_data_cache():
    """è·å–å…¨å±€æ•°æ®ç¼“å­˜"""
    global _data_cache
    if _data_cache is None:
        _data_cache = DataCache(max_size_mb=1024)
    return _data_cache

# =============================================================================
# åŸæœ‰ä»£ç ç»§ç»­...
# =============================================================================

class YearTimeSeriesPixelDataset(Dataset):
    """
    åŸºäºå¹´ä»½æ•°æ®çš„æ—¶é—´åºåˆ—åƒç´ æ•°æ®é›†
    
    ä¸åŸTimeSeriesPixelDatasetå®Œå…¨å…¼å®¹çš„æ¥å£ï¼Œä½†ä½¿ç”¨æ–°çš„å¹´ä»½æ•°æ®æº
    """
    
    def __init__(self, h5_dir, years=None, return_metadata=True, 
                 positive_ratio=1.0, pos_neg_ratio=1.0, 
                 resample_each_epoch=False, epoch_seed=None, verbose_sampling=True,
                 lookback_seq=365, forecast_hor=7, min_fire_threshold=0.001,
                 cache_dir=None, force_resample=False, 
                 enable_performance_optimizations=True, max_file_handles=50, 
                 data_cache_size_mb=1024):
        """
        åˆå§‹åŒ–å¹´ä»½æ—¶é—´åºåˆ—åƒç´ æ•°æ®é›†
        
        Args:
            h5_dir: å¹´ä»½æ•°æ®H5æ–‡ä»¶ç›®å½•
            years: è¦åŠ è½½çš„å¹´ä»½åˆ—è¡¨ï¼ŒNoneè¡¨ç¤ºåŠ è½½æ‰€æœ‰å¹´ä»½
            return_metadata: æ˜¯å¦è¿”å›å…ƒæ•°æ®ï¼ˆæ—¥æœŸã€åæ ‡ç­‰ï¼‰
            positive_ratio: æ­£æ ·æœ¬ä½¿ç”¨æ¯”ä¾‹ (0.0-1.0)
            pos_neg_ratio: æ­£è´Ÿæ ·æœ¬æ¯”ä¾‹ï¼Œå³è´Ÿæ ·æœ¬æ•° = æ­£æ ·æœ¬æ•° Ã— pos_neg_ratio
            resample_each_epoch: æ˜¯å¦åœ¨æ¯ä¸ªepoché‡æ–°è¿›è¡Œæ ·æœ¬æŠ½æ ·
            epoch_seed: å½“å‰epochçš„éšæœºç§å­
            verbose_sampling: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†é‡‡æ ·ä¿¡æ¯
            lookback_seq: å†å²æ—¶é—´é•¿åº¦ï¼ˆå¤©ï¼‰
            forecast_hor: æœªæ¥æ—¶é—´é•¿åº¦ï¼ˆå¤©ï¼‰
            min_fire_threshold: FIRMSé˜ˆå€¼ï¼Œ>=è¯¥å€¼è®¤ä¸ºæ˜¯æ­£æ ·æœ¬
            cache_dir: é‡‡æ ·ç¼“å­˜ç›®å½•ï¼ŒNoneè¡¨ç¤ºä½¿ç”¨h5_dir/cache
            force_resample: æ˜¯å¦å¼ºåˆ¶é‡æ–°é‡‡æ ·ï¼Œå¿½ç•¥ç¼“å­˜
            enable_performance_optimizations: æ˜¯å¦å¯ç”¨æ€§èƒ½ä¼˜åŒ–
            max_file_handles: æœ€å¤§æ–‡ä»¶å¥æŸ„ç¼“å­˜æ•°é‡
            data_cache_size_mb: æ•°æ®ç¼“å­˜å¤§å°ï¼ˆMBï¼‰
        """
        self.h5_dir = h5_dir
        self.years = years
        self.return_metadata = return_metadata
        self.positive_ratio = positive_ratio
        self.pos_neg_ratio = pos_neg_ratio
        self.resample_each_epoch = resample_each_epoch
        self.epoch_seed = epoch_seed
        self.verbose_sampling = verbose_sampling
        self.lookback_seq = lookback_seq
        self.forecast_hor = forecast_hor
        self.min_fire_threshold = min_fire_threshold
        self.force_resample = force_resample
        self.enable_performance_optimizations = enable_performance_optimizations
        
        # ç¼“å­˜é…ç½®
        self.cache_dir = cache_dir if cache_dir else os.path.join(h5_dir, 'cache')
        os.makedirs(self.cache_dir, exist_ok=True)
        
        # æ€§èƒ½ä¼˜åŒ–é…ç½®
        if enable_performance_optimizations:
            # åˆå§‹åŒ–å…¨å±€ä¼˜åŒ–ç»„ä»¶
            global _file_handle_manager, _data_cache
            if _file_handle_manager is None:
                _file_handle_manager = FileHandleManager(max_handles=max_file_handles)
            if _data_cache is None:
                _data_cache = DataCache(max_size_mb=data_cache_size_mb)
            
            logger.info(f"ğŸš€ æ€§èƒ½ä¼˜åŒ–å·²å¯ç”¨: æ–‡ä»¶å¥æŸ„ç¼“å­˜({max_file_handles}), æ•°æ®ç¼“å­˜({data_cache_size_mb}MB)")
        else:
            logger.info("âš ï¸  æ€§èƒ½ä¼˜åŒ–å·²ç¦ç”¨")
        
        # è·å–å¹´ä»½æ–‡ä»¶åˆ—è¡¨
        self.year_files = self._get_year_files()
        
        # æ„å»ºæˆ–åŠ è½½é‡‡æ ·ç»“æœ
        self.full_sample_index = []  # å­˜å‚¨æ‰€æœ‰æ ·æœ¬çš„å®Œæ•´ç´¢å¼•
        self.sample_index = []  # å½“å‰ä½¿ç”¨çš„æ ·æœ¬ç´¢å¼•
        self.dataset_info = {}  # å­˜å‚¨æ•°æ®é›†ä¿¡æ¯
        
        self._build_or_load_samples()
        
        # åˆå§‹çš„æ ·æœ¬æ¯”ä¾‹ç­›é€‰
        if positive_ratio < 1.0 or pos_neg_ratio != 1.0:
            if resample_each_epoch:
                if self.verbose_sampling:
                    logger.info("å¯ç”¨æ¯epoché‡æ–°æŠ½æ ·æ¨¡å¼")
                # ä¿å­˜å®Œæ•´ç´¢å¼•å¹¶è¿›è¡Œåˆå§‹æŠ½æ ·
                self.full_sample_index = self.sample_index.copy()
                self._apply_sample_ratio_filtering()
            else:
                # ä¼ ç»Ÿæ¨¡å¼ï¼šä¸€æ¬¡æ€§æŠ½æ ·
                self._apply_sample_ratio_filtering()
        
        logger.info(f"å¹´ä»½æ•°æ®é›†åˆå§‹åŒ–å®Œæˆï¼Œå…± {len(self.sample_index)} ä¸ªæ ·æœ¬")
        logger.info(f"æ­£æ ·æœ¬ä½¿ç”¨æ¯”ä¾‹: {positive_ratio:.2f}, æ­£è´Ÿæ ·æœ¬æ¯”ä¾‹: 1:{pos_neg_ratio:.2f}")
        if resample_each_epoch:
            logger.info(f"å¯ç”¨æ¯epoché‡æ–°æŠ½æ ·ï¼Œæ€»æ ·æœ¬æ± : {len(self.full_sample_index)} ä¸ªæ ·æœ¬")
        
        # æ˜¾ç¤ºæ€§èƒ½ä¼˜åŒ–çŠ¶æ€
        if enable_performance_optimizations:
            cache_stats = get_data_cache().get_stats()
            logger.info(f"ğŸ“Š æ€§èƒ½ç¼“å­˜çŠ¶æ€: æ•°æ®ç¼“å­˜ {cache_stats['entries']} æ¡ç›®, {cache_stats['size_mb']:.1f}MB")
    
    def _get_year_files(self):
        """è·å–å¹´ä»½H5æ–‡ä»¶åˆ—è¡¨"""
        year_files = {}
        
        # æŸ¥æ‰¾æ‰€æœ‰å¹´ä»½æ–‡ä»¶
        for filename in os.listdir(self.h5_dir):
            if filename.endswith('_year_dataset.h5'):
                try:
                    year = int(filename.split('_')[0])
                    if self.years is None or year in self.years:
                        year_files[year] = os.path.join(self.h5_dir, filename)
                except ValueError:
                    continue
        
        if not year_files:
            raise ValueError(f"æœªæ‰¾åˆ°å¹´ä»½æ•°æ®æ–‡ä»¶ï¼Œç›®å½•: {self.h5_dir}")
        
        logger.info(f"æ‰¾åˆ° {len(year_files)} ä¸ªå¹´ä»½æ•°æ®æ–‡ä»¶: {sorted(year_files.keys())}")
        return year_files
    
    def _get_cache_filename(self):
        """ç”Ÿæˆç¼“å­˜æ–‡ä»¶å"""
        # æ ¹æ®å…³é”®å‚æ•°ç”Ÿæˆå”¯ä¸€æ ‡è¯†
        cache_params = {
            'h5_dir': self.h5_dir,
            'years': sorted(self.year_files.keys()),
            'lookback_seq': self.lookback_seq,
            'forecast_hor': self.forecast_hor,
            'min_fire_threshold': self.min_fire_threshold,
            'version': '1.0'  # ç‰ˆæœ¬å·ï¼Œç”¨äºç¼“å­˜æ ¼å¼å˜æ›´
        }
        
        # ç”Ÿæˆå‚æ•°å“ˆå¸Œ
        param_str = json.dumps(cache_params, sort_keys=True)
        param_hash = hashlib.md5(param_str.encode()).hexdigest()[:12]
        
        return os.path.join(self.cache_dir, f"samples_{param_hash}.h5")
    
    def _check_cache_validity(self, cache_file):
        """æ£€æŸ¥ç¼“å­˜æ–‡ä»¶æ˜¯å¦æœ‰æ•ˆ"""
        if self.force_resample:
            return False
        
        if not os.path.exists(cache_file):
            return False
        
        try:
            with h5py.File(cache_file, 'r') as f:
                # æ£€æŸ¥ç¼“å­˜å‚æ•°
                cached_params = json.loads(f.attrs.get('cache_params', '{}'))
                current_params = {
                    'h5_dir': self.h5_dir,
                    'years': sorted(self.year_files.keys()),
                    'lookback_seq': self.lookback_seq,
                    'forecast_hor': self.forecast_hor,
                    'min_fire_threshold': self.min_fire_threshold,
                    'version': '1.0'
                }
                
                if cached_params != current_params:
                    return False
                
                # æ£€æŸ¥æºæ•°æ®æ–‡ä»¶æ˜¯å¦æœ‰å˜åŒ–
                cached_file_mtimes = json.loads(f.attrs.get('source_file_mtimes', '{}'))
                current_file_mtimes = {}
                
                for year, file_path in self.year_files.items():
                    if os.path.exists(file_path):
                        current_file_mtimes[str(year)] = os.path.getmtime(file_path)
                
                if cached_file_mtimes != current_file_mtimes:
                    return False
                
                return True
                
        except Exception as e:
            logger.warning(f"æ£€æŸ¥ç¼“å­˜æ–‡ä»¶å¤±è´¥: {e}")
            return False
    
    def _process_year_for_no_fire_days(self, year_file_info):
        """å¤„ç†å•ä¸ªå¹´ä»½çš„æ— ç«æ—¥æœŸæŸ¥æ‰¾"""
        year, file_path = year_file_info
        year_no_fire_days = []
        
        try:
            with h5py.File(file_path, 'r') as f:
                year_days = int(f.attrs.get('total_time_steps', 365))
                
                # ä¸€æ¬¡æ€§åŠ è½½æ‰€æœ‰åƒç´ çš„FIRMSæ•°æ®
                pixel_names = [name for name in f.keys() if '_' in name]
                
                # ğŸ” æ·»åŠ è°ƒè¯•ä¿¡æ¯
                logger.info(f"å¹´ä»½ {year}: æ‰¾åˆ° {len(pixel_names)} ä¸ªåƒç´ æ•°æ®é›†ï¼ˆæŸ¥æ‰¾æ— ç«æ—¥æœŸï¼‰")
                if len(pixel_names) == 0:
                    logger.warning(f"å¹´ä»½ {year} æ–‡ä»¶ {file_path} æ²¡æœ‰æ‰¾åˆ°åƒç´ æ•°æ®é›†")
                    all_keys = list(f.keys())
                    logger.warning(f"æ–‡ä»¶ä¸­çš„æ‰€æœ‰é”®: {all_keys[:10]}...")  # æ˜¾ç¤ºå‰10ä¸ª
                    return year_no_fire_days
                
                if not pixel_names:
                    return year_no_fire_days
                
                # æ‰¹é‡åŠ è½½FIRMSæ•°æ®
                firms_data_list = []
                for pixel_name in pixel_names:
                    try:
                        pixel_data = f[pixel_name]
                        firms_channel = pixel_data[0, :]  # FIRMSé€šé“æ•°æ®
                        firms_data_list.append(firms_channel)
                    except Exception as e:
                        logger.warning(f"åŠ è½½åƒç´  {pixel_name} å¤±è´¥: {e}")
                        continue
                
                if not firms_data_list:
                    return year_no_fire_days
                
                # è½¬æ¢ä¸ºnumpyæ•°ç»„è¿›è¡Œå‘é‡åŒ–è®¡ç®—
                # shape: (num_pixels, time_steps)
                firms_array = np.array(firms_data_list)
                
                # å‘é‡åŒ–è®¡ç®—æ¯å¤©çš„æœ€å¤§FIRMSå€¼
                daily_max_firms = np.nanmax(firms_array, axis=0)
                
                # ğŸ” æ·»åŠ è°ƒè¯•ä¿¡æ¯
                overall_max_firms = np.nanmax(daily_max_firms)
                overall_min_firms = np.nanmin(daily_max_firms)
                logger.info(f"å¹´ä»½ {year}: æ¯æ—¥æœ€å¤§FIRMSå€¼èŒƒå›´: {overall_min_firms:.2f}-{overall_max_firms:.2f}, é˜ˆå€¼: {self.min_fire_threshold}")
                
                # æ‰¾å‡ºæ— ç«æ—¥æœŸï¼ˆæœ€å¤§FIRMSå€¼å°äºé˜ˆå€¼çš„å¤©ï¼‰
                no_fire_days = np.where(daily_max_firms < self.min_fire_threshold)[0]
                
                # ğŸ” æ·»åŠ è°ƒè¯•ä¿¡æ¯
                logger.info(f"å¹´ä»½ {year}: æ‰¾åˆ° {len(no_fire_days)} ä¸ªæ— ç«æ—¥æœŸï¼ˆå…± {len(daily_max_firms)} å¤©ï¼‰")
                
                # æ„å»ºç»“æœ
                start_date = datetime(year, 1, 1)
                for day_idx in no_fire_days:
                    actual_date = start_date + timedelta(days=int(day_idx))
                    year_no_fire_days.append({
                        'year': year,
                        'day_of_year': int(day_idx),
                        'date': actual_date,
                        'file_path': file_path
                    })
                    
        except Exception as e:
            logger.error(f"å¤„ç†å¹´ä»½ {year} æ—¶å‡ºé”™: {e}")
        
        # ğŸ” æ·»åŠ æ±‡æ€»ä¿¡æ¯
        logger.info(f"å¹´ä»½ {year}: æ€»å…±æ‰¾åˆ° {len(year_no_fire_days)} ä¸ªæ— ç«æ—¥æœŸ")
        
        return year_no_fire_days

    def _find_global_no_fire_days(self):
        """æŸ¥æ‰¾å…¨å±€æ— ç«æ—¥æœŸ - ä¼˜åŒ–ç‰ˆæœ¬"""
        logger.info("æ­£åœ¨æŸ¥æ‰¾å…¨å±€æ— ç«æ—¥æœŸ...")
        
        global_no_fire_days = []
        
        # ä½¿ç”¨çº¿ç¨‹æ± è€Œä¸æ˜¯è¿›ç¨‹æ± ï¼Œé¿å…pickleé—®é¢˜
        from concurrent.futures import ThreadPoolExecutor, as_completed
        
        # å¹¶è¡Œå¤„ç†æ‰€æœ‰å¹´ä»½ (é™åˆ¶å¹¶è¡Œæ•°é¿å…å†…å­˜è¿‡è½½)
        max_workers = min(4, len(self.year_files), os.cpu_count() or 1)
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = [
                executor.submit(self._process_year_for_no_fire_days, (year, file_path))
                for year, file_path in self.year_files.items()
            ]
            
            for future in tqdm(as_completed(futures), total=len(futures), desc="æ‰«æå¹´ä»½"):
                try:
                    year_results = future.result()
                    global_no_fire_days.extend(year_results)
                except Exception as e:
                    logger.error(f"å¤„ç†å¹´ä»½æ—¶å‡ºé”™: {e}")
        
        logger.info(f"æ‰¾åˆ° {len(global_no_fire_days)} ä¸ªå…¨å±€æ— ç«æ—¥æœŸ")
        return global_no_fire_days
    
    def _process_year_for_positive_samples(self, year_file_info):
        """å¤„ç†å•ä¸ªå¹´ä»½çš„æ­£æ ·æœ¬æŸ¥æ‰¾"""
        year, file_path = year_file_info
        year_positive_samples = []
        
        try:
            with h5py.File(file_path, 'r') as f:
                year_days = int(f.attrs.get('total_time_steps', 365))
                start_date = datetime(year, 1, 1)
                
                # è·å–æ‰€æœ‰åƒç´ æ•°æ®é›†åç§°
                pixel_names = [name for name in f.keys() if '_' in name]
                
                # ğŸ” æ·»åŠ è°ƒè¯•ä¿¡æ¯
                logger.info(f"å¹´ä»½ {year}: æ‰¾åˆ° {len(pixel_names)} ä¸ªåƒç´ æ•°æ®é›†")
                if len(pixel_names) == 0:
                    logger.warning(f"å¹´ä»½ {year} æ–‡ä»¶ {file_path} æ²¡æœ‰æ‰¾åˆ°åƒç´ æ•°æ®é›†")
                    all_keys = list(f.keys())
                    logger.warning(f"æ–‡ä»¶ä¸­çš„æ‰€æœ‰é”®: {all_keys[:10]}...")  # æ˜¾ç¤ºå‰10ä¸ª
                
                for pixel_name in pixel_names:
                    try:
                        row, col = map(int, pixel_name.split('_'))
                        pixel_data = f[pixel_name]
                        firms_channel = pixel_data[0, :]  # FIRMSé€šé“æ•°æ®
                        
                        # ğŸ” æ·»åŠ è°ƒè¯•ä¿¡æ¯
                        max_firms = np.nanmax(firms_channel)
                        min_firms = np.nanmin(firms_channel)
                        
                        # å‘é‡åŒ–æŸ¥æ‰¾è¶…è¿‡é˜ˆå€¼çš„å¤©æ•°
                        fire_days = np.where(firms_channel >= self.min_fire_threshold)[0]
                        
                        # ğŸ” å¦‚æœæ‰¾åˆ°ç«ç¾å¤©ï¼Œæ·»åŠ è°ƒè¯•ä¿¡æ¯
                        if len(fire_days) > 0:
                            logger.info(f"åƒç´  {pixel_name}: æ‰¾åˆ° {len(fire_days)} ä¸ªç«ç¾å¤©ï¼ŒFIRMSèŒƒå›´: {min_firms:.2f}-{max_firms:.2f}")
                        
                        # ä¸ºæ¯ä¸ªç«ç¾å¤©åˆ›å»ºæ­£æ ·æœ¬
                        for day_idx in fire_days:
                            actual_date = start_date + timedelta(days=int(day_idx))
                            year_positive_samples.append({
                                'year': year,
                                'day_of_year': int(day_idx),
                                'date': actual_date,
                                'pixel_row': row,
                                'pixel_col': col,
                                'firms_value': float(firms_channel[day_idx]),
                                'file_path': file_path
                            })
                            
                    except ValueError:
                        # è·³è¿‡éåƒç´ æ•°æ®é›†
                        continue
                    except Exception as e:
                        logger.warning(f"å¤„ç†åƒç´  {pixel_name} æ—¶å‡ºé”™: {e}")
                        continue
                        
        except Exception as e:
            logger.error(f"å¤„ç†å¹´ä»½ {year} æ—¶å‡ºé”™: {e}")
        
        # ğŸ” æ·»åŠ æ±‡æ€»ä¿¡æ¯
        logger.info(f"å¹´ä»½ {year}: æ€»å…±æ‰¾åˆ° {len(year_positive_samples)} ä¸ªæ­£æ ·æœ¬")
        
        return year_positive_samples

    def _find_positive_samples(self):
        """æŸ¥æ‰¾æ­£æ ·æœ¬ - ä¼˜åŒ–ç‰ˆæœ¬"""
        logger.info("æ­£åœ¨æŸ¥æ‰¾æ­£æ ·æœ¬...")
        
        positive_samples = []
        
        # ä½¿ç”¨çº¿ç¨‹æ± è€Œä¸æ˜¯è¿›ç¨‹æ± ï¼Œé¿å…pickleé—®é¢˜
        from concurrent.futures import ThreadPoolExecutor, as_completed
        
        # å¹¶è¡Œå¤„ç†æ‰€æœ‰å¹´ä»½ (é™åˆ¶å¹¶è¡Œæ•°é¿å…å†…å­˜è¿‡è½½)
        max_workers = min(4, len(self.year_files), os.cpu_count() or 1)
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = [
                executor.submit(self._process_year_for_positive_samples, (year, file_path))
                for year, file_path in self.year_files.items()
            ]
            
            for future in tqdm(as_completed(futures), total=len(futures), desc="æ‰«ææ­£æ ·æœ¬"):
                try:
                    year_results = future.result()
                    positive_samples.extend(year_results)
                except Exception as e:
                    logger.error(f"å¤„ç†å¹´ä»½æ—¶å‡ºé”™: {e}")
        
        logger.info(f"æ‰¾åˆ° {len(positive_samples)} ä¸ªæ­£æ ·æœ¬")
        return positive_samples
    
    def _generate_negative_samples(self, global_no_fire_days, num_negative_samples):
        """ç”Ÿæˆè´Ÿæ ·æœ¬ - ä¼˜åŒ–ç‰ˆæœ¬"""
        logger.info(f"æ­£åœ¨ç”Ÿæˆ {num_negative_samples} ä¸ªè´Ÿæ ·æœ¬...")
        
        if not global_no_fire_days:
            logger.warning("æ²¡æœ‰å…¨å±€æ— ç«æ—¥æœŸï¼Œæ— æ³•ç”Ÿæˆè´Ÿæ ·æœ¬")
            return []
        
        negative_samples = []
        
        # é¢„å…ˆç¼“å­˜æ¯ä¸ªå¹´ä»½çš„åƒç´ åˆ—è¡¨ï¼Œé¿å…é‡å¤è¯»å–
        year_pixels_cache = {}
        
        logger.info("é¢„ç¼“å­˜å¹´ä»½åƒç´ ä¿¡æ¯...")
        for year, file_path in tqdm(self.year_files.items(), desc="ç¼“å­˜åƒç´ ä¿¡æ¯"):
            try:
                with h5py.File(file_path, 'r') as f:
                    pixels = []
                    for dataset_name in f.keys():
                        if '_' in dataset_name:
                            try:
                                row, col = map(int, dataset_name.split('_'))
                                pixels.append((row, col))
                            except ValueError:
                                continue
                    
                    if pixels:
                        year_pixels_cache[year] = pixels
                        
            except Exception as e:
                logger.warning(f"ç¼“å­˜å¹´ä»½ {year} åƒç´ ä¿¡æ¯æ—¶å‡ºé”™: {e}")
                continue
        
        if not year_pixels_cache:
            logger.warning("æ²¡æœ‰å¯ç”¨çš„åƒç´ ä½ç½®ï¼Œæ— æ³•ç”Ÿæˆè´Ÿæ ·æœ¬")
            return []
        
        # æ„å»ºå¯ç”¨çš„æ— ç«æ—¥æœŸåˆ—è¡¨ï¼ˆåªåŒ…å«æœ‰åƒç´ æ•°æ®çš„å¹´ä»½ï¼‰
        valid_no_fire_days = []
        for day_info in global_no_fire_days:
            if day_info['year'] in year_pixels_cache:
                valid_no_fire_days.append(day_info)
        
        if not valid_no_fire_days:
            logger.warning("æ²¡æœ‰æœ‰æ•ˆçš„æ— ç«æ—¥æœŸï¼Œæ— æ³•ç”Ÿæˆè´Ÿæ ·æœ¬")
            return []
        
        # éšæœºé‡‡æ ·è´Ÿæ ·æœ¬
        random.seed(42)  # ç¡®ä¿å¯é‡å¤æ€§
        
        logger.info(f"ä» {len(valid_no_fire_days)} ä¸ªæœ‰æ•ˆæ— ç«æ—¥æœŸä¸­éšæœºé‡‡æ ·...")
        for _ in tqdm(range(num_negative_samples), desc="ç”Ÿæˆè´Ÿæ ·æœ¬"):
            # éšæœºé€‰æ‹©ä¸€ä¸ªæ— ç«æ—¥æœŸ
            day_info = random.choice(valid_no_fire_days)
            
            # éšæœºé€‰æ‹©ä¸€ä¸ªåƒç´ ä½ç½®
            available_pixels = year_pixels_cache[day_info['year']]
            pixel_row, pixel_col = random.choice(available_pixels)
            
            negative_samples.append({
                'year': day_info['year'],
                'day_of_year': day_info['day_of_year'],
                'date': day_info['date'],
                'pixel_row': pixel_row,
                'pixel_col': pixel_col,
                'firms_value': 0.0,  # è´Ÿæ ·æœ¬FIRMSå€¼ä¸º0
                'file_path': day_info['file_path']
            })
        
        logger.info(f"ç”Ÿæˆ {len(negative_samples)} ä¸ªè´Ÿæ ·æœ¬")
        return negative_samples
    
    def _sample_data_with_cache(self):
        """é‡‡æ ·æ•°æ®ï¼Œä½¿ç”¨ç¼“å­˜æœºåˆ¶"""
        cache_file = self._get_cache_filename()
        
        # æ£€æŸ¥ç¼“å­˜æ˜¯å¦æœ‰æ•ˆ
        if self._check_cache_validity(cache_file):
            logger.info(f"ğŸš€ ä½¿ç”¨ç¼“å­˜æ–‡ä»¶: {cache_file}")
            return self._load_samples_from_cache(cache_file)
        
        # ç¼“å­˜æ— æ•ˆï¼Œé‡æ–°é‡‡æ ·
        logger.info("ğŸ’¾ ç¼“å­˜æ— æ•ˆï¼Œå¼€å§‹é‡æ–°é‡‡æ ·...")
        
        # è®°å½•å¼€å§‹æ—¶é—´
        start_time = time.time()
        
        # æŸ¥æ‰¾å…¨å±€æ— ç«æ—¥æœŸ
        logger.info("ğŸ” æ­¥éª¤1: æŸ¥æ‰¾å…¨å±€æ— ç«æ—¥æœŸ...")
        step_start = time.time()
        global_no_fire_days = self._find_global_no_fire_days()
        step_duration = time.time() - step_start
        logger.info(f"âœ… å®Œæˆï¼Œè€—æ—¶: {step_duration:.2f}ç§’")
        
        # æŸ¥æ‰¾æ­£æ ·æœ¬
        logger.info("ğŸ” æ­¥éª¤2: æŸ¥æ‰¾æ­£æ ·æœ¬...")
        step_start = time.time()
        positive_samples = self._find_positive_samples()
        step_duration = time.time() - step_start
        logger.info(f"âœ… å®Œæˆï¼Œè€—æ—¶: {step_duration:.2f}ç§’")
        
        if not positive_samples:
            raise ValueError("æœªæ‰¾åˆ°ä»»ä½•æ­£æ ·æœ¬")
        
        # ç”Ÿæˆè´Ÿæ ·æœ¬
        logger.info("ğŸ” æ­¥éª¤3: ç”Ÿæˆè´Ÿæ ·æœ¬...")
        step_start = time.time()
        num_negative_samples = len(positive_samples) * 2  # é»˜è®¤2å€è´Ÿæ ·æœ¬
        negative_samples = self._generate_negative_samples(global_no_fire_days, num_negative_samples)
        step_duration = time.time() - step_start
        logger.info(f"âœ… å®Œæˆï¼Œè€—æ—¶: {step_duration:.2f}ç§’")
        
        # ä¿å­˜åˆ°ç¼“å­˜
        logger.info("ğŸ’¾ æ­¥éª¤4: ä¿å­˜åˆ°ç¼“å­˜...")
        step_start = time.time()
        self._save_samples_to_cache(cache_file, positive_samples, negative_samples, global_no_fire_days)
        step_duration = time.time() - step_start
        logger.info(f"âœ… å®Œæˆï¼Œè€—æ—¶: {step_duration:.2f}ç§’")
        
        total_duration = time.time() - start_time
        logger.info(f"ğŸ‰ é‡‡æ ·å®Œæˆï¼æ€»è€—æ—¶: {total_duration:.2f}ç§’")
        logger.info(f"ğŸ“Š é‡‡æ ·ç»“æœ: {len(positive_samples)} æ­£æ ·æœ¬, {len(negative_samples)} è´Ÿæ ·æœ¬")
        
        return positive_samples, negative_samples
    
    def _load_samples_from_cache(self, cache_file):
        """ä»ç¼“å­˜åŠ è½½æ ·æœ¬ - ä¼˜åŒ–ç‰ˆæœ¬"""
        import time
        start_time = time.time()
        
        try:
            with h5py.File(cache_file, 'r') as f:
                positive_samples = []
                negative_samples = []
                
                # æ‰¹é‡åŠ è½½æ­£æ ·æœ¬æ•°æ®
                if 'positive_samples' in f:
                    pos_group = f['positive_samples']
                    # ä¸€æ¬¡æ€§è¯»å–æ‰€æœ‰æ•°ç»„
                    pos_years = pos_group['year'][:]
                    pos_days = pos_group['day_of_year'][:]
                    pos_dates = pos_group['date'][:]
                    pos_rows = pos_group['pixel_row'][:]
                    pos_cols = pos_group['pixel_col'][:]
                    pos_firms = pos_group['firms_value'][:]
                    
                    # å‘é‡åŒ–å¤„ç† - é¢„è§£ç å­—ç¬¦ä¸²
                    decoded_dates = [pos_dates[i].decode() for i in range(len(pos_dates))]
                    for i in range(len(pos_years)):
                        year = int(pos_years[i])
                        positive_samples.append({
                            'year': year,
                            'day_of_year': int(pos_days[i]),
                            'date': datetime.strptime(decoded_dates[i], '%Y-%m-%d'),
                            'pixel_row': int(pos_rows[i]),
                            'pixel_col': int(pos_cols[i]),
                            'firms_value': float(pos_firms[i]),
                            'file_path': self.year_files[year]
                        })
                
                # æ‰¹é‡åŠ è½½è´Ÿæ ·æœ¬æ•°æ®
                if 'negative_samples' in f:
                    neg_group = f['negative_samples']
                    # ä¸€æ¬¡æ€§è¯»å–æ‰€æœ‰æ•°ç»„
                    neg_years = neg_group['year'][:]
                    neg_days = neg_group['day_of_year'][:]
                    neg_dates = neg_group['date'][:]
                    neg_rows = neg_group['pixel_row'][:]
                    neg_cols = neg_group['pixel_col'][:]
                    neg_firms = neg_group['firms_value'][:]
                    
                    # å‘é‡åŒ–å¤„ç† - é¢„è§£ç å­—ç¬¦ä¸²
                    decoded_dates = [neg_dates[i].decode() for i in range(len(neg_dates))]
                    for i in range(len(neg_years)):
                        year = int(neg_years[i])
                        negative_samples.append({
                            'year': year,
                            'day_of_year': int(neg_days[i]),
                            'date': datetime.strptime(decoded_dates[i], '%Y-%m-%d'),
                            'pixel_row': int(neg_rows[i]),
                            'pixel_col': int(neg_cols[i]),
                            'firms_value': float(neg_firms[i]),
                            'file_path': self.year_files[year]
                        })
                
                load_time = time.time() - start_time
                logger.info(f"ä»ç¼“å­˜åŠ è½½æ ·æœ¬: {len(positive_samples)} æ­£æ ·æœ¬, {len(negative_samples)} è´Ÿæ ·æœ¬ (è€—æ—¶: {load_time:.2f}ç§’)")
                return positive_samples, negative_samples
                
        except Exception as e:
            logger.error(f"ä»ç¼“å­˜åŠ è½½æ ·æœ¬å¤±è´¥: {e}")
            # åˆ é™¤æ— æ•ˆç¼“å­˜
            try:
                os.remove(cache_file)
            except:
                pass
            
            # é‡æ–°é‡‡æ ·
            return self._sample_data_with_cache()
    
    def _save_samples_to_cache(self, cache_file, positive_samples, negative_samples, global_no_fire_days):
        """ä¿å­˜æ ·æœ¬åˆ°ç¼“å­˜"""
        try:
            with h5py.File(cache_file, 'w') as f:
                # ä¿å­˜ç¼“å­˜å‚æ•°
                cache_params = {
                    'h5_dir': self.h5_dir,
                    'years': sorted(self.year_files.keys()),
                    'lookback_seq': self.lookback_seq,
                    'forecast_hor': self.forecast_hor,
                    'min_fire_threshold': self.min_fire_threshold,
                    'version': '1.0'
                }
                f.attrs['cache_params'] = json.dumps(cache_params)
                
                # ä¿å­˜æºæ–‡ä»¶ä¿®æ”¹æ—¶é—´
                source_file_mtimes = {}
                for year, file_path in self.year_files.items():
                    if os.path.exists(file_path):
                        source_file_mtimes[str(year)] = os.path.getmtime(file_path)
                f.attrs['source_file_mtimes'] = json.dumps(source_file_mtimes)
                
                # ä¿å­˜å…ƒæ•°æ®
                f.attrs['total_positive'] = len(positive_samples)
                f.attrs['total_negative'] = len(negative_samples)
                f.attrs['global_no_fire_days'] = len(global_no_fire_days)
                f.attrs['creation_time'] = datetime.now().isoformat()
                
                # ä¿å­˜æ­£æ ·æœ¬
                if positive_samples:
                    pos_group = f.create_group('positive_samples')
                    pos_group.create_dataset('year', data=[s['year'] for s in positive_samples])
                    pos_group.create_dataset('day_of_year', data=[s['day_of_year'] for s in positive_samples])
                    pos_group.create_dataset('date', data=[s['date'].strftime('%Y-%m-%d').encode() for s in positive_samples])
                    pos_group.create_dataset('pixel_row', data=[s['pixel_row'] for s in positive_samples])
                    pos_group.create_dataset('pixel_col', data=[s['pixel_col'] for s in positive_samples])
                    pos_group.create_dataset('firms_value', data=[s['firms_value'] for s in positive_samples])
                
                # ä¿å­˜è´Ÿæ ·æœ¬
                if negative_samples:
                    neg_group = f.create_group('negative_samples')
                    neg_group.create_dataset('year', data=[s['year'] for s in negative_samples])
                    neg_group.create_dataset('day_of_year', data=[s['day_of_year'] for s in negative_samples])
                    neg_group.create_dataset('date', data=[s['date'].strftime('%Y-%m-%d').encode() for s in negative_samples])
                    neg_group.create_dataset('pixel_row', data=[s['pixel_row'] for s in negative_samples])
                    neg_group.create_dataset('pixel_col', data=[s['pixel_col'] for s in negative_samples])
                    neg_group.create_dataset('firms_value', data=[s['firms_value'] for s in negative_samples])
                
            logger.info(f"æ ·æœ¬å·²ä¿å­˜åˆ°ç¼“å­˜: {cache_file}")
            
        except Exception as e:
            logger.error(f"ä¿å­˜æ ·æœ¬åˆ°ç¼“å­˜å¤±è´¥: {e}")
            # åˆ é™¤æŸåçš„ç¼“å­˜æ–‡ä»¶
            try:
                os.remove(cache_file)
            except:
                pass
    
    def _build_or_load_samples(self):
        """æ„å»ºæˆ–åŠ è½½æ ·æœ¬ç´¢å¼•"""
        positive_samples, negative_samples = self._sample_data_with_cache()
        
        # åˆå¹¶æ ·æœ¬å¹¶æ„å»ºç´¢å¼•
        all_samples = positive_samples + negative_samples
        
        self.sample_index = []
        self.dataset_info = {}
        
        # æ‰¹é‡å¤„ç†æ ·æœ¬ï¼Œå‡å°‘é‡å¤è®¡ç®—
        valid_samples = []
        for sample in all_samples:
            # æ£€æŸ¥æ ·æœ¬æ˜¯å¦æœ‰æ•ˆï¼ˆå†å²å’Œæœªæ¥æ•°æ®å……è¶³ï¼‰
            if self._is_sample_valid(sample):
                valid_samples.append(sample)
        
        # æ‰¹é‡æ„å»ºæ ·æœ¬ç´¢å¼•
        for sample in valid_samples:
            pixel_row = sample['pixel_row']
            pixel_col = sample['pixel_col']
            date_obj = sample['date']
            
            sample_metadata = {
                'year': sample['year'],
                'day_of_year': sample['day_of_year'],
                'date': date_obj,
                'date_int': int(date_obj.strftime('%Y%m%d')),
                'pixel_row': pixel_row,
                'pixel_col': pixel_col,
                'firms_value': sample['firms_value'],
                'row': pixel_row,  # å…¼å®¹åŸæ¥å£
                'col': pixel_col,  # å…¼å®¹åŸæ¥å£
                'sample_type': 'positive' if sample['firms_value'] >= self.min_fire_threshold else 'negative'
            }
            
            self.sample_index.append((sample['file_path'], f"{pixel_row}_{pixel_col}", sample_metadata))
        
        # æ„å»ºæ•°æ®é›†ä¿¡æ¯
        for year, file_path in self.year_files.items():
            try:
                with h5py.File(file_path, 'r') as f:
                    self.dataset_info[file_path] = {
                        'year': year,
                        'total_time_steps': int(f.attrs.get('total_time_steps', 365)),
                        'total_channels': int(f.attrs.get('total_channels', 39)),
                        'past_days': self.lookback_seq,
                        'future_days': self.forecast_hor
                    }
            except Exception as e:
                logger.warning(f"è¯»å–æ•°æ®é›†ä¿¡æ¯å¤±è´¥: {file_path}, {e}")
    
    def _is_sample_valid(self, sample):
        """æ£€æŸ¥æ ·æœ¬æ˜¯å¦æœ‰æ•ˆï¼ˆå†å²å’Œæœªæ¥æ•°æ®å……è¶³ï¼‰"""
        target_date = sample['date']
        
        # æ£€æŸ¥å†å²æ•°æ®
        history_start = target_date - timedelta(days=self.lookback_seq - 1)
        if history_start.year < min(self.year_files.keys()):
            return False
        
        # æ£€æŸ¥æœªæ¥æ•°æ®
        future_end = target_date + timedelta(days=self.forecast_hor - 1)
        if future_end.year > max(self.year_files.keys()):
            return False
        
        return True
    
    def _load_pixel_data_for_date_range(self, pixel_row, pixel_col, start_date, end_date):
        """åŠ è½½æŒ‡å®šåƒç´ åœ¨æŒ‡å®šæ—¥æœŸèŒƒå›´å†…çš„æ•°æ® - æ€§èƒ½ä¼˜åŒ–ç‰ˆæœ¬"""
        # ä½¿ç”¨ç¼“å­˜æ£€æŸ¥
        cache_key = f"{pixel_row}_{pixel_col}_{start_date.strftime('%Y%m%d')}_{end_date.strftime('%Y%m%d')}"
        cached_data = get_data_cache().get(cache_key)
        if cached_data is not None:
            return cached_data
        
        # æŒ‰å¹´ä»½åˆ†ç»„å¤„ç†æ—¥æœŸèŒƒå›´
        year_ranges = {}
        current_date = start_date
        
        while current_date <= end_date:
            year = current_date.year
            if year not in year_ranges:
                year_ranges[year] = []
            year_ranges[year].append(current_date)
            current_date += timedelta(days=1)
        
        data_segments = []
        file_handle_manager = get_file_handle_manager()
        
        # æŒ‰å¹´ä»½æ‰¹é‡åŠ è½½æ•°æ®
        for year in sorted(year_ranges.keys()):
            year_dates = year_ranges[year]
            
            if year not in self.year_files:
                # ç”¨NaNå¡«å……ç¼ºå¤±å¹´ä»½çš„æ‰€æœ‰å¤©
                data_segments.append(np.full((39, len(year_dates)), np.nan))
                continue
            
            try:
                # ä½¿ç”¨æ–‡ä»¶å¥æŸ„ç®¡ç†å™¨è·å–æ–‡ä»¶å¥æŸ„
                f = file_handle_manager.get_handle(self.year_files[year])
                if f is None:
                    data_segments.append(np.full((39, len(year_dates)), np.nan))
                    continue
                
                dataset_name = f"{pixel_row}_{pixel_col}"
                
                if dataset_name not in f:
                    # åƒç´ ä¸å­˜åœ¨ï¼Œç”¨NaNå¡«å……
                    data_segments.append(np.full((39, len(year_dates)), np.nan))
                    continue
                
                # ä¸€æ¬¡æ€§åŠ è½½æ•´ä¸ªåƒç´ çš„å¹´åº¦æ•°æ®
                pixel_data = f[dataset_name][:]  # shape: (channels, time_steps)
                
                # æ‰¹é‡è®¡ç®—æ‰€æœ‰æ—¥æœŸçš„ç´¢å¼•
                year_start = datetime(year, 1, 1)
                day_indices = [(date - year_start).days for date in year_dates]
                
                # å‘é‡åŒ–æå–æ•°æ®
                year_data_list = []
                for day_idx in day_indices:
                    if day_idx >= pixel_data.shape[1] or day_idx < 0:
                        year_data_list.append(np.full((39, 1), np.nan))
                    else:
                        year_data_list.append(pixel_data[:, day_idx:day_idx+1])
                
                # åˆå¹¶å¹´åº¦æ•°æ®
                if year_data_list:
                    year_data = np.concatenate(year_data_list, axis=1)
                    data_segments.append(year_data)
                else:
                    data_segments.append(np.full((39, len(year_dates)), np.nan))
                        
            except Exception as e:
                logger.warning(f"åŠ è½½åƒç´ æ•°æ®å¤±è´¥: {pixel_row}_{pixel_col}, å¹´ä»½{year}, {e}")
                data_segments.append(np.full((39, len(year_dates)), np.nan))
        
        # åˆå¹¶æ‰€æœ‰æ•°æ®æ®µ
        if data_segments:
            result = np.concatenate(data_segments, axis=1)
        else:
            result = np.full((39, 0), np.nan)
        
        # ç¼“å­˜ç»“æœ
        get_data_cache().put(cache_key, result)
        
        return result
    
    def __len__(self):
        """è¿”å›æ ·æœ¬æ•°é‡"""
        return len(self.sample_index)
    
    def __getitem__(self, idx):
        """
        è·å–å•ä¸ªæ ·æœ¬ - æ€§èƒ½ä¼˜åŒ–ç‰ˆæœ¬
        
        Returns:
            - past_data: (channels, past_time_steps) 
            - future_data: (channels, future_time_steps)
            - metadata (å¯é€‰): [date_int, row, col]
        """
        file_path, dataset_key, metadata = self.sample_index[idx]
        
        try:
            target_date = metadata['date']
            pixel_row = metadata['pixel_row']
            pixel_col = metadata['pixel_col']
            
            # è®¡ç®—å®Œæ•´æ•°æ®èŒƒå›´ï¼ˆå†å²+æœªæ¥ï¼‰
            history_start = target_date - timedelta(days=self.lookback_seq - 1)
            future_end = target_date + timedelta(days=self.forecast_hor - 1)
            
            # ä¸€æ¬¡æ€§åŠ è½½å®Œæ•´æ•°æ®èŒƒå›´ï¼Œå‡å°‘I/Oæ“ä½œ
            full_data = self._load_pixel_data_for_date_range(pixel_row, pixel_col, history_start, future_end)
            
            # åˆ†å‰²å†å²å’Œæœªæ¥æ•°æ®
            past_data = full_data[:, :self.lookback_seq]
            future_data = full_data[:, self.lookback_seq:]
            
            # æ£€æŸ¥æ•°æ®ç»´åº¦å¹¶å¤„ç†ä¸åŒ¹é…æƒ…å†µ
            if past_data.shape[1] != self.lookback_seq:
                # å¤„ç†å†å²æ•°æ®é•¿åº¦ä¸åŒ¹é…ï¼ˆå¯èƒ½æ˜¯é—°å¹´/å¹³å¹´å¯¼è‡´ï¼‰
                if past_data.shape[1] < self.lookback_seq:
                    # æ•°æ®ä¸è¶³ï¼Œåœ¨å¼€å¤´å¡«å……
                    padding_days = self.lookback_seq - past_data.shape[1]
                    padding = np.zeros((39, padding_days))
                    past_data = np.concatenate([padding, past_data], axis=1)
                    logger.debug(f"å†å²æ•°æ®é•¿åº¦ä¸è¶³: æœŸæœ›{self.lookback_seq}, å®é™…{past_data.shape[1]-padding_days}, å·²å¡«å……{padding_days}å¤©")
                else:
                    # æ•°æ®è¿‡å¤šï¼Œæˆªå–æœ€è¿‘çš„éƒ¨åˆ†
                    past_data = past_data[:, -self.lookback_seq:]
                    logger.debug(f"å†å²æ•°æ®è¿‡é•¿: æœŸæœ›{self.lookback_seq}, å®é™…{past_data.shape[1]}, å·²æˆªå–æœ€è¿‘{self.lookback_seq}å¤©")
            
            if future_data.shape[1] != self.forecast_hor:
                # å¤„ç†æœªæ¥æ•°æ®é•¿åº¦ä¸åŒ¹é…
                if future_data.shape[1] < self.forecast_hor:
                    # æ•°æ®ä¸è¶³ï¼Œåœ¨æœ«å°¾å¡«å……
                    padding_days = self.forecast_hor - future_data.shape[1]
                    padding = np.zeros((39, padding_days))
                    future_data = np.concatenate([future_data, padding], axis=1)
                    logger.debug(f"æœªæ¥æ•°æ®é•¿åº¦ä¸è¶³: æœŸæœ›{self.forecast_hor}, å®é™…{future_data.shape[1]-padding_days}, å·²å¡«å……{padding_days}å¤©")
                else:
                    # æ•°æ®è¿‡å¤šï¼Œæˆªå–å‰é¢éƒ¨åˆ†
                    future_data = future_data[:, :self.forecast_hor]
                    logger.debug(f"æœªæ¥æ•°æ®è¿‡é•¿: æœŸæœ›{self.forecast_hor}, å®é™…{future_data.shape[1]}, å·²æˆªå–å‰{self.forecast_hor}å¤©")
            
            # å¤„ç†NaNå€¼
            past_data = np.nan_to_num(past_data, nan=0.0, posinf=0.0, neginf=0.0)
            future_data = np.nan_to_num(future_data, nan=0.0, posinf=0.0, neginf=0.0)
            
            # è½¬æ¢ä¸ºtorch tensor
            past_data = torch.from_numpy(past_data).float()
            future_data = torch.from_numpy(future_data).float()
            
            # æœ€ç»ˆæ£€æŸ¥
            if torch.isnan(past_data).any() or torch.isinf(past_data).any():
                past_data = torch.nan_to_num(past_data, nan=0.0, posinf=0.0, neginf=0.0)
            
            if torch.isnan(future_data).any() or torch.isinf(future_data).any():
                future_data = torch.nan_to_num(future_data, nan=0.0, posinf=0.0, neginf=0.0)
            
            if self.return_metadata:
                # è¿”å›ç®€åŒ–çš„metadataæ ¼å¼: [æ—¥æœŸ, xåæ ‡, yåæ ‡]
                simplified_metadata = [metadata['date_int'], metadata['row'], metadata['col']]
                return past_data, future_data, simplified_metadata
            else:
                return past_data, future_data
                
        except Exception as e:
            logger.debug(f"è·å–æ ·æœ¬å¤±è´¥: {dataset_key}, {e}")
            raise e
    
    def custom_collate_fn(self, batch):
        """
        è‡ªå®šä¹‰çš„collateå‡½æ•°ï¼Œä¸åŸdataload.pyå®Œå…¨ä¸€è‡´
        """
        if self.return_metadata:
            past_data_list, future_data_list, metadata_list = zip(*batch)
        else:
            past_data_list, future_data_list = zip(*batch)
        
        # æ£€æŸ¥æ‰€æœ‰tensorçš„å½¢çŠ¶
        past_shapes = [data.shape for data in past_data_list]
        future_shapes = [data.shape for data in future_data_list]
        
        # æ£€æŸ¥pastæ•°æ®å½¢çŠ¶ä¸€è‡´æ€§
        if len(set(past_shapes)) > 1:
            raise ValueError(f"Pastæ•°æ®å½¢çŠ¶ä¸ä¸€è‡´: {set(past_shapes)}")
        
        # æ£€æŸ¥futureæ•°æ®å½¢çŠ¶ä¸€è‡´æ€§
        if len(set(future_shapes)) > 1:
            raise ValueError(f"Futureæ•°æ®å½¢çŠ¶ä¸ä¸€è‡´: {set(future_shapes)}")
        
        # ç›´æ¥å †å ï¼Œå½¢çŠ¶å¿…é¡»ä¸€è‡´
        past_batch = torch.stack(past_data_list, dim=0)
        future_batch = torch.stack(future_data_list, dim=0)
        
        if self.return_metadata:
            return past_batch, future_batch, metadata_list
        else:
            return past_batch, future_batch
    
    def resample_for_epoch(self, epoch_seed):
        """
        ä¸ºæ–°çš„epoché‡æ–°è¿›è¡Œæ ·æœ¬æŠ½æ ·
        """
        if not self.resample_each_epoch:
            return
        
        if not hasattr(self, 'full_sample_index') or not self.full_sample_index:
            logger.warning("æ— æ³•é‡æ–°æŠ½æ ·ï¼šæ²¡æœ‰å®Œæ•´æ ·æœ¬ç´¢å¼•")
            return
        
        self.epoch_seed = epoch_seed
        
        # ä½¿ç”¨å®Œæ•´æ ·æœ¬ç´¢å¼•é‡æ–°æŠ½æ ·
        self.sample_index = self.full_sample_index.copy()
        
        # ä½¿ç”¨æ–°çš„éšæœºç§å­é‡æ–°æŠ½æ ·
        self._apply_sample_ratio_filtering(seed=epoch_seed)
    
    def _apply_sample_ratio_filtering(self, seed=None):
        """åº”ç”¨æ­£è´Ÿæ ·æœ¬æ¯”ä¾‹ç­›é€‰"""
        if seed is None:
            seed = 42
        
        if self.verbose_sampling:
            logger.info(f"å¼€å§‹åº”ç”¨æ­£è´Ÿæ ·æœ¬æ¯”ä¾‹ç­›é€‰... (éšæœºç§å­: {seed})")
        
        # åˆ†ç¦»æ­£æ ·æœ¬å’Œè´Ÿæ ·æœ¬
        positive_samples = []
        negative_samples = []
        
        for idx, (file_path, dataset_key, metadata) in enumerate(self.sample_index):
            if metadata['firms_value'] >= self.min_fire_threshold:
                positive_samples.append(idx)
            else:
                negative_samples.append(idx)
        
        positive_count = len(positive_samples)
        negative_count = len(negative_samples)
        
        if self.verbose_sampling:
            logger.info(f"åŸå§‹æ ·æœ¬ç»Ÿè®¡: æ­£æ ·æœ¬ {positive_count} ä¸ª, è´Ÿæ ·æœ¬ {negative_count} ä¸ª")
        
        # è®¡ç®—éœ€è¦ä¿ç•™çš„æ ·æœ¬æ•°
        retained_positive_count = int(positive_count * self.positive_ratio)
        retained_negative_count = int(retained_positive_count * self.pos_neg_ratio)
        
        # ç¡®ä¿ä¸è¶…è¿‡å¯ç”¨æ•°é‡
        retained_negative_count = min(retained_negative_count, negative_count)
        
        if self.verbose_sampling:
            logger.info(f"è®¡åˆ’ä¿ç•™: æ­£æ ·æœ¬ {retained_positive_count} ä¸ª, è´Ÿæ ·æœ¬ {retained_negative_count} ä¸ª")
        
        # éšæœºæŠ½æ ·
        random.seed(seed)
        
        selected_positive_indices = random.sample(positive_samples, retained_positive_count) if retained_positive_count < len(positive_samples) else positive_samples
        selected_negative_indices = random.sample(negative_samples, retained_negative_count) if retained_negative_count < len(negative_samples) else negative_samples
        
        # åˆå¹¶å¹¶é‡å»ºç´¢å¼•
        selected_indices = selected_positive_indices + selected_negative_indices
        
        new_sample_index = []
        for idx in selected_indices:
            new_sample_index.append(self.sample_index[idx])
        
        self.sample_index = new_sample_index
        
        if self.verbose_sampling:
            logger.info(f"æ ·æœ¬ç­›é€‰å®Œæˆ: æ­£æ ·æœ¬ {len(selected_positive_indices)} ä¸ª, è´Ÿæ ·æœ¬ {len(selected_negative_indices)} ä¸ª")
    
    def get_current_sample_stats(self):
        """è·å–å½“å‰æ ·æœ¬çš„ç»Ÿè®¡ä¿¡æ¯"""
        positive_count = 0
        negative_count = 0
        
        for _, _, metadata in self.sample_index:
            if metadata['firms_value'] >= self.min_fire_threshold:
                positive_count += 1
            else:
                negative_count += 1
        
        return {
            'total_samples': len(self.sample_index),
            'positive_samples': positive_count,
            'negative_samples': negative_count,
            'positive_ratio': positive_count / len(self.sample_index) if len(self.sample_index) > 0 else 0,
            'pos_neg_ratio': negative_count / positive_count if positive_count > 0 else 0
        }
    
    def get_statistics(self):
        """è·å–æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            'total_samples': len(self.sample_index),
            'years': set(),
            'firms_values': set(),
            'sample_types': set(),
            'files': len(self.year_files)
        }
        
        for file_path, dataset_key, metadata in self.sample_index:
            stats['years'].add(metadata['year'])
            stats['firms_values'].add(metadata['firms_value'])
            stats['sample_types'].add(metadata['sample_type'])
        
        # è½¬æ¢ä¸ºåˆ—è¡¨å¹¶æ’åº
        stats['years'] = sorted(list(stats['years']))
        stats['firms_values'] = sorted(list(stats['firms_values']))
        stats['sample_types'] = sorted(list(stats['sample_types']))
        
        return stats
    
    def get_dataset_info(self):
        """è·å–æ•°æ®é›†ä¿¡æ¯"""
        return self.dataset_info
    
    def get_performance_stats(self):
        """è·å–æ€§èƒ½ç»Ÿè®¡ä¿¡æ¯"""
        if not self.enable_performance_optimizations:
            return {"performance_optimizations": False}
        
        cache_stats = get_data_cache().get_stats()
        return {
            "performance_optimizations": True,
            "data_cache": cache_stats,
            "file_handle_manager": {
                "active_handles": len(get_file_handle_manager().handles),
                "max_handles": get_file_handle_manager().max_handles
            }
        }
    
    def clear_performance_caches(self):
        """æ¸…ç©ºæ€§èƒ½ç¼“å­˜"""
        if self.enable_performance_optimizations:
            get_data_cache().clear()
            get_file_handle_manager().close_all()
            logger.info("ğŸ§¹ æ€§èƒ½ç¼“å­˜å·²æ¸…ç©º")
        else:
            logger.info("âš ï¸  æ€§èƒ½ä¼˜åŒ–æœªå¯ç”¨ï¼Œæ— éœ€æ¸…ç©ºç¼“å­˜")


class YearTimeSeriesDataLoader:
    """
    å¹´ä»½æ—¶é—´åºåˆ—æ•°æ®åŠ è½½å™¨ - ä¸åŸTimeSeriesDataLoaderå®Œå…¨å…¼å®¹
    """
    
    def __init__(self, h5_dir, positive_ratio=1.0, pos_neg_ratio=1.0, 
                 resample_each_epoch=False, verbose_sampling=True,
                 lookback_seq=365, forecast_hor=7, min_fire_threshold=0.001,
                 cache_dir=None, force_resample=False, 
                 enable_performance_optimizations=True, max_file_handles=50, 
                 data_cache_size_mb=1024, **kwargs):
        """
        åˆå§‹åŒ–å¹´ä»½æ—¶é—´åºåˆ—æ•°æ®åŠ è½½å™¨
        
        Args:
            h5_dir: å¹´ä»½æ•°æ®H5æ–‡ä»¶ç›®å½•
            positive_ratio: æ­£æ ·æœ¬ä½¿ç”¨æ¯”ä¾‹
            pos_neg_ratio: æ­£è´Ÿæ ·æœ¬æ¯”ä¾‹
            resample_each_epoch: æ˜¯å¦åœ¨æ¯ä¸ªepoché‡æ–°æŠ½æ ·
            verbose_sampling: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†é‡‡æ ·ä¿¡æ¯
            lookback_seq: å†å²æ—¶é—´é•¿åº¦
            forecast_hor: æœªæ¥æ—¶é—´é•¿åº¦
            min_fire_threshold: FIRMSé˜ˆå€¼
            cache_dir: ç¼“å­˜ç›®å½•
            force_resample: æ˜¯å¦å¼ºåˆ¶é‡æ–°é‡‡æ ·
            enable_performance_optimizations: æ˜¯å¦å¯ç”¨æ€§èƒ½ä¼˜åŒ–
            max_file_handles: æœ€å¤§æ–‡ä»¶å¥æŸ„ç¼“å­˜æ•°é‡
            data_cache_size_mb: æ•°æ®ç¼“å­˜å¤§å°ï¼ˆMBï¼‰
            **kwargs: å…¶ä»–å‚æ•°ï¼ˆä¸ºäº†å…¼å®¹æ€§ï¼‰
        """
        self.h5_dir = h5_dir
        self.positive_ratio = positive_ratio
        self.pos_neg_ratio = pos_neg_ratio
        self.resample_each_epoch = resample_each_epoch
        self.verbose_sampling = verbose_sampling
        self.lookback_seq = lookback_seq
        self.forecast_hor = forecast_hor
        self.min_fire_threshold = min_fire_threshold
        self.cache_dir = cache_dir
        self.force_resample = force_resample
        self.enable_performance_optimizations = enable_performance_optimizations
        
        # åˆ›å»ºæ•°æ®é›†
        self.dataset = YearTimeSeriesPixelDataset(
            h5_dir=h5_dir,
            positive_ratio=positive_ratio,
            pos_neg_ratio=pos_neg_ratio,
            resample_each_epoch=resample_each_epoch,
            verbose_sampling=verbose_sampling,
            lookback_seq=lookback_seq,
            forecast_hor=forecast_hor,
            min_fire_threshold=min_fire_threshold,
            cache_dir=cache_dir,
            force_resample=force_resample,
            enable_performance_optimizations=enable_performance_optimizations,
            max_file_handles=max_file_handles,
            data_cache_size_mb=data_cache_size_mb
        )
        
        logger.info(f"å¹´ä»½æ—¶é—´åºåˆ—æ•°æ®åŠ è½½å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def get_year_based_split(self, train_years, val_years, test_years, test_full_years=None):
        """
        åŸºäºå¹´ä»½è¿›è¡Œæ•°æ®åˆ’åˆ† - ä¸åŸæ¥å£å®Œå…¨ä¸€è‡´
        """
        train_indices = []
        val_indices = []
        test_indices = []
        
        for idx, (file_path, dataset_key, metadata) in enumerate(self.dataset.sample_index):
            year = metadata['year']
            
            if year in train_years:
                train_indices.append(idx)
            elif year in val_years:
                val_indices.append(idx)
            elif year in test_years:
                test_indices.append(idx)
        
        if self.verbose_sampling:
            logger.info(f"å¹´ä»½åˆ’åˆ†ç»“æœ:")
            logger.info(f"  è®­ç»ƒé›†: {len(train_indices)} æ ·æœ¬ (å¹´ä»½: {train_years})")
            logger.info(f"  éªŒè¯é›†: {len(val_indices)} æ ·æœ¬ (å¹´ä»½: {val_years})")
            logger.info(f"  æµ‹è¯•é›†: {len(test_indices)} æ ·æœ¬ (å¹´ä»½: {test_years})")
        
        # å¦‚æœéœ€è¦å®Œæ•´æ•°æ®æµ‹è¯•é›†
        if test_full_years is not None:
            full_dataset = YearTimeSeriesPixelDataset(
                h5_dir=self.h5_dir,
                years=test_full_years,
                positive_ratio=1.0,
                pos_neg_ratio=999999,  # ä½¿ç”¨æ‰€æœ‰è´Ÿæ ·æœ¬
                resample_each_epoch=False,
                verbose_sampling=self.verbose_sampling,
                lookback_seq=self.lookback_seq,
                forecast_hor=self.forecast_hor,
                min_fire_threshold=self.min_fire_threshold,
                cache_dir=self.cache_dir,
                force_resample=self.force_resample
            )
            
            test_full_indices = list(range(len(full_dataset)))
            logger.info(f"å®Œæ•´æ•°æ®æµ‹è¯•é›†: {len(test_full_indices)} æ ·æœ¬ (å¹´ä»½: {test_full_years})")
            
            return train_indices, val_indices, test_indices, test_full_indices, full_dataset
        
        return train_indices, val_indices, test_indices
    
    def create_optimized_dataloader(self, indices, batch_size=32, shuffle=True, 
                                   num_workers=None, pin_memory=True, 
                                   persistent_workers=True, prefetch_factor=2):
        """
        åˆ›å»ºä¼˜åŒ–çš„DataLoader
        
        Args:
            indices: æ ·æœ¬ç´¢å¼•åˆ—è¡¨
            batch_size: æ‰¹æ¬¡å¤§å°
            shuffle: æ˜¯å¦æ‰“ä¹±
            num_workers: å·¥ä½œè¿›ç¨‹æ•°ï¼ŒNoneè¡¨ç¤ºè‡ªåŠ¨è®¾ç½®
            pin_memory: æ˜¯å¦ä½¿ç”¨pin_memory
            persistent_workers: æ˜¯å¦ä½¿ç”¨æŒä¹…åŒ–å·¥ä½œè¿›ç¨‹
            prefetch_factor: é¢„å–å› å­
        """
        from torch.utils.data import Subset
        
        # è‡ªåŠ¨è®¾ç½®å·¥ä½œè¿›ç¨‹æ•°
        if num_workers is None:
            if self.enable_performance_optimizations:
                num_workers = min(4, os.cpu_count() or 1)
            else:
                num_workers = 0
        
        # åˆ›å»ºå­é›†
        subset = Subset(self.dataset, indices)
        
        # ä¼˜åŒ–çš„DataLoaderé…ç½®
        dataloader_kwargs = {
            'batch_size': batch_size,
            'shuffle': shuffle,
            'num_workers': num_workers,
            'collate_fn': self.dataset.custom_collate_fn,
            'pin_memory': pin_memory and torch.cuda.is_available(),
            'drop_last': True if shuffle else False,  # è®­ç»ƒæ—¶ä¸¢å¼ƒæœ€åä¸€ä¸ªä¸å®Œæ•´æ‰¹æ¬¡
        }
        
        # å¤šè¿›ç¨‹ä¼˜åŒ–
        if num_workers > 0:
            dataloader_kwargs.update({
                'persistent_workers': persistent_workers,
                'prefetch_factor': prefetch_factor,
            })
        
        dataloader = DataLoader(subset, **dataloader_kwargs)
        
        if self.dataset.verbose_sampling:
            logger.info(f"ğŸš€ åˆ›å»ºä¼˜åŒ–DataLoader: æ‰¹æ¬¡å¤§å°={batch_size}, å·¥ä½œè¿›ç¨‹={num_workers}, "
                       f"æ ·æœ¬æ•°={len(indices)}, æ€§èƒ½ä¼˜åŒ–={'å¯ç”¨' if self.enable_performance_optimizations else 'ç¦ç”¨'}")
        
        return dataloader


class YearFullDatasetLoader:
    """
    å¹´ä»½å®Œæ•´æ•°æ®é›†åŠ è½½å™¨ - ä¸åŸFullDatasetLoaderå®Œå…¨å…¼å®¹
    """
    
    def __init__(self, h5_dir, years=None, return_metadata=True, 
                 lookback_seq=365, forecast_hor=7, min_fire_threshold=0.001,
                 cache_dir=None):
        """
        åˆå§‹åŒ–å¹´ä»½å®Œæ•´æ•°æ®é›†åŠ è½½å™¨
        """
        self.h5_dir = h5_dir
        self.years = years
        self.return_metadata = return_metadata
        self.lookback_seq = lookback_seq
        self.forecast_hor = forecast_hor
        self.min_fire_threshold = min_fire_threshold
        self.cache_dir = cache_dir
        
        # åˆ›å»ºåŸºç¡€æ•°æ®é›†ï¼Œä¸è¿›è¡Œä»»ä½•é‡‡æ ·
        self.dataset = YearTimeSeriesPixelDataset(
            h5_dir=h5_dir,
            years=years,
            return_metadata=return_metadata,
            positive_ratio=1.0,
            pos_neg_ratio=999999,  # ä½¿ç”¨æ‰€æœ‰è´Ÿæ ·æœ¬
            resample_each_epoch=False,
            verbose_sampling=True,
            lookback_seq=lookback_seq,
            forecast_hor=forecast_hor,
            min_fire_threshold=min_fire_threshold,
            cache_dir=cache_dir,
            force_resample=False
        )
        
        logger.info(f"å¹´ä»½å®Œæ•´æ•°æ®é›†åŠ è½½å™¨åˆå§‹åŒ–å®Œæˆï¼Œå…± {len(self.dataset)} ä¸ªæ ·æœ¬")
        
        # è·å–ç»Ÿè®¡ä¿¡æ¯
        stats = self.dataset.get_statistics()
        logger.info(f"æ•°æ®é›†ç»Ÿè®¡: {stats}")
    
    def create_dataloader(self, batch_size=32, shuffle=False, num_workers=4, worker_init_fn=None, **dataloader_kwargs):
        """
        åˆ›å»ºPyTorch DataLoader
        """
        return DataLoader(
            self.dataset,
            batch_size=batch_size,
            shuffle=shuffle,
            num_workers=num_workers,
            collate_fn=self.dataset.custom_collate_fn,
            worker_init_fn=worker_init_fn,
            **dataloader_kwargs
        )


# ä¸ºäº†å®Œå…¨å…¼å®¹åŸæ¥å£ï¼Œåˆ›å»ºåˆ«å
TimeSeriesDataLoader = YearTimeSeriesDataLoader
TimeSeriesPixelDataset = YearTimeSeriesPixelDataset
FullDatasetLoader = YearFullDatasetLoader


# æµ‹è¯•å‡½æ•°
def test_year_dataloader():
    """æµ‹è¯•å¹´ä»½æ•°æ®åŠ è½½å™¨ - æ€§èƒ½ä¼˜åŒ–ç‰ˆæœ¬"""
    
    # æµ‹è¯•å‚æ•°
    h5_dir = "/mnt/raid/zhengsen/wildfire_dataset/self_built_materials/year_datasets_h5"
    
    try:
        logger.info("ğŸš€ å¼€å§‹æµ‹è¯•å¹´ä»½æ•°æ®åŠ è½½å™¨ï¼ˆæ€§èƒ½ä¼˜åŒ–ç‰ˆæœ¬ï¼‰...")
        
        # æµ‹è¯•1: æ€§èƒ½ä¼˜åŒ–å¯ç”¨
        logger.info("ğŸ“Š æµ‹è¯•1: æ€§èƒ½ä¼˜åŒ–å¯ç”¨...")
        start_time = time.time()
        
        # åˆ›å»ºä¼˜åŒ–çš„æ•°æ®åŠ è½½å™¨
        data_loader = YearTimeSeriesDataLoader(
            h5_dir=h5_dir,
            positive_ratio=0.1,
            pos_neg_ratio=2.0,
            lookback_seq=365,
            forecast_hor=7,
            min_fire_threshold=1.0,
            verbose_sampling=True,
            force_resample=False,  # ä½¿ç”¨ç¼“å­˜
            enable_performance_optimizations=True,
            max_file_handles=50,
            data_cache_size_mb=1024
        )
        
        init_time_optimized = time.time() - start_time
        logger.info(f"â° ä¼˜åŒ–ç‰ˆæ•°æ®åŠ è½½å™¨åˆå§‹åŒ–è€—æ—¶: {init_time_optimized:.2f}ç§’")
        
        # æ˜¾ç¤ºæ€§èƒ½ç»Ÿè®¡
        perf_stats = data_loader.dataset.get_performance_stats()
        logger.info(f"ğŸ“Š æ€§èƒ½ç»Ÿè®¡: {perf_stats}")
        
        # æµ‹è¯•2: æ€§èƒ½ä¼˜åŒ–ç¦ç”¨å¯¹æ¯”
        logger.info("ğŸ“Š æµ‹è¯•2: æ€§èƒ½ä¼˜åŒ–ç¦ç”¨å¯¹æ¯”...")
        start_time = time.time()
        
        data_loader_no_opt = YearTimeSeriesDataLoader(
            h5_dir=h5_dir,
            positive_ratio=0.1,
            pos_neg_ratio=2.0,
            lookback_seq=365,
            forecast_hor=7,
            min_fire_threshold=1.0,
            verbose_sampling=True,
            force_resample=False,
            enable_performance_optimizations=False
        )
        
        init_time_no_opt = time.time() - start_time
        logger.info(f"â° æœªä¼˜åŒ–ç‰ˆæ•°æ®åŠ è½½å™¨åˆå§‹åŒ–è€—æ—¶: {init_time_no_opt:.2f}ç§’")
        logger.info(f"ğŸš€ åˆå§‹åŒ–åŠ é€Ÿæ¯”: {init_time_no_opt/init_time_optimized:.2f}x")
        
        # æµ‹è¯•3: å¹´ä»½åˆ’åˆ†
        logger.info("ğŸ“Š æµ‹è¯•3: å¹´ä»½åˆ’åˆ†...")
        train_indices, val_indices, test_indices = data_loader.get_year_based_split(
            train_years=[2021, 2022],
            val_years=[2023],
            test_years=[2024]
        )
        
        logger.info(f"âœ… è®­ç»ƒé›†: {len(train_indices)} æ ·æœ¬")
        logger.info(f"âœ… éªŒè¯é›†: {len(val_indices)} æ ·æœ¬")
        logger.info(f"âœ… æµ‹è¯•é›†: {len(test_indices)} æ ·æœ¬")
        
        # æµ‹è¯•4: æ•°æ®åŠ è½½æ€§èƒ½å¯¹æ¯”
        logger.info("ğŸ“Š æµ‹è¯•4: æ•°æ®åŠ è½½æ€§èƒ½å¯¹æ¯”...")
        if train_indices:
            sample_idx = train_indices[0]
            
            # æµ‹è¯•ä¼˜åŒ–ç‰ˆæœ¬å•ä¸ªæ ·æœ¬åŠ è½½æ—¶é—´
            start_time = time.time()
            past_data, future_data, metadata = data_loader.dataset[sample_idx]
            sample_time_optimized = time.time() - start_time
            
            # æµ‹è¯•æœªä¼˜åŒ–ç‰ˆæœ¬å•ä¸ªæ ·æœ¬åŠ è½½æ—¶é—´
            start_time = time.time()
            past_data_no_opt, future_data_no_opt, metadata_no_opt = data_loader_no_opt.dataset[sample_idx]
            sample_time_no_opt = time.time() - start_time
            
            logger.info(f"â° ä¼˜åŒ–ç‰ˆå•æ ·æœ¬åŠ è½½è€—æ—¶: {sample_time_optimized:.4f}ç§’")
            logger.info(f"â° æœªä¼˜åŒ–ç‰ˆå•æ ·æœ¬åŠ è½½è€—æ—¶: {sample_time_no_opt:.4f}ç§’")
            logger.info(f"ğŸš€ å•æ ·æœ¬åŠ è½½åŠ é€Ÿæ¯”: {sample_time_no_opt/sample_time_optimized:.2f}x")
            
            logger.info(f"âœ… æ ·æœ¬å½¢çŠ¶: past={past_data.shape}, future={future_data.shape}")
            logger.info(f"âœ… å…ƒæ•°æ®: {metadata}")
            
            # æ£€æŸ¥æ•°æ®è´¨é‡
            logger.info("ğŸ“Š æµ‹è¯•5: æ•°æ®è´¨é‡æ£€æŸ¥...")
            logger.info(f"âœ… Pastæ•°æ®èŒƒå›´: [{past_data.min():.4f}, {past_data.max():.4f}]")
            logger.info(f"âœ… Futureæ•°æ®èŒƒå›´: [{future_data.min():.4f}, {future_data.max():.4f}]")
            logger.info(f"âœ… NaNæ£€æŸ¥: Past={torch.isnan(past_data).sum()}, Future={torch.isnan(future_data).sum()}")
            
            # éªŒè¯æ•°æ®ä¸€è‡´æ€§
            logger.info("ğŸ“Š æµ‹è¯•6: æ•°æ®ä¸€è‡´æ€§éªŒè¯...")
            data_equal = torch.allclose(past_data, past_data_no_opt, rtol=1e-5, atol=1e-8)
            logger.info(f"âœ… ä¼˜åŒ–å‰åæ•°æ®ä¸€è‡´æ€§: {'é€šè¿‡' if data_equal else 'å¤±è´¥'}")
        
        # æµ‹è¯•7: ä¼˜åŒ–DataLoaderæ‰¹å¤„ç†æ€§èƒ½
        logger.info("ğŸ“Š æµ‹è¯•7: ä¼˜åŒ–DataLoaderæ‰¹å¤„ç†æ€§èƒ½...")
        test_sample_size = min(100, len(train_indices))
        
        # æµ‹è¯•ä¼˜åŒ–ç‰ˆDataLoader
        train_loader_optimized = data_loader.create_optimized_dataloader(
            train_indices[:test_sample_size],
            batch_size=8,
            shuffle=True,
            num_workers=2
        )
        
        start_time = time.time()
        for i, batch in enumerate(train_loader_optimized):
            past_batch, future_batch, metadata_batch = batch
            if i == 0:
                logger.info(f"âœ… æ‰¹æ¬¡å½¢çŠ¶: past={past_batch.shape}, future={future_batch.shape}, metadata={len(metadata_batch)}")
            if i >= 4:  # åªæµ‹è¯•å‰5ä¸ªæ‰¹æ¬¡
                break
        
        batch_time_optimized = time.time() - start_time
        logger.info(f"â° ä¼˜åŒ–ç‰ˆæ‰¹å¤„ç†è€—æ—¶: {batch_time_optimized:.2f}ç§’ (5ä¸ªæ‰¹æ¬¡)")
        
        # æµ‹è¯•æœªä¼˜åŒ–ç‰ˆDataLoader
        from torch.utils.data import Subset
        train_dataset_no_opt = Subset(data_loader_no_opt.dataset, train_indices[:test_sample_size])
        train_loader_no_opt = DataLoader(
            train_dataset_no_opt,
            batch_size=8,
            shuffle=True,
            collate_fn=data_loader_no_opt.dataset.custom_collate_fn,
            num_workers=0  # ç¦ç”¨å¤šè¿›ç¨‹é¿å…å¹²æ‰°
        )
        
        start_time = time.time()
        for i, batch in enumerate(train_loader_no_opt):
            if i >= 4:  # åªæµ‹è¯•å‰5ä¸ªæ‰¹æ¬¡
                break
        
        batch_time_no_opt = time.time() - start_time
        logger.info(f"â° æœªä¼˜åŒ–ç‰ˆæ‰¹å¤„ç†è€—æ—¶: {batch_time_no_opt:.2f}ç§’ (5ä¸ªæ‰¹æ¬¡)")
        logger.info(f"ğŸš€ æ‰¹å¤„ç†åŠ é€Ÿæ¯”: {batch_time_no_opt/batch_time_optimized:.2f}x")
        
        # æµ‹è¯•8: æ ·æœ¬ç»Ÿè®¡
        logger.info("ğŸ“Š æµ‹è¯•8: æ ·æœ¬ç»Ÿè®¡...")
        stats = data_loader.dataset.get_current_sample_stats()
        logger.info(f"âœ… æ ·æœ¬ç»Ÿè®¡: {stats}")
        
        # æµ‹è¯•9: ç¼“å­˜æ•ˆæœ
        logger.info("ğŸ“Š æµ‹è¯•9: ç¼“å­˜æ•ˆæœ...")
        start_time = time.time()
        
        data_loader2 = YearTimeSeriesDataLoader(
            h5_dir=h5_dir,
            positive_ratio=0.1,
            pos_neg_ratio=2.0,
            lookback_seq=365,
            forecast_hor=7,
            min_fire_threshold=1.0,
            verbose_sampling=True,
            force_resample=False,
            enable_performance_optimizations=True
        )
        
        cache_time = time.time() - start_time
        logger.info(f"â° ç¼“å­˜åŠ è½½è€—æ—¶: {cache_time:.2f}ç§’")
        logger.info(f"ğŸš€ ç¼“å­˜åŠ é€Ÿæ¯”: {init_time_optimized/cache_time:.1f}x")
        
        # æµ‹è¯•10: æ€§èƒ½ç¼“å­˜ç»Ÿè®¡
        logger.info("ğŸ“Š æµ‹è¯•10: æ€§èƒ½ç¼“å­˜ç»Ÿè®¡...")
        final_perf_stats = data_loader.dataset.get_performance_stats()
        logger.info(f"ğŸ“Š æœ€ç»ˆæ€§èƒ½ç»Ÿè®¡: {final_perf_stats}")
        
        # æ¸…ç†ç¼“å­˜æµ‹è¯•
        logger.info("ğŸ“Š æµ‹è¯•11: ç¼“å­˜æ¸…ç†...")
        data_loader.dataset.clear_performance_caches()
        cleaned_stats = data_loader.dataset.get_performance_stats()
        logger.info(f"ğŸ“Š æ¸…ç†åæ€§èƒ½ç»Ÿè®¡: {cleaned_stats}")
        
        logger.info("ğŸ‰ æ‰€æœ‰æµ‹è¯•å®Œæˆï¼")
        
    except Exception as e:
        logger.error(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        raise


def print_performance_optimization_summary():
    """æ‰“å°æ€§èƒ½ä¼˜åŒ–æ€»ç»“"""
    print("""
ğŸš€ DataLoad Year æ€§èƒ½ä¼˜åŒ–æ€»ç»“
================================

ä¸»è¦ä¼˜åŒ–å†…å®¹ï¼š
1. ğŸ“ æ–‡ä»¶å¥æŸ„ç¼“å­˜ç®¡ç† (FileHandleManager)
   - LRUç¼“å­˜æœºåˆ¶ï¼Œé¿å…é¢‘ç¹æ‰“å¼€/å…³é—­æ–‡ä»¶
   - æœ€å¤§å¥æŸ„æ•°é™åˆ¶ï¼Œé˜²æ­¢èµ„æºè€—å°½
   - çº¿ç¨‹å®‰å…¨ï¼Œæ”¯æŒå¤šè¿›ç¨‹æ•°æ®åŠ è½½

2. ğŸ’¾ æ™ºèƒ½æ•°æ®ç¼“å­˜ (DataCache)
   - å†…å­˜ç¼“å­˜çƒ­ç‚¹æ•°æ®ï¼Œå‡å°‘é‡å¤I/O
   - è‡ªåŠ¨å¤§å°ç®¡ç†ï¼Œé˜²æ­¢å†…å­˜æº¢å‡º
   - LRUæ·˜æ±°ç­–ç•¥ï¼Œä¿æŒé«˜å‘½ä¸­ç‡

3. ğŸ”„ æ‰¹é‡æ•°æ®åŠ è½½
   - æŒ‰å¹´ä»½åˆ†ç»„æ‰¹é‡å¤„ç†ï¼Œå‡å°‘æ–‡ä»¶æ“ä½œ
   - å‘é‡åŒ–æ•°æ®æå–ï¼Œæé«˜è®¡ç®—æ•ˆç‡
   - ä¸€æ¬¡æ€§åŠ è½½å®Œæ•´æ—¶é—´èŒƒå›´ï¼Œå‡å°‘é‡å¤è®¿é—®

4. âš¡ ä¼˜åŒ–çš„DataLoaderé…ç½®
   - è‡ªåŠ¨å¤šè¿›ç¨‹æ•°é‡è®¾ç½®
   - æŒä¹…åŒ–å·¥ä½œè¿›ç¨‹ï¼Œå‡å°‘å¯åŠ¨å¼€é”€
   - æ™ºèƒ½é¢„å–å’Œå†…å­˜å›ºå®š

é¢„æœŸæ€§èƒ½æå‡ï¼š
- æ•°æ®åŠ è½½é€Ÿåº¦æå‡ 5-10x
- ç£ç›˜I/Oæ“ä½œå‡å°‘ 80%+
- å†…å­˜ä½¿ç”¨æ›´é«˜æ•ˆ
- æ”¯æŒæ›´å¤§è§„æ¨¡å¹¶è¡Œè®­ç»ƒ

ä½¿ç”¨æ–¹æ³•ï¼š
```python
# å¯ç”¨æ€§èƒ½ä¼˜åŒ–ï¼ˆé»˜è®¤ï¼‰
data_loader = YearTimeSeriesDataLoader(
    h5_dir="your_data_dir",
    enable_performance_optimizations=True,
    max_file_handles=50,
    data_cache_size_mb=1024
)

# åˆ›å»ºä¼˜åŒ–çš„DataLoader
train_loader = data_loader.create_optimized_dataloader(
    train_indices, 
    batch_size=32, 
    num_workers=4
)

# ç›‘æ§æ€§èƒ½
stats = data_loader.dataset.get_performance_stats()
print(f"ç¼“å­˜ç»Ÿè®¡: {stats}")
```

å…¼å®¹æ€§ï¼š
- å®Œå…¨å‘åå…¼å®¹åŸæœ‰æ¥å£
- å¯é€‰æ‹©æ€§å¯ç”¨/ç¦ç”¨ä¼˜åŒ–
- ä¸å½±å“ç°æœ‰è®­ç»ƒè„šæœ¬
""")


if __name__ == "__main__":
    # æ‰“å°æ€§èƒ½ä¼˜åŒ–æ€»ç»“
    print_performance_optimization_summary()
    
    # è¿è¡Œæµ‹è¯•
    test_year_dataloader() 