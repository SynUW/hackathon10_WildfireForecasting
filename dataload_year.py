#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Áî®‰∫éÂä†ËΩΩgenerate_full_dataset.pyÁîüÊàêÁöÑÊï∞ÊçÆÈõÜ
Year-based Time Series Data Loader - Performance Optimized Version
Adapted for year data generated by generate_whole_dataset.py, fully compatible with original dataload.py interface

Data Format:
- Filename: {year}_year_dataset.h5
- Dataset name: {row}_{col} (pixel coordinates)
- Data shape: (channels, time_steps) where channels=39, time_steps=365/366
- Channel 0: FIRMS data, used for positive/negative sample determination

Features:
- Positive samples: Days with FIRMS >= min_fire_threshold
- Negative samples: Random pixel locations on global no-fire days
- Support for cross-year data loading (historical/future data)
- Intelligent sampling cache mechanism to avoid repeated calculations
- Fully compatible interface with original dataload.py

Performance Optimizations:
- File handle caching and reuse
- Data memory caching
- Batch data loading
- Vectorized operations
- Reduced disk I/O operations
"""

import os
import h5py
import torch
import numpy as np
from torch.utils.data import Dataset, DataLoader
from tqdm import tqdm
import logging
import hashlib
import pickle
import random
import time
from datetime import datetime, timedelta
from collections import defaultdict, OrderedDict
import glob
import json
import gc
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading
import weakref
from functools import lru_cache

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# =============================================================================
# Performance Optimization Components
# =============================================================================

class FileHandleManager:
    """
    File Handle Manager - Uses LRU cache to optimize file open/close operations
    """
    
    def __init__(self, max_handles=50):
        self.max_handles = max_handles
        self.handles = OrderedDict()
        self.lock = threading.Lock()
        
    def get_handle(self, file_path):
        """Get file handle, automatically manage LRU cache"""
        with self.lock:
            if file_path in self.handles:
                # Move to end (most recently used)
                handle = self.handles.pop(file_path)
                self.handles[file_path] = handle
                return handle
            
            # If cache is full, close the oldest file
            if len(self.handles) >= self.max_handles:
                oldest_path, oldest_handle = self.handles.popitem(last=False)
                try:
                    oldest_handle.close()
                except:
                    pass
            
            # Open new file
            try:
                handle = h5py.File(file_path, 'r')
                self.handles[file_path] = handle
                return handle
            except Exception as e:
                logger.error(f"Cannot open file {file_path}: {e}")
                return None
    
    def close_all(self):
        """Close all file handles"""
        with self.lock:
            for handle in self.handles.values():
                try:
                    handle.close()
                except:
                    pass
            self.handles.clear()
    
    def __del__(self):
        self.close_all()


class DataCache:
    """
    Data Cache Manager - Intelligent Memory Cache
    """
    
    def __init__(self, max_size_mb=1024):
        self.max_size_bytes = max_size_mb * 1024 * 1024
        self.cache = OrderedDict()
        self.current_size = 0
        self.lock = threading.Lock()
        
    def get(self, key):
        """Get cached data"""
        with self.lock:
            if key in self.cache:
                # Move to end (most recently used)
                data = self.cache.pop(key)
                self.cache[key] = data
                return data
            return None
    
    def put(self, key, data):
        """Store data in cache"""
        data_size = data.nbytes if hasattr(data, 'nbytes') else len(str(data))
        
        with self.lock:
            # If data is too large, don't cache
            if data_size > self.max_size_bytes * 0.5:
                return
            
            # Clean up space
            while self.current_size + data_size > self.max_size_bytes and self.cache:
                oldest_key, oldest_data = self.cache.popitem(last=False)
                old_size = oldest_data.nbytes if hasattr(oldest_data, 'nbytes') else len(str(oldest_data))
                self.current_size -= old_size
            
            # Add new data
            self.cache[key] = data
            self.current_size += data_size
    
    def clear(self):
        """Clear cache"""
        with self.lock:
            self.cache.clear()
            self.current_size = 0
    
    def get_stats(self):
        """Get cache statistics"""
        with self.lock:
            return {
                'entries': len(self.cache),
                'size_mb': self.current_size / (1024 * 1024),
                'max_size_mb': self.max_size_bytes / (1024 * 1024)
            }


# Global singleton instances
_file_handle_manager = None
_data_cache = None

def get_file_handle_manager():
    """Get global file handle manager"""
    global _file_handle_manager
    if _file_handle_manager is None:
        _file_handle_manager = FileHandleManager(max_handles=50)
    return _file_handle_manager

def get_data_cache():
    """Get global data cache"""
    global _data_cache
    if _data_cache is None:
        _data_cache = DataCache(max_size_mb=1024)
    return _data_cache

# =============================================================================
# Original code continues...
# =============================================================================

class YearTimeSeriesPixelDataset(Dataset):
    """
    Year-based Time Series Pixel Dataset
    
    Fully compatible interface with original TimeSeriesPixelDataset, but uses new year-based data source
    """
    
    def __init__(self, h5_dir, years=None, return_metadata=True, 
                 positive_ratio=1.0, pos_neg_ratio=1.0, 
                 resample_each_epoch=False, epoch_seed=None, verbose_sampling=True,
                 lookback_seq=365, forecast_hor=7, min_fire_threshold=0.001,
                 cache_dir=None, force_resample=False, 
                 enable_performance_optimizations=True, max_file_handles=50, 
                 data_cache_size_mb=1024):
        """
        Initialize year-based time series pixel dataset
        
        Args:
            h5_dir: Directory containing year data H5 files
            years: List of years to load, None means load all years
            return_metadata: Whether to return metadata (dates, coordinates, etc.)
            positive_ratio: Positive sample usage ratio (0.0-1.0)
            pos_neg_ratio: Positive-negative sample ratio, i.e., negative samples = positive samples √ó pos_neg_ratio
            resample_each_epoch: Whether to resample samples in each epoch
            epoch_seed: Random seed for current epoch
            verbose_sampling: Whether to display detailed sampling information
            lookback_seq: Historical time length (days)
            forecast_hor: Future time length (days)
            min_fire_threshold: FIRMS threshold, >= this value is considered positive sample
            cache_dir: Sampling cache directory, None means use h5_dir/cache
            force_resample: Whether to force resampling, ignore cache
            enable_performance_optimizations: Whether to enable performance optimizations
            max_file_handles: Maximum number of file handle cache
            data_cache_size_mb: Data cache size (MB)
        """
        self.h5_dir = h5_dir
        self.years = years
        self.return_metadata = return_metadata
        self.positive_ratio = positive_ratio
        self.pos_neg_ratio = pos_neg_ratio
        self.resample_each_epoch = resample_each_epoch
        self.epoch_seed = epoch_seed
        self.verbose_sampling = verbose_sampling
        self.lookback_seq = lookback_seq
        self.forecast_hor = forecast_hor
        self.min_fire_threshold = min_fire_threshold
        self.force_resample = force_resample
        self.enable_performance_optimizations = enable_performance_optimizations
        
        # Cache configuration
        self.cache_dir = cache_dir if cache_dir else os.path.join(h5_dir, 'cache')
        os.makedirs(self.cache_dir, exist_ok=True)
        
        # Performance optimization configuration
        if enable_performance_optimizations:
            # Initialize global optimization components
            global _file_handle_manager, _data_cache
            if _file_handle_manager is None:
                _file_handle_manager = FileHandleManager(max_handles=max_file_handles)
            if _data_cache is None:
                _data_cache = DataCache(max_size_mb=data_cache_size_mb)
            
            logger.info(f"üöÄ Performance optimizations enabled: File handle cache({max_file_handles}), Data cache({data_cache_size_mb}MB)")
        else:
            logger.info("‚ö†Ô∏è  Performance optimizations disabled")
        
        # Get year file list
        self.year_files = self._get_year_files()
        
        # Build or load sampling results
        self.full_sample_index = []  # Store complete index of all samples
        self.sample_index = []  # Currently used sample index
        self.dataset_info = {}  # Store dataset information
        
        self._build_or_load_samples()
        
        # Initial sample ratio filtering
        logger.info(f"üîç Sampling check: positive_ratio={positive_ratio}, pos_neg_ratio={pos_neg_ratio}")
        logger.info(f"üîç Sample index before filtering: {len(self.sample_index)} samples")
        
        # Always apply sampling to ensure consistent behavior
        # This ensures that both train/val (pos_neg_ratio=2.0) and test (pos_neg_ratio=1.0) 
        # are sampled from the complete dataset, not from each other
        if True:  # Always apply sampling
            logger.info(f"üîç Applying sample ratio filtering...")
            if resample_each_epoch:
                if self.verbose_sampling:
                    logger.info("Enabling per-epoch resampling mode")
                # Save complete index and perform initial sampling
                self.full_sample_index = self.sample_index.copy()
                self._apply_sample_ratio_filtering()
            else:
                # Traditional mode: one-time sampling
                self._apply_sample_ratio_filtering()
            logger.info(f"üîç Sample index after filtering: {len(self.sample_index)} samples")
        else:
            logger.info(f"üîç No sampling needed (positive_ratio=1.0, pos_neg_ratio=1.0)")
            logger.info(f"üîç Sample index unchanged: {len(self.sample_index)} samples")
        
        logger.info(f"Year dataset initialization completed, total {len(self.sample_index)} samples")
        logger.info(f"Positive sample usage ratio: {positive_ratio:.2f}, Positive-negative ratio: 1:{pos_neg_ratio:.2f}")
        if resample_each_epoch:
            logger.info(f"Per-epoch resampling enabled, total sample pool: {len(self.full_sample_index)} samples")
        
        # Display performance optimization status
        if enable_performance_optimizations:
            cache_stats = get_data_cache().get_stats()
            logger.info(f"üìä Performance cache status: Data cache {cache_stats['entries']} entries, {cache_stats['size_mb']:.1f}MB")
    
    def _get_year_files(self):
        """Get list of year H5 files"""
        year_files = {}
        
        # Find all year files
        for filename in os.listdir(self.h5_dir):
            if filename.endswith('_year_dataset.h5'):
                try:
                    year = int(filename.split('_')[0])
                    if self.years is None or year in self.years:
                        year_files[year] = os.path.join(self.h5_dir, filename)
                except ValueError:
                    continue
        
        if not year_files:
            raise ValueError(f"No year data files found, directory: {self.h5_dir}")
        
        logger.info(f"Found {len(year_files)} year data files: {sorted(year_files.keys())}")
        return year_files
    
    def _get_cache_filename(self):
        """Generate cache filename"""
        # Generate unique identifier based on key parameters (excluding path)
        cache_params = {
            'years': sorted(self.year_files.keys()),
            'lookback_seq': self.lookback_seq,
            'forecast_hor': self.forecast_hor,
            'min_fire_threshold': self.min_fire_threshold,
            'version': '1.0'  # Version number for cache format changes
        }
        
        # Generate parameter hash
        param_str = json.dumps(cache_params, sort_keys=True)
        param_hash = hashlib.md5(param_str.encode()).hexdigest()[:12]
        
        return os.path.join(self.cache_dir, f"samples_{param_hash}.h5")
    
    def _check_cache_validity(self, cache_file):
        """Check if cache file is valid"""
        if self.force_resample:
            return False
        
        if not os.path.exists(cache_file):
            return False
        
        try:
            with h5py.File(cache_file, 'r') as f:
                # Check cache parameters (excluding path-related parameters)
                cached_params = json.loads(f.attrs.get('cache_params', '{}'))
                current_params = {
                    'years': sorted(self.year_files.keys()),
                    'lookback_seq': self.lookback_seq,
                    'forecast_hor': self.forecast_hor,
                    'min_fire_threshold': self.min_fire_threshold,
                    'version': '1.0'
                }
                
                # Only check core parameters, ignore path-related parameters
                core_params = ['years', 'lookback_seq', 'forecast_hor', 'min_fire_threshold', 'version']
                for param in core_params:
                    if param not in cached_params or cached_params[param] != current_params[param]:
                        logger.info(f"üîç Cache parameter mismatch: {param}")
                        return False
                
                # Optionally check if source data files have changed (can be disabled for flexibility)
                # cached_file_mtimes = json.loads(f.attrs.get('source_file_mtimes', '{}'))
                # current_file_mtimes = {}
                # 
                # for year, file_path in self.year_files.items():
                #     if os.path.exists(file_path):
                #         current_file_mtimes[str(year)] = os.path.getmtime(file_path)
                # 
                # if cached_file_mtimes != current_file_mtimes:
                #     return False
                
                return True
                
        except Exception as e:
            logger.warning(f"Failed to check cache file: {e}")
            return False
    
    def _process_year_for_no_fire_days(self, year_file_info):
        """Process single year for no-fire days search"""
        year, file_path = year_file_info
        year_no_fire_days = []
        
        try:
            with h5py.File(file_path, 'r') as f:
                year_days = int(f.attrs.get('total_time_steps', 365))
                
                # Load FIRMS data for all pixels at once
                pixel_names = [name for name in f.keys() if '_' in name]
                
                # üîç Add debug information
                logger.info(f"Year {year}: Found {len(pixel_names)} pixel datasets (searching for no-fire days)")
                if len(pixel_names) == 0:
                    logger.warning(f"Year {year} file {file_path} has no pixel datasets found")
                    all_keys = list(f.keys())
                    logger.warning(f"All keys in file: {all_keys[:10]}...")  # Show first 10
                    return year_no_fire_days
                
                if not pixel_names:
                    return year_no_fire_days
                
                # Batch load FIRMS data
                firms_data_list = []
                for pixel_name in pixel_names:
                    try:
                        pixel_data = f[pixel_name]
                        firms_channel = pixel_data[0, :]  # FIRMS channel data
                        firms_data_list.append(firms_channel)
                    except Exception as e:
                        logger.warning(f"Failed to load pixel {pixel_name}: {e}")
                        continue
                
                if not firms_data_list:
                    return year_no_fire_days
                
                # Convert to numpy array for vectorized computation
                # shape: (num_pixels, time_steps)
                firms_array = np.array(firms_data_list)
                
                # Vectorized computation of daily maximum FIRMS values
                daily_max_firms = np.nanmax(firms_array, axis=0)
                
                # üîç Add debug information
                overall_max_firms = np.nanmax(daily_max_firms)
                overall_min_firms = np.nanmin(daily_max_firms)
                logger.info(f"Year {year}: Daily max FIRMS range: {overall_min_firms:.2f}-{overall_max_firms:.2f}, threshold: {self.min_fire_threshold}")
                
                # Find no-fire days (days with max FIRMS value below threshold)
                no_fire_days = np.where(daily_max_firms < self.min_fire_threshold)[0]
                
                # üîç Add debug information
                logger.info(f"Year {year}: Found {len(no_fire_days)} no-fire days (out of {len(daily_max_firms)} days)")
                
                # Build results
                start_date = datetime(year, 1, 1)
                for day_idx in no_fire_days:
                    actual_date = start_date + timedelta(days=int(day_idx))
                    year_no_fire_days.append({
                        'year': year,
                        'day_of_year': int(day_idx),
                        'date': actual_date,
                        'file_path': file_path
                    })
                    
        except Exception as e:
            logger.error(f"Error processing year {year}: {e}")
        
        # üîç Add summary information
        logger.info(f"Year {year}: Total {len(year_no_fire_days)} no-fire days found")
        
        return year_no_fire_days

    def _find_global_no_fire_days(self):
        """Find global no-fire days - optimized version"""
        logger.info("Searching for global no-fire days...")
        
        global_no_fire_days = []
        
        # Use thread pool instead of process pool to avoid pickle issues
        from concurrent.futures import ThreadPoolExecutor, as_completed
        
        # Parallel processing of all years (limit parallelism to avoid memory overload)
        max_workers = min(4, len(self.year_files), os.cpu_count() or 1)
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = [
                executor.submit(self._process_year_for_no_fire_days, (year, file_path))
                for year, file_path in self.year_files.items()
            ]
            
            for future in tqdm(as_completed(futures), total=len(futures), desc="Scanning years"):
                try:
                    year_results = future.result()
                    global_no_fire_days.extend(year_results)
                except Exception as e:
                    logger.error(f"Error processing year: {e}")
        
        logger.info(f"Found {len(global_no_fire_days)} global no-fire days")
        return global_no_fire_days
    
    def _process_year_for_positive_samples(self, year_file_info):
        """Process single year for positive sample search"""
        year, file_path = year_file_info
        year_positive_samples = []
        
        try:
            with h5py.File(file_path, 'r') as f:
                year_days = int(f.attrs.get('total_time_steps', 365))
                start_date = datetime(year, 1, 1)
                
                # Get all pixel dataset names
                pixel_names = [name for name in f.keys() if '_' in name]
                
                # üîç Add debug information
                logger.info(f"Year {year}: Found {len(pixel_names)} pixel datasets")
                if len(pixel_names) == 0:
                    logger.warning(f"Year {year} file {file_path} has no pixel datasets found")
                    all_keys = list(f.keys())
                    logger.warning(f"All keys in file: {all_keys[:10]}...")  # Show first 10
                
                for pixel_name in pixel_names:
                    try:
                        row, col = map(int, pixel_name.split('_'))
                        pixel_data = f[pixel_name]
                        firms_channel = pixel_data[0, :]  # FIRMS channel data
                        
                        # üîç Add debug information
                        max_firms = np.nanmax(firms_channel)
                        min_firms = np.nanmin(firms_channel)
                        
                        # Vectorized search for days exceeding threshold
                        fire_days = np.where(firms_channel >= self.min_fire_threshold)[0]
                        
                        # üîç If fire days found, add debug information
                        if len(fire_days) > 0:
                            logger.info(f"Pixel {pixel_name}: Found {len(fire_days)} fire days, FIRMS range: {min_firms:.2f}-{max_firms:.2f}")
                        
                        # Create positive sample for each fire day
                        for day_idx in fire_days:
                            actual_date = start_date + timedelta(days=int(day_idx))
                            year_positive_samples.append({
                                'year': year,
                                'day_of_year': int(day_idx),
                                'date': actual_date,
                                'pixel_row': row,
                                'pixel_col': col,
                                'firms_value': float(firms_channel[day_idx]),
                                'file_path': file_path
                            })
                            
                    except ValueError:
                        # Skip non-pixel datasets
                        continue
                    except Exception as e:
                        logger.warning(f"Error processing pixel {pixel_name}: {e}")
                        continue
                        
        except Exception as e:
            logger.error(f"Error processing year {year}: {e}")
        
        # üîç Add summary information
        logger.info(f"Year {year}: Total {len(year_positive_samples)} positive samples found")
        
        return year_positive_samples

    def _find_positive_samples(self):
        """Find positive samples - optimized version"""
        logger.info("Searching for positive samples...")
        
        positive_samples = []
        
        # Use thread pool instead of process pool to avoid pickle issues
        from concurrent.futures import ThreadPoolExecutor, as_completed
        
        # Parallel processing of all years (limit parallelism to avoid memory overload)
        max_workers = min(4, len(self.year_files), os.cpu_count() or 1)
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = [
                executor.submit(self._process_year_for_positive_samples, (year, file_path))
                for year, file_path in self.year_files.items()
            ]
            
            for future in tqdm(as_completed(futures), total=len(futures), desc="Scanning positive samples"):
                try:
                    year_results = future.result()
                    positive_samples.extend(year_results)
                except Exception as e:
                    logger.error(f"Error processing year: {e}")
        
        logger.info(f"Found {len(positive_samples)} positive samples")
        return positive_samples
    
    def _generate_negative_samples(self, global_no_fire_days, num_negative_samples):
        """Generate negative samples - optimized version"""
        logger.info(f"Generating {num_negative_samples} negative samples...")
        
        if not global_no_fire_days:
            logger.warning("No global no-fire days, cannot generate negative samples")
            return []
        
        negative_samples = []
        
        # Pre-cache pixel list for each year to avoid repeated reading
        year_pixels_cache = {}
        
        logger.info("Pre-caching year pixel information...")
        for year, file_path in tqdm(self.year_files.items(), desc="Caching pixel info"):
            try:
                with h5py.File(file_path, 'r') as f:
                    pixels = []
                    for dataset_name in f.keys():
                        if '_' in dataset_name:
                            try:
                                row, col = map(int, dataset_name.split('_'))
                                pixels.append((row, col))
                            except ValueError:
                                continue
                    
                    if pixels:
                        year_pixels_cache[year] = pixels
                        
            except Exception as e:
                logger.warning(f"Error caching year {year} pixel information: {e}")
                continue
        
        if not year_pixels_cache:
            logger.warning("No available pixel locations, cannot generate negative samples")
            return []
        
        # Build available no-fire days list (only years with pixel data)
        valid_no_fire_days = []
        for day_info in global_no_fire_days:
            if day_info['year'] in year_pixels_cache:
                valid_no_fire_days.append(day_info)
        
        if not valid_no_fire_days:
            logger.warning("No valid no-fire days, cannot generate negative samples")
            return []
        
        # Random sampling of negative samples
        random.seed(42)  # Ensure reproducibility
        
        logger.info(f"Random sampling from {len(valid_no_fire_days)} valid no-fire days...")
        for _ in tqdm(range(num_negative_samples), desc="Generating negative samples"):
            # Randomly select a no-fire day
            day_info = random.choice(valid_no_fire_days)
            
            # Randomly select a pixel location
            available_pixels = year_pixels_cache[day_info['year']]
            pixel_row, pixel_col = random.choice(available_pixels)
            
            negative_samples.append({
                'year': day_info['year'],
                'day_of_year': day_info['day_of_year'],
                'date': day_info['date'],
                'pixel_row': pixel_row,
                'pixel_col': pixel_col,
                'firms_value': 0.0,  # Negative sample FIRMS value is 0
                'file_path': day_info['file_path']
            })
        
        logger.info(f"Generated {len(negative_samples)} negative samples")
        return negative_samples
    
    def _sample_data_with_cache(self):
        """Sample data using cache mechanism"""
        cache_file = self._get_cache_filename()
        
        # Check if cache is valid
        if self._check_cache_validity(cache_file):
            logger.info(f"üöÄ Using cache file: {cache_file}")
            return self._load_samples_from_cache(cache_file)
        
        # Cache invalid, resample
        logger.info("üíæ Cache invalid, starting resampling...")
        
        # Record start time
        start_time = time.time()
        
        # Find global no-fire days
        logger.info("üîç Step 1: Finding global no-fire days...")
        step_start = time.time()
        global_no_fire_days = self._find_global_no_fire_days()
        step_duration = time.time() - step_start
        logger.info(f"‚úÖ Completed, time: {step_duration:.2f}s")
        
        # Find positive samples
        logger.info("üîç Step 2: Finding positive samples...")
        step_start = time.time()
        positive_samples = self._find_positive_samples()
        step_duration = time.time() - step_start
        logger.info(f"‚úÖ Completed, time: {step_duration:.2f}s")
        
        if not positive_samples:
            raise ValueError("No positive samples found")
        
        # Generate negative samples
        logger.info("üîç Step 3: Generating negative samples...")
        step_start = time.time()
        # Generate comprehensive negative samples to store complete data in cache
        # This allows flexible sampling ratios later
        num_negative_samples = len(positive_samples) * 20  # Generate comprehensive negative samples for flexibility
        logger.info(f"Generating {num_negative_samples} negative samples for cache flexibility")
        negative_samples = self._generate_negative_samples(global_no_fire_days, num_negative_samples)
        step_duration = time.time() - step_start
        logger.info(f"‚úÖ Completed, time: {step_duration:.2f}s")
        
        # Save to cache
        logger.info("üíæ Step 4: Saving to cache...")
        step_start = time.time()
        self._save_samples_to_cache(cache_file, positive_samples, negative_samples, global_no_fire_days)
        step_duration = time.time() - step_start
        logger.info(f"‚úÖ Completed, time: {step_duration:.2f}s")
        
        total_duration = time.time() - start_time
        logger.info(f"üéâ Sampling completed! Total time: {total_duration:.2f}s")
        logger.info(f"üìä Sampling results: {len(positive_samples)} positive samples, {len(negative_samples)} negative samples")
        
        return positive_samples, negative_samples
    
    def _load_samples_from_cache(self, cache_file):
        """Load samples from cache - optimized version"""
        import time
        start_time = time.time()
        
        try:
            with h5py.File(cache_file, 'r') as f:
                positive_samples = []
                negative_samples = []
                
                # Batch load positive sample data
                if 'positive_samples' in f:
                    pos_group = f['positive_samples']
                    # Read all arrays at once
                    pos_years = pos_group['year'][:]
                    pos_days = pos_group['day_of_year'][:]
                    pos_dates = pos_group['date'][:]
                    pos_rows = pos_group['pixel_row'][:]
                    pos_cols = pos_group['pixel_col'][:]
                    pos_firms = pos_group['firms_value'][:]
                    
                    # Vectorized processing - pre-decode strings
                    decoded_dates = [pos_dates[i].decode() for i in range(len(pos_dates))]
                    for i in range(len(pos_years)):
                        year = int(pos_years[i])
                        positive_samples.append({
                            'year': year,
                            'day_of_year': int(pos_days[i]),
                            'date': datetime.strptime(decoded_dates[i], '%Y-%m-%d'),
                            'pixel_row': int(pos_rows[i]),
                            'pixel_col': int(pos_cols[i]),
                            'firms_value': float(pos_firms[i]),
                            'file_path': self.year_files[year]
                        })
                
                # Batch load negative sample data
                if 'negative_samples' in f:
                    neg_group = f['negative_samples']
                    # Read all arrays at once
                    neg_years = neg_group['year'][:]
                    neg_days = neg_group['day_of_year'][:]
                    neg_dates = neg_group['date'][:]
                    neg_rows = neg_group['pixel_row'][:]
                    neg_cols = neg_group['pixel_col'][:]
                    neg_firms = neg_group['firms_value'][:]
                    
                    # Vectorized processing - pre-decode strings
                    decoded_dates = [neg_dates[i].decode() for i in range(len(neg_dates))]
                    for i in range(len(neg_years)):
                        year = int(neg_years[i])
                        negative_samples.append({
                            'year': year,
                            'day_of_year': int(neg_days[i]),
                            'date': datetime.strptime(decoded_dates[i], '%Y-%m-%d'),
                            'pixel_row': int(neg_rows[i]),
                            'pixel_col': int(neg_cols[i]),
                            'firms_value': float(neg_firms[i]),
                            'file_path': self.year_files[year]
                        })
                
                load_time = time.time() - start_time
                logger.info(f"Loaded samples from cache: {len(positive_samples)} positive samples, {len(negative_samples)} negative samples (time: {load_time:.2f}s)")
                return positive_samples, negative_samples
                
        except Exception as e:
            logger.error(f"Failed to load samples from cache: {e}")
            # Delete invalid cache
            try:
                os.remove(cache_file)
            except:
                pass
            
            # Resample
            return self._sample_data_with_cache()
    
    def _save_samples_to_cache(self, cache_file, positive_samples, negative_samples, global_no_fire_days):
        """Save samples to cache"""
        try:
            with h5py.File(cache_file, 'w') as f:
                # Save cache parameters (excluding path-related parameters)
                cache_params = {
                    'years': sorted(self.year_files.keys()),
                    'lookback_seq': self.lookback_seq,
                    'forecast_hor': self.forecast_hor,
                    'min_fire_threshold': self.min_fire_threshold,
                    'version': '1.0'
                }
                f.attrs['cache_params'] = json.dumps(cache_params)
                
                # Save source file modification times
                source_file_mtimes = {}
                for year, file_path in self.year_files.items():
                    if os.path.exists(file_path):
                        source_file_mtimes[str(year)] = os.path.getmtime(file_path)
                f.attrs['source_file_mtimes'] = json.dumps(source_file_mtimes)
                
                # Save metadata
                f.attrs['total_positive'] = len(positive_samples)
                f.attrs['total_negative'] = len(negative_samples)
                f.attrs['global_no_fire_days'] = len(global_no_fire_days)
                f.attrs['creation_time'] = datetime.now().isoformat()
                
                # Save positive samples
                if positive_samples:
                    pos_group = f.create_group('positive_samples')
                    pos_group.create_dataset('year', data=[s['year'] for s in positive_samples])
                    pos_group.create_dataset('day_of_year', data=[s['day_of_year'] for s in positive_samples])
                    pos_group.create_dataset('date', data=[s['date'].strftime('%Y-%m-%d').encode() for s in positive_samples])
                    pos_group.create_dataset('pixel_row', data=[s['pixel_row'] for s in positive_samples])
                    pos_group.create_dataset('pixel_col', data=[s['pixel_col'] for s in positive_samples])
                    pos_group.create_dataset('firms_value', data=[s['firms_value'] for s in positive_samples])
                
                # Save negative samples
                if negative_samples:
                    neg_group = f.create_group('negative_samples')
                    neg_group.create_dataset('year', data=[s['year'] for s in negative_samples])
                    neg_group.create_dataset('day_of_year', data=[s['day_of_year'] for s in negative_samples])
                    neg_group.create_dataset('date', data=[s['date'].strftime('%Y-%m-%d').encode() for s in negative_samples])
                    neg_group.create_dataset('pixel_row', data=[s['pixel_row'] for s in negative_samples])
                    neg_group.create_dataset('pixel_col', data=[s['pixel_col'] for s in negative_samples])
                    neg_group.create_dataset('firms_value', data=[s['firms_value'] for s in negative_samples])
                
            logger.info(f"Samples saved to cache: {cache_file}")
            
        except Exception as e:
            logger.error(f"Failed to save samples to cache: {e}")
            # Delete corrupted cache file
            try:
                os.remove(cache_file)
            except:
                pass
    
    def _build_or_load_samples(self):
        """Build or load sample index"""
        positive_samples, negative_samples = self._sample_data_with_cache()
        
        # Merge samples and build index
        all_samples = positive_samples + negative_samples
        
        self.sample_index = []
        self.dataset_info = {}
        
        # Batch process samples to reduce repeated calculations
        valid_samples = []
        for sample in all_samples:
            # Check if sample is valid (sufficient historical and future data)
            if self._is_sample_valid(sample):
                valid_samples.append(sample)
        
        # Batch build sample index
        for sample in valid_samples:
            pixel_row = sample['pixel_row']
            pixel_col = sample['pixel_col']
            date_obj = sample['date']
            
            sample_metadata = {
                'year': sample['year'],
                'day_of_year': sample['day_of_year'],
                'date': date_obj,
                'date_int': int(date_obj.strftime('%Y%m%d')),
                'pixel_row': pixel_row,
                'pixel_col': pixel_col,
                'firms_value': sample['firms_value'],
                'row': pixel_row,  # Compatible with original interface
                'col': pixel_col,  # Compatible with original interface
                'sample_type': 'positive' if sample['firms_value'] >= self.min_fire_threshold else 'negative'
            }
            
            self.sample_index.append((sample['file_path'], f"{pixel_row}_{pixel_col}", sample_metadata))
        
        # Build dataset information
        for year, file_path in self.year_files.items():
            try:
                with h5py.File(file_path, 'r') as f:
                    self.dataset_info[file_path] = {
                        'year': year,
                        'total_time_steps': int(f.attrs.get('total_time_steps', 365)),
                        'total_channels': int(f.attrs.get('total_channels', 39)),
                        'past_days': self.lookback_seq,
                        'future_days': self.forecast_hor
                    }
            except Exception as e:
                logger.warning(f"Failed to read dataset info: {file_path}, {e}")
    
    def _is_sample_valid(self, sample):
        """Check if sample is valid (sufficient historical and future data)"""
        target_date = sample['date']
        
        # Check historical data
        history_start = target_date - timedelta(days=self.lookback_seq - 1)
        if history_start.year < min(self.year_files.keys()):
            return False
        
        # Check future data
        future_end = target_date + timedelta(days=self.forecast_hor - 1)
        if future_end.year > max(self.year_files.keys()):
            return False
        
        return True
    
    def _load_pixel_data_for_date_range(self, pixel_row, pixel_col, start_date, end_date):
        """Load data for specified pixel in specified date range - performance optimized version"""
        # Use cache check
        cache_key = f"{pixel_row}_{pixel_col}_{start_date.strftime('%Y%m%d')}_{end_date.strftime('%Y%m%d')}"
        cached_data = get_data_cache().get(cache_key)
        if cached_data is not None:
            return cached_data
        
        # Group date ranges by year
        year_ranges = {}
        current_date = start_date
        
        while current_date <= end_date:
            year = current_date.year
            if year not in year_ranges:
                year_ranges[year] = []
            year_ranges[year].append(current_date)
            current_date += timedelta(days=1)
        
        data_segments = []
        num_channels_inferred = None
        file_handle_manager = get_file_handle_manager()
        
        # Batch load data by year
        for year in sorted(year_ranges.keys()):
            year_dates = year_ranges[year]
            
            if year not in self.year_files:
                # Fill missing years with NaN
                c = num_channels_inferred if num_channels_inferred is not None else 39
                data_segments.append(np.full((c, len(year_dates)), np.nan))
                continue
            
            try:
                # Use file handle manager to get file handle
                f = file_handle_manager.get_handle(self.year_files[year])
                if f is None:
                    c = num_channels_inferred if num_channels_inferred is not None else 39
                    data_segments.append(np.full((c, len(year_dates)), np.nan))
                    continue
                
                dataset_name = f"{pixel_row}_{pixel_col}"
                
                if dataset_name not in f:
                    # Pixel doesn't exist, fill with NaN
                    c = num_channels_inferred if num_channels_inferred is not None else int(f.attrs.get('total_channels', 39))
                    data_segments.append(np.full((c, len(year_dates)), np.nan))
                    continue
                
                # Load entire pixel's annual data at once
                pixel_data = f[dataset_name][:]  # shape: (channels, time_steps)
                if pixel_data.ndim != 2:
                    # Fallback safe guard
                    c = num_channels_inferred if num_channels_inferred is not None else int(f.attrs.get('total_channels', 39))
                    data_segments.append(np.full((c, len(year_dates)), np.nan))
                    continue
                if num_channels_inferred is None:
                    num_channels_inferred = int(pixel_data.shape[0])
                
                # Batch calculate indices for all dates
                year_start = datetime(year, 1, 1)
                day_indices = [(date - year_start).days for date in year_dates]
                
                # Vectorized data extraction
                year_data_list = []
                for day_idx in day_indices:
                    if day_idx >= pixel_data.shape[1] or day_idx < 0:
                        c = int(pixel_data.shape[0])
                        year_data_list.append(np.full((c, 1), np.nan))
                    else:
                        year_data_list.append(pixel_data[:, day_idx:day_idx+1])
                
                # Merge annual data
                if year_data_list:
                    year_data = np.concatenate(year_data_list, axis=1)
                    data_segments.append(year_data)
                else:
                    c = int(pixel_data.shape[0])
                    data_segments.append(np.full((c, len(year_dates)), np.nan))
                        
            except Exception as e:
                logger.warning(f"Failed to load pixel data: {pixel_row}_{pixel_col}, year{year}, {e}")
                data_segments.append(np.full((39, len(year_dates)), np.nan))
        
        # Merge all data segments
        if data_segments:
            # Ensure all segments have the same channel dimension
            max_c = max(seg.shape[0] for seg in data_segments if isinstance(seg, np.ndarray))
            normalized = []
            for seg in data_segments:
                if seg.shape[0] == max_c:
                    normalized.append(seg)
                else:
                    # Pad channel dimension with NaN to match
                    pad_c = max_c - seg.shape[0]
                    if pad_c > 0:
                        pad = np.full((pad_c, seg.shape[1]), np.nan)
                        normalized.append(np.concatenate([seg, pad], axis=0))
                    else:
                        normalized.append(seg[:max_c, :])
            result = np.concatenate(normalized, axis=1)
        else:
            c = num_channels_inferred if num_channels_inferred is not None else 39
            result = np.full((c, 0), np.nan)
        
        # Cache result
        get_data_cache().put(cache_key, result)
        
        return result
    
    def __len__(self):
        """Return number of samples"""
        return len(self.sample_index)
    
    def __getitem__(self, idx):
        """
        Get single sample - performance optimized version
        
        Returns:
            - past_data: (channels, past_time_steps) 
            - future_data: (channels, future_time_steps)
            - metadata (optional): [date_int, row, col]
        """
        file_path, dataset_key, metadata = self.sample_index[idx]
        
        try:
            target_date = metadata['date']
            pixel_row = metadata['pixel_row']
            pixel_col = metadata['pixel_col']
            
            # Calculate complete data range (historical + future)
            history_start = target_date - timedelta(days=self.lookback_seq - 1)
            future_end = target_date + timedelta(days=self.forecast_hor - 1)
            
            # Load complete data range at once to reduce I/O operations
            full_data = self._load_pixel_data_for_date_range(pixel_row, pixel_col, history_start, future_end)
            
            # Split historical and future data
            past_data = full_data[:, :self.lookback_seq]
            future_data = full_data[:, self.lookback_seq:]
            
            # Check data dimensions and handle mismatches
            if past_data.shape[1] != self.lookback_seq:
                # Handle historical data length mismatch (may be due to leap year/normal year)
                if past_data.shape[1] < self.lookback_seq:
                    # Insufficient data, pad at the beginning
                    padding_days = self.lookback_seq - past_data.shape[1]
                    padding = np.zeros((past_data.shape[0], padding_days))
                    past_data = np.concatenate([padding, past_data], axis=1)
                    logger.debug(f"Insufficient historical data length: expected{self.lookback_seq}, actual{past_data.shape[1]-padding_days}, padded{padding_days} days")
                else:
                    # Too much data, take the most recent part
                    past_data = past_data[:, -self.lookback_seq:]
                    logger.debug(f"Historical data too long: expected{self.lookback_seq}, actual{past_data.shape[1]}, took recent{self.lookback_seq} days")
            
            if future_data.shape[1] != self.forecast_hor:
                # Handle future data length mismatch
                if future_data.shape[1] < self.forecast_hor:
                    # Insufficient data, pad at the end
                    padding_days = self.forecast_hor - future_data.shape[1]
                    padding = np.zeros((future_data.shape[0], padding_days))
                    future_data = np.concatenate([future_data, padding], axis=1)
                    logger.debug(f"Insufficient future data length: expected{self.forecast_hor}, actual{future_data.shape[1]-padding_days}, padded{padding_days} days")
                else:
                    # Too much data, take the front part
                    future_data = future_data[:, :self.forecast_hor]
                    logger.debug(f"Future data too long: expected{self.forecast_hor}, actual{future_data.shape[1]}, took front{self.forecast_hor} days")
            
            # Handle NaN values
            past_data = np.nan_to_num(past_data, nan=0.0, posinf=0.0, neginf=0.0)
            future_data = np.nan_to_num(future_data, nan=0.0, posinf=0.0, neginf=0.0)
            
            # Convert to torch tensor
            past_data = torch.from_numpy(past_data).float()
            future_data = torch.from_numpy(future_data).float()
            
            # Final check
            if torch.isnan(past_data).any() or torch.isinf(past_data).any():
                past_data = torch.nan_to_num(past_data, nan=0.0, posinf=0.0, neginf=0.0)
            
            if torch.isnan(future_data).any() or torch.isinf(future_data).any():
                future_data = torch.nan_to_num(future_data, nan=0.0, posinf=0.0, neginf=0.0)
            
            if self.return_metadata:
                # Return simplified metadata format: [date, x_coordinate, y_coordinate]
                simplified_metadata = [metadata['date_int'], metadata['row'], metadata['col']]
                return past_data, future_data, simplified_metadata
            else:
                return past_data, future_data
                
        except Exception as e:
            logger.debug(f"Failed to get sample: {dataset_key}, {e}")
            raise e
    
    def custom_collate_fn(self, batch):
        """
        Custom collate function, fully compatible with original dataload.py
        """
        if self.return_metadata:
            past_data_list, future_data_list, metadata_list = zip(*batch)
        else:
            past_data_list, future_data_list = zip(*batch)
        
        # Check shapes of all tensors
        past_shapes = [data.shape for data in past_data_list]
        future_shapes = [data.shape for data in future_data_list]
        
        # Check past data shape consistency
        if len(set(past_shapes)) > 1:
            raise ValueError(f"Past data shapes inconsistent: {set(past_shapes)}")
        
        # Check future data shape consistency
        if len(set(future_shapes)) > 1:
            raise ValueError(f"Future data shapes inconsistent: {set(future_shapes)}")
        
        # Direct stacking, shapes must be consistent
        past_batch = torch.stack(past_data_list, dim=0)
        future_batch = torch.stack(future_data_list, dim=0)
        
        if self.return_metadata:
            return past_batch, future_batch, metadata_list
        else:
            return past_batch, future_batch
    
    def resample_for_epoch(self, epoch_seed):
        """
        Resample for new epoch
        """
        if not self.resample_each_epoch:
            return
        
        if not hasattr(self, 'full_sample_index') or not self.full_sample_index:
            logger.warning("Cannot resample: no complete sample index")
            return
        
        self.epoch_seed = epoch_seed
        
        # Use complete sample index for resampling
        self.sample_index = self.full_sample_index.copy()
        
        # Resample with new random seed
        self._apply_sample_ratio_filtering(seed=epoch_seed)
    
    def _apply_sample_ratio_filtering(self, seed=None):
        """Apply positive-negative sample ratio filtering with per-year sampling"""
        if seed is None:
            seed = 42
        
        logger.info(f"üîç _apply_sample_ratio_filtering called with seed={seed}, pos_neg_ratio={self.pos_neg_ratio}")
        
        if self.verbose_sampling:
            logger.info(f"Starting per-year positive-negative sample ratio filtering... (random seed: {seed})")
        
        # Set random seed for reproducibility
        random.seed(seed)
        np.random.seed(seed)
        
        # Group samples by year
        year_samples = {}
        for idx, (file_path, dataset_key, metadata) in enumerate(self.sample_index):
            year = metadata['year']
            if year not in year_samples:
                year_samples[year] = {'positive': [], 'negative': []}
            
            if metadata['firms_value'] >= self.min_fire_threshold:
                year_samples[year]['positive'].append(idx)
            else:
                year_samples[year]['negative'].append(idx)
        
        # Log original statistics per year
        if self.verbose_sampling:
            total_positive = sum(len(samples['positive']) for samples in year_samples.values())
            total_negative = sum(len(samples['negative']) for samples in year_samples.values())
            logger.info(f"Original sample statistics: {total_positive} positive samples, {total_negative} negative samples")
            logger.info(f"Per-year breakdown:")
            for year in sorted(year_samples.keys()):
                pos_count = len(year_samples[year]['positive'])
                neg_count = len(year_samples[year]['negative'])
                logger.info(f"  Year {year}: {pos_count} positive, {neg_count} negative")
        
        # Apply per-year sampling
        selected_indices = []
        total_selected_positive = 0
        total_selected_negative = 0
        
        for year in sorted(year_samples.keys()):
            year_positive = year_samples[year]['positive']
            year_negative = year_samples[year]['negative']
            
            # Keep all positive samples (or apply positive_ratio if needed)
            retained_positive_count = int(len(year_positive) * self.positive_ratio)
            selected_positive = random.sample(year_positive, retained_positive_count) if retained_positive_count < len(year_positive) else year_positive
            
            # Sample negative samples based on pos_neg_ratio
            retained_negative_count = int(retained_positive_count * self.pos_neg_ratio)
            retained_negative_count = min(retained_negative_count, len(year_negative))  # Don't exceed available
            selected_negative = random.sample(year_negative, retained_negative_count) if retained_negative_count < len(year_negative) else year_negative
            
            # Add selected samples for this year
            selected_indices.extend(selected_positive)
            selected_indices.extend(selected_negative)
            
            total_selected_positive += len(selected_positive)
            total_selected_negative += len(selected_negative)
            
            if self.verbose_sampling:
                logger.info(f"Year {year}: selected {len(selected_positive)} positive, {len(selected_negative)} negative")
        
        # Rebuild sample index
        new_sample_index = []
        for idx in selected_indices:
            new_sample_index.append(self.sample_index[idx])
        
        self.sample_index = new_sample_index
        
        if self.verbose_sampling:
            logger.info(f"Per-year sample filtering completed: {total_selected_positive} positive samples, {total_selected_negative} negative samples")
            logger.info(f"Overall ratio: 1:{total_selected_negative/total_selected_positive:.2f}")
    
    def get_current_sample_stats(self):
        """Get current sample statistics"""
        positive_count = 0
        negative_count = 0
        
        for _, _, metadata in self.sample_index:
            if metadata['firms_value'] >= self.min_fire_threshold:
                positive_count += 1
            else:
                negative_count += 1
        
        return {
            'total_samples': len(self.sample_index),
            'positive_samples': positive_count,
            'negative_samples': negative_count,
            'positive_ratio': positive_count / len(self.sample_index) if len(self.sample_index) > 0 else 0,
            'pos_neg_ratio': negative_count / positive_count if positive_count > 0 else 0
        }
    
    def get_statistics(self):
        """Get dataset statistics"""
        stats = {
            'total_samples': len(self.sample_index),
            'years': set(),
            'firms_values': set(),
            'sample_types': set(),
            'files': len(self.year_files)
        }
        
        for file_path, dataset_key, metadata in self.sample_index:
            stats['years'].add(metadata['year'])
            stats['firms_values'].add(metadata['firms_value'])
            stats['sample_types'].add(metadata['sample_type'])
        
        # Convert to list and sort
        stats['years'] = sorted(list(stats['years']))
        stats['firms_values'] = sorted(list(stats['firms_values']))
        stats['sample_types'] = sorted(list(stats['sample_types']))
        
        return stats
    
    def get_dataset_info(self):
        """Get dataset information"""
        return self.dataset_info
    
    def get_performance_stats(self):
        """Get performance statistics"""
        if not self.enable_performance_optimizations:
            return {"performance_optimizations": False}
        
        cache_stats = get_data_cache().get_stats()
        return {
            "performance_optimizations": True,
            "data_cache": cache_stats,
            "file_handle_manager": {
                "active_handles": len(get_file_handle_manager().handles),
                "max_handles": get_file_handle_manager().max_handles
            }
        }
    
    def clear_performance_caches(self):
        """Clear performance caches"""
        if self.enable_performance_optimizations:
            get_data_cache().clear()
            get_file_handle_manager().close_all()
            logger.info("üßπ Performance caches cleared")
        else:
            logger.info("‚ö†Ô∏è  Performance optimizations not enabled, no need to clear caches")


class YearTimeSeriesDataLoader:
    """
    Year-based Time Series Data Loader - Fully compatible with original TimeSeriesDataLoader
    """
    
    def __init__(self, h5_dir, positive_ratio=1.0, pos_neg_ratio=1.0, 
                 resample_each_epoch=False, verbose_sampling=True,
                 lookback_seq=365, forecast_hor=7, min_fire_threshold=0.001,
                 cache_dir=None, force_resample=False, 
                 enable_performance_optimizations=True, max_file_handles=50, 
                 data_cache_size_mb=1024, **kwargs):
        """
        Initialize year-based time series data loader
        
        Args:
            h5_dir: Directory containing year data H5 files
            positive_ratio: Positive sample usage ratio
            pos_neg_ratio: Positive-negative sample ratio
            resample_each_epoch: Whether to resample in each epoch
            verbose_sampling: Whether to display detailed sampling information
            lookback_seq: Historical time length
            forecast_hor: Future time length
            min_fire_threshold: FIRMS threshold
            cache_dir: Cache directory
            force_resample: Whether to force resampling
            enable_performance_optimizations: Whether to enable performance optimizations
            max_file_handles: Maximum number of file handle cache
            data_cache_size_mb: Data cache size (MB)
            **kwargs: Other parameters (for compatibility)
        """
        self.h5_dir = h5_dir
        self.positive_ratio = positive_ratio
        self.pos_neg_ratio = pos_neg_ratio
        self.resample_each_epoch = resample_each_epoch
        self.verbose_sampling = verbose_sampling
        self.lookback_seq = lookback_seq
        self.forecast_hor = forecast_hor
        self.min_fire_threshold = min_fire_threshold
        self.cache_dir = cache_dir
        self.force_resample = force_resample
        self.enable_performance_optimizations = enable_performance_optimizations
        
        # Create dataset
        self.dataset = YearTimeSeriesPixelDataset(
            h5_dir=h5_dir,
            positive_ratio=positive_ratio,
            pos_neg_ratio=pos_neg_ratio,
            resample_each_epoch=resample_each_epoch,
            verbose_sampling=verbose_sampling,
            lookback_seq=lookback_seq,
            forecast_hor=forecast_hor,
            min_fire_threshold=min_fire_threshold,
            cache_dir=cache_dir,
            force_resample=force_resample,
            enable_performance_optimizations=enable_performance_optimizations,
            max_file_handles=max_file_handles,
            data_cache_size_mb=data_cache_size_mb
        )
        
        logger.info(f"Year-based time series data loader initialization completed")
    
    def get_year_based_split(self, train_years, val_years, test_years, test_full_years=None):
        """
        Year-based data splitting - Fully compatible with original interface
        """
        train_indices = []
        val_indices = []
        test_indices = []
        
        for idx, (file_path, dataset_key, metadata) in enumerate(self.dataset.sample_index):
            year = metadata['year']
            
            if year in train_years:
                train_indices.append(idx)
            elif year in val_years:
                val_indices.append(idx)
            elif year in test_years:
                test_indices.append(idx)
        
        if self.verbose_sampling:
            logger.info(f"Year-based splitting results:")
            logger.info(f"  Training set: {len(train_indices)} samples (years: {train_years})")
            logger.info(f"  Validation set: {len(val_indices)} samples (years: {val_years})")
            logger.info(f"  Test set: {len(test_indices)} samples (years: {test_years})")
        
        # If full data test set is needed
        if test_full_years is not None:
            full_dataset = YearTimeSeriesPixelDataset(
                h5_dir=self.h5_dir,
                years=test_full_years,
                positive_ratio=1.0,
                pos_neg_ratio=999999,  # Use all negative samples
                resample_each_epoch=False,
                verbose_sampling=self.verbose_sampling,
                lookback_seq=self.lookback_seq,
                forecast_hor=self.forecast_hor,
                min_fire_threshold=self.min_fire_threshold,
                cache_dir=self.cache_dir,
                force_resample=self.force_resample
            )
            
            test_full_indices = list(range(len(full_dataset)))
            logger.info(f"Full data test set: {len(test_full_indices)} samples (years: {test_full_years})")
            
            return train_indices, val_indices, test_indices, test_full_indices, full_dataset
        
        return train_indices, val_indices, test_indices
    
    def create_optimized_dataloader(self, indices, batch_size=32, shuffle=True, 
                                   num_workers=None, pin_memory=True, 
                                   persistent_workers=True, prefetch_factor=2):
        """
        Create optimized DataLoader
        
        Args:
            indices: List of sample indices
            batch_size: Batch size
            shuffle: Whether to shuffle
            num_workers: Number of worker processes, None means auto-set
            pin_memory: Whether to use pin_memory
            persistent_workers: Whether to use persistent workers
            prefetch_factor: Prefetch factor
        """
        from torch.utils.data import Subset
        
        # Auto-set number of workers
        if num_workers is None:
            if self.enable_performance_optimizations:
                num_workers = min(4, os.cpu_count() or 1)
            else:
                num_workers = 0
        
        # Create subset
        subset = Subset(self.dataset, indices)
        
        # Optimized DataLoader configuration
        dataloader_kwargs = {
            'batch_size': batch_size,
            'shuffle': shuffle,
            'num_workers': num_workers,
            'collate_fn': self.dataset.custom_collate_fn,
            'pin_memory': pin_memory and torch.cuda.is_available(),
            'drop_last': True if shuffle else False,  # Drop last incomplete batch during training
        }
        
        # Multi-process optimization
        if num_workers > 0:
            dataloader_kwargs.update({
                'persistent_workers': persistent_workers,
                'prefetch_factor': prefetch_factor,
            })
        
        dataloader = DataLoader(subset, **dataloader_kwargs)
        
        if self.dataset.verbose_sampling:
            logger.info(f"üöÄ Created optimized DataLoader: batch_size={batch_size}, workers={num_workers}, "
                       f"samples={len(indices)}, performance_optimizations={'enabled' if self.enable_performance_optimizations else 'disabled'}")
        
        return dataloader


class YearFullDatasetLoader:
    """
    Year full dataset loader - Fully compatible with original FullDatasetLoader
    """
    
    def __init__(self, h5_dir, years=None, return_metadata=True, 
                 lookback_seq=365, forecast_hor=7, min_fire_threshold=0.001,
                 cache_dir=None):
        """
        Initialize year full dataset loader
        """
        self.h5_dir = h5_dir
        self.years = years
        self.return_metadata = return_metadata
        self.lookback_seq = lookback_seq
        self.forecast_hor = forecast_hor
        self.min_fire_threshold = min_fire_threshold
        self.cache_dir = cache_dir
        
        # Create base dataset, no sampling
        self.dataset = YearTimeSeriesPixelDataset(
            h5_dir=h5_dir,
            years=years,
            return_metadata=return_metadata,
            positive_ratio=1.0,
            pos_neg_ratio=999999,  # Use all negative samples
            resample_each_epoch=False,
            verbose_sampling=True,
            lookback_seq=lookback_seq,
            forecast_hor=forecast_hor,
            min_fire_threshold=min_fire_threshold,
            cache_dir=cache_dir,
            force_resample=False
        )
        
        logger.info(f"Year full dataset loader initialized, total {len(self.dataset)} samples")
        
        # Get statistics
        stats = self.dataset.get_statistics()
        logger.info(f"Dataset statistics: {stats}")
    
    def create_dataloader(self, batch_size=32, shuffle=False, num_workers=4, worker_init_fn=None, **dataloader_kwargs):
        """
        Create PyTorch DataLoader
        """
        return DataLoader(
            self.dataset,
            batch_size=batch_size,
            shuffle=shuffle,
            num_workers=num_workers,
            collate_fn=self.dataset.custom_collate_fn,
            worker_init_fn=worker_init_fn,
            **dataloader_kwargs
        )


# For full compatibility with the original interface, create aliases
TimeSeriesDataLoader = YearTimeSeriesDataLoader
TimeSeriesPixelDataset = YearTimeSeriesPixelDataset
FullDatasetLoader = YearFullDatasetLoader


# Test function
def test_year_dataloader():
    """Test year data loader - performance optimized version"""
    
    # Test parameters
    h5_dir = "/mnt/raid/zhengsen/wildfire_dataset/self_built_materials/year_datasets_h5"
    
    try:
        logger.info("üöÄ Start testing year data loader (performance optimized version)...")
        
        # Test 1: Performance optimization enabled
        logger.info("üìä Test 1: Performance optimization enabled...")
        start_time = time.time()
        
        # Create optimized data loader
        data_loader = YearTimeSeriesDataLoader(
            h5_dir=h5_dir,
            positive_ratio=0.1,
            pos_neg_ratio=2.0,
            lookback_seq=365,
            forecast_hor=7,
            min_fire_threshold=1.0,
            verbose_sampling=True,
            force_resample=False,  # Use cache
            enable_performance_optimizations=True,
            max_file_handles=50,
            data_cache_size_mb=1024
        )
        
        init_time_optimized = time.time() - start_time
        logger.info(f"‚è∞ Optimized data loader initialization time: {init_time_optimized:.2f}s")
        
        # Show performance statistics
        perf_stats = data_loader.dataset.get_performance_stats()
        logger.info(f"üìä Performance statistics: {perf_stats}")
        
        # Test 2: Performance optimization disabled comparison
        logger.info("üìä Test 2: Performance optimization disabled comparison...")
        start_time = time.time()
        
        data_loader_no_opt = YearTimeSeriesDataLoader(
            h5_dir=h5_dir,
            positive_ratio=0.1,
            pos_neg_ratio=2.0,
            lookback_seq=365,
            forecast_hor=7,
            min_fire_threshold=1.0,
            verbose_sampling=True,
            force_resample=False,
            enable_performance_optimizations=False
        )
        
        init_time_no_opt = time.time() - start_time
        logger.info(f"‚è∞ Non-optimized data loader initialization time: {init_time_no_opt:.2f}s")
        logger.info(f"üöÄ Initialization speedup: {init_time_no_opt/init_time_optimized:.2f}x")
        
        # Test 3: Year-based split
        logger.info("üìä Test 3: Year-based split...")
        train_indices, val_indices, test_indices = data_loader.get_year_based_split(
            train_years=[2021, 2022],
            val_years=[2023],
            test_years=[2024]
        )
        
        logger.info(f"‚úÖ Training set: {len(train_indices)} samples")
        logger.info(f"‚úÖ Validation set: {len(val_indices)} samples")
        logger.info(f"‚úÖ Test set: {len(test_indices)} samples")
        
        # Test 4: Data loading performance comparison
        logger.info("üìä Test 4: Data loading performance comparison...")
        if train_indices:
            sample_idx = train_indices[0]
            
            # Test optimized version single sample loading time
            start_time = time.time()
            past_data, future_data, metadata = data_loader.dataset[sample_idx]
            sample_time_optimized = time.time() - start_time
            
            # Test non-optimized version single sample loading time
            start_time = time.time()
            past_data_no_opt, future_data_no_opt, metadata_no_opt = data_loader_no_opt.dataset[sample_idx]
            sample_time_no_opt = time.time() - start_time
            
            logger.info(f"‚è∞ Optimized single sample loading time: {sample_time_optimized:.4f}s")
            logger.info(f"‚è∞ Non-optimized single sample loading time: {sample_time_no_opt:.4f}s")
            logger.info(f"üöÄ Single sample loading speedup: {sample_time_no_opt/sample_time_optimized:.2f}x")
            
            logger.info(f"‚úÖ Sample shape: past={past_data.shape}, future={future_data.shape}")
            logger.info(f"‚úÖ Metadata: {metadata}")
            
            # Check data quality
            logger.info("üìä Test 5: Data quality check...")
            logger.info(f"‚úÖ Past data range: [{past_data.min():.4f}, {past_data.max():.4f}]")
            logger.info(f"‚úÖ Future data range: [{future_data.min():.4f}, {future_data.max():.4f}]")
            logger.info(f"‚úÖ NaN check: Past={torch.isnan(past_data).sum()}, Future={torch.isnan(future_data).sum()}")
            
            # Validate data consistency
            logger.info("üìä Test 6: Data consistency validation...")
            data_equal = torch.allclose(past_data, past_data_no_opt, rtol=1e-5, atol=1e-8)
            logger.info(f"‚úÖ Data consistency before and after optimization: {'Passed' if data_equal else 'Failed'}")
        
        # Test 7: Optimized DataLoader batch performance
        logger.info("üìä Test 7: Optimized DataLoader batch performance...")
        test_sample_size = min(100, len(train_indices))
        
        # Test optimized DataLoader
        train_loader_optimized = data_loader.create_optimized_dataloader(
            train_indices[:test_sample_size],
            batch_size=8,
            shuffle=True,
            num_workers=2
        )
        
        start_time = time.time()
        for i, batch in enumerate(train_loader_optimized):
            past_batch, future_batch, metadata_batch = batch
            if i == 0:
                logger.info(f"‚úÖ Batch shape: past={past_batch.shape}, future={future_batch.shape}, metadata={len(metadata_batch)}")
            if i >= 4:  # Only test first 5 batches
                break
        
        batch_time_optimized = time.time() - start_time
        logger.info(f"‚è∞ Optimized batch processing time: {batch_time_optimized:.2f}s (5 batches)")
        
        # Test non-optimized DataLoader
        from torch.utils.data import Subset
        train_dataset_no_opt = Subset(data_loader_no_opt.dataset, train_indices[:test_sample_size])
        train_loader_no_opt = DataLoader(
            train_dataset_no_opt,
            batch_size=8,
            shuffle=True,
            collate_fn=data_loader_no_opt.dataset.custom_collate_fn,
            num_workers=0  # Disable multiprocessing to avoid interference
        )
        
        start_time = time.time()
        for i, batch in enumerate(train_loader_no_opt):
            if i >= 4:  # Only test first 5 batches
                break
        
        batch_time_no_opt = time.time() - start_time
        logger.info(f"‚è∞ Non-optimized batch processing time: {batch_time_no_opt:.2f}s (5 batches)")
        logger.info(f"üöÄ Batch processing speedup: {batch_time_no_opt/batch_time_optimized:.2f}x")
        
        # Test 8: Sample statistics
        logger.info("üìä Test 8: Sample statistics...")
        stats = data_loader.dataset.get_current_sample_stats()
        logger.info(f"‚úÖ Sample statistics: {stats}")
        
        # Test 9: Cache effect
        logger.info("üìä Test 9: Cache effect...")
        start_time = time.time()
        
        data_loader2 = YearTimeSeriesDataLoader(
            h5_dir=h5_dir,
            positive_ratio=0.1,
            pos_neg_ratio=2.0,
            lookback_seq=365,
            forecast_hor=7,
            min_fire_threshold=1.0,
            verbose_sampling=True,
            force_resample=False,
            enable_performance_optimizations=True
        )
        
        cache_time = time.time() - start_time
        logger.info(f"‚è∞ Cache loading time: {cache_time:.2f}s")
        logger.info(f"üöÄ Cache speedup: {init_time_optimized/cache_time:.1f}x")
        
        # Test 10: Performance cache statistics
        logger.info("üìä Test 10: Performance cache statistics...")
        final_perf_stats = data_loader.dataset.get_performance_stats()
        logger.info(f"üìä Final performance statistics: {final_perf_stats}")
        
        # Test cache clearing
        logger.info("üìä Test 11: Cache clearing...")
        data_loader.dataset.clear_performance_caches()
        cleaned_stats = data_loader.dataset.get_performance_stats()
        logger.info(f"üìä Performance statistics after clearing: {cleaned_stats}")
        
        logger.info("üéâ All tests completed!")
        
    except Exception as e:
        logger.error(f"‚ùå Test failed: {e}")
        import traceback
        traceback.print_exc()
        raise
    