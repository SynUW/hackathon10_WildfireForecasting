import torch
import torch.nn as nn
import numpy as np
from datetime import datetime, timedelta
from typing import List, Tuple, Union

class UnifiedModelAdapter:
    """
    ç»Ÿä¸€æ¨¡å‹é€‚é…å™¨ï¼šæ”¯æŒæ ‡å‡†æ¨¡å‹å’Œ10xæ¨¡å‹çš„åŠ¨æ€é…ç½®
    
    æ”¯æŒçš„æ¨¡å‹ç±»å‹ï¼š
    - æ ‡å‡†Transformerç±»æ¨¡å‹ï¼ˆéœ€è¦x_enc, x_mark_enc, x_dec, x_mark_decï¼‰
    - æ”¯æŒé€šè¿‡model_typeå‚æ•°åŠ¨æ€åˆ‡æ¢æ ‡å‡†/10xå‚æ•°é…ç½®
    """
    
    def __init__(self, config_or_seq_len=15, pred_len=7, d_model=39, label_len=0, model_type='standard', **kwargs):
        """
        åˆå§‹åŒ–ç»Ÿä¸€é€‚é…å™¨
        
        Args:
            config_or_seq_len: Configå¯¹è±¡æˆ–è¾“å…¥åºåˆ—é•¿åº¦
            pred_len: é¢„æµ‹åºåˆ—é•¿åº¦  
            d_model: ç‰¹å¾ç»´åº¦
            label_len: æ ‡ç­¾é•¿åº¦ï¼ˆç”¨äºæŸäº›æ¨¡å‹å¦‚Autoformerï¼‰
            model_type: æ¨¡å‹ç±»å‹ ('standard' æˆ– '10x')
        """
        # å¦‚æœä¼ å…¥çš„æ˜¯Configå¯¹è±¡ï¼Œä»ä¸­æå–å‚æ•°
        if hasattr(config_or_seq_len, 'seq_len'):
            config = config_or_seq_len
            self.seq_len = int(config.seq_len) if hasattr(config, 'seq_len') else 15
            self.pred_len = int(config.pred_len) if hasattr(config, 'pred_len') else 7
            self.d_model = int(config.d_model) if hasattr(config, 'd_model') else 39
            self.label_len = int(config.label_len) if hasattr(config, 'label_len') else 0
            self.model_type = getattr(config, 'model_type', 'standard')
            # å°è¯•è·å–æ¨¡å‹åç§°ç”¨äºæ—¶é—´ç‰¹å¾åˆ¤æ–­
            self.model_name = getattr(config, 'model_name', None)
        else:
            # ä¼ ç»Ÿæ–¹å¼ï¼Œç›´æ¥ä½¿ç”¨å‚æ•°
            self.seq_len = int(kwargs.get('seq_len', config_or_seq_len))
            self.pred_len = int(pred_len)
            self.d_model = int(d_model)
            self.label_len = int(label_len)
            self.model_type = model_type
            self.model_name = kwargs.get('model_name', None)
        
    def create_time_marks(self, date_strings: List[str], label_len: int = 0) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        åˆ›å»ºæ—¶é—´æ ‡è®°ç‰¹å¾
        
        Args:
            date_strings: æ—¥æœŸå­—ç¬¦ä¸²åˆ—è¡¨ï¼Œæ ¼å¼ä¸ºYYYYMMDD
            label_len: æ ‡ç­¾é•¿åº¦ï¼Œç”¨äºæŸäº›æ¨¡å‹ï¼ˆå¦‚Autoformerï¼‰
            
        Returns:
            x_mark_enc: ç¼–ç å™¨æ—¶é—´æ ‡è®° (B, seq_len, time_features)
            x_mark_dec: è§£ç å™¨æ—¶é—´æ ‡è®° (B, dec_time_len, time_features)
        """
        batch_size = len(date_strings)
        # æ ¹æ®æ¨¡å‹éœ€æ±‚å†³å®šæ—¶é—´ç‰¹å¾æ•°é‡
        # åŸºç¡€4ä¸ªç‰¹å¾ï¼šyear, month, day(æœˆä¸­å¤©æ•°), weekday
        # ç®€åŒ–ç‰ˆ3ä¸ªç‰¹å¾ï¼šmonth, day(æœˆä¸­å¤©æ•°), weekdayï¼ˆå»æ‰yearä»¥å…¼å®¹æ›´å¤šæ¨¡å‹ï¼‰
        if hasattr(self, 'model_name') and self.model_name in ['TimeMixer', 'Pyraformer']:
            time_features = 4  # ä½¿ç”¨å®Œæ•´çš„4ä¸ªç‰¹å¾ï¼šyear, month, day, weekday
        else:
            time_features = 3  # ä½¿ç”¨ç®€åŒ–çš„3ä¸ªç‰¹å¾ï¼šmonth, day, weekday
        
        # è§£ç å™¨æ—¶é—´æ ‡è®°çš„é•¿åº¦ï¼šæ ‡å‡†çš„label_len + pred_len
        dec_time_len = label_len + self.pred_len
        
        # è§£æåŸºå‡†æ—¥æœŸï¼ˆè®¾å®šå›ºå®šå°æ—¶ä¸º12ç‚¹ä¸­åˆï¼‰
        base_dates = []
        for date_str in date_strings:
            try:
                date_str = str(date_str)
                if len(date_str) == 8:
                    year = int(date_str[:4])
                    month = int(date_str[4:6])
                    day = int(date_str[6:8])
                    base_date = datetime(year, month, day, 12, 0, 0)  # å›ºå®šè®¾å®šä¸º12ç‚¹ä¸­åˆ
                else:
                    base_date = datetime(2010, 5, 6, 12, 0, 0)  # é»˜è®¤æ—¥æœŸï¼Œ12ç‚¹ä¸­åˆ
            except:
                base_date = datetime(2010, 5, 6, 12, 0, 0)  # 12ç‚¹ä¸­åˆ
            base_dates.append(base_date)
        
        # åˆ›å»ºç¼–ç å™¨æ—¶é—´æ ‡è®°ï¼ˆè¿‡å»seq_lenå¤©ï¼‰
        x_mark_enc = torch.zeros(batch_size, self.seq_len, time_features)
        for b in range(batch_size):
            base_date = base_dates[b]
            for t in range(self.seq_len):
                # è®¡ç®—è¿‡å»ç¬¬tå¤©çš„æ—¥æœŸï¼ˆä»-seq_len+1åˆ°0ï¼‰
                days_offset = t - self.seq_len + 1
                current_date = base_date + timedelta(days=days_offset)
                
                # æå–æ—¶é—´ç‰¹å¾ - æ ¹æ®æ—¶é—´ç‰¹å¾æ•°é‡å†³å®šå†…å®¹
                if time_features == 3:
                    # 3ä¸ªç‰¹å¾ï¼šyear, month, day(æœˆä¸­å¤©æ•°)
                    year = current_date.year - 2000   # ç›¸å¯¹å¹´ä»½ï¼ˆä»¥2000å¹´ä¸ºåŸºå‡†ï¼‰
                    month = current_date.month - 1    # 0-11
                    day = current_date.day - 1        # 0-30 (æœˆä¸­å¤©æ•°)
                    
                    x_mark_enc[b, t, :] = torch.tensor([
                        year, month, day
                    ], dtype=torch.long)
                    
                elif time_features == 4:
                    # 4ä¸ªç‰¹å¾ï¼šyear, month, day(æœˆä¸­å¤©æ•°), weekday
                    year = current_date.year - 2000   # ç›¸å¯¹å¹´ä»½ï¼ˆä»¥2000å¹´ä¸ºåŸºå‡†ï¼‰
                    month = current_date.month - 1    # 0-11
                    day = current_date.day - 1        # 0-30 (æœˆä¸­å¤©æ•°)
                    weekday = current_date.weekday()  # 0-6
                    
                    x_mark_enc[b, t, :] = torch.tensor([
                        year, month, day, weekday
                    ], dtype=torch.long)
                    
                else:
                    # ä¿ç•™åŸæœ‰çš„å¤æ‚ç‰¹å¾é€»è¾‘ï¼ˆå¦‚æœæœ‰å…¶ä»–æ¨¡å‹éœ€è¦ï¼‰
                    month = current_date.month - 1
                    weekday = current_date.weekday()
                    day_of_year = current_date.timetuple().tm_yday - 1
                    
                    x_mark_enc[b, t, :3] = torch.tensor([
                        month, weekday, day_of_year
                    ], dtype=torch.long)
        
        # åˆ›å»ºè§£ç å™¨æ—¶é—´æ ‡è®°ï¼ˆlabel_len + pred_lenå¤©ï¼‰
        x_mark_dec = torch.zeros(batch_size, dec_time_len, time_features)
        for b in range(batch_size):
            base_date = base_dates[b]
            for t in range(dec_time_len):
                # å¯¹äºAutoformerï¼šå‰label_lenæ˜¯å†å²çš„ï¼Œåpred_lenæ˜¯æœªæ¥çš„
                # è®¡ç®—æ—¥æœŸåç§»ï¼šä»(-label_len+1)åˆ°pred_len
                days_offset = t - label_len + 1
                current_date = base_date + timedelta(days=days_offset)
                
                # æå–æ—¶é—´ç‰¹å¾ - æ ¹æ®æ—¶é—´ç‰¹å¾æ•°é‡å†³å®šå†…å®¹
                if time_features == 3:
                    # 3ä¸ªç‰¹å¾ï¼šyear, month, day(æœˆä¸­å¤©æ•°)
                    year = current_date.year - 2000   # ç›¸å¯¹å¹´ä»½ï¼ˆä»¥2000å¹´ä¸ºåŸºå‡†ï¼‰
                    month = current_date.month - 1    # 0-11
                    day = current_date.day - 1        # 0-30 (æœˆä¸­å¤©æ•°)
                    
                    x_mark_dec[b, t, :] = torch.tensor([
                        year, month, day
                    ], dtype=torch.long)
                    
                elif time_features == 4:
                    # 4ä¸ªç‰¹å¾ï¼šyear, month, day(æœˆä¸­å¤©æ•°), weekday
                    year = current_date.year - 2000   # ç›¸å¯¹å¹´ä»½ï¼ˆä»¥2000å¹´ä¸ºåŸºå‡†ï¼‰
                    month = current_date.month - 1    # 0-11
                    day = current_date.day - 1        # 0-30 (æœˆä¸­å¤©æ•°)
                    weekday = current_date.weekday()  # 0-6
                    
                    x_mark_dec[b, t, :] = torch.tensor([
                        year, month, day, weekday
                    ], dtype=torch.long)
                    
                else:
                    # ä¿ç•™åŸæœ‰çš„å¤æ‚ç‰¹å¾é€»è¾‘ï¼ˆå¦‚æœæœ‰å…¶ä»–æ¨¡å‹éœ€è¦ï¼‰
                    month = current_date.month - 1
                    weekday = current_date.weekday()
                    day_of_year = current_date.timetuple().tm_yday - 1
                    
                    x_mark_dec[b, t, :3] = torch.tensor([
                        month, weekday, day_of_year
                    ], dtype=torch.long)
        return x_mark_enc, x_mark_dec
    
    def prepare_standard_inputs(self, past_data: torch.Tensor, future_data: torch.Tensor, 
                              date_strings: List[str]) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        ä¸ºæ ‡å‡†Transformerç±»æ¨¡å‹å‡†å¤‡è¾“å…¥
        
        Args:
            past_data: (B, C, T_past) è¿‡å»æ•°æ®
            future_data: (B, C, T_future) æœªæ¥æ•°æ®
            date_strings: æ—¥æœŸå­—ç¬¦ä¸²åˆ—è¡¨
            
        Returns:
            x_enc: ç¼–ç å™¨è¾“å…¥ (B, seq_len, C)
            x_mark_enc: ç¼–ç å™¨æ—¶é—´æ ‡è®° (B, seq_len, time_features)
            x_dec: è§£ç å™¨è¾“å…¥ (B, pred_len, C) 
            x_mark_dec: è§£ç å™¨æ—¶é—´æ ‡è®° (B, pred_len, time_features)
        """
        batch_size = past_data.shape[0]
        
        # 1. å‡†å¤‡ç¼–ç å™¨è¾“å…¥
        past_truncated = past_data[:, :, -self.seq_len:]  # å–æœ€åseq_lenå¤©
        x_enc = past_truncated.transpose(1, 2)  # (B, seq_len, C)
        
        # 2. å‡†å¤‡è§£ç å™¨è¾“å…¥
        future_truncated = future_data[:, :, :self.pred_len]  # å–å‰pred_lenå¤©
        
        # å¯¹äºåƒAutoformerè¿™æ ·çš„æ¨¡å‹ï¼Œè§£ç å™¨è¾“å…¥åº”è¯¥ä¸å®é™…çš„seasonal_initå’Œtrend_initå¯¹åº”
        # ä½†Autoformerä¼šåœ¨å†…éƒ¨å¤„ç†è¿™äº›ï¼Œæ‰€ä»¥æˆ‘ä»¬åªéœ€è¦æä¾›ä¸€ä¸ªå ä½ç¬¦
        # x_decå®é™…ä¸Šä¸ä¼šè¢«ç›´æ¥ä½¿ç”¨ï¼Œå› ä¸ºAutoformerå†…éƒ¨ä¼šåˆ›å»ºseasonal_initå’Œtrend_init
        x_dec = torch.zeros(batch_size, self.pred_len, past_data.shape[1])
        
        # 3. åˆ›å»ºæ—¶é—´æ ‡è®° - è€ƒè™‘label_len
        x_mark_enc, x_mark_dec = self.create_time_marks(date_strings, label_len=self.label_len)
        return x_enc, x_mark_enc, x_dec, x_mark_dec
    
    def adapt_inputs(self, past_data: torch.Tensor, future_data: torch.Tensor, 
                    date_strings: List[str]) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        ä¸ºæ ‡å‡†Transformerç±»æ¨¡å‹é€‚é…è¾“å…¥
        
        Args:
            past_data: (B, C, T_past) è¿‡å»æ•°æ®
            future_data: (B, C, T_future) æœªæ¥æ•°æ®
            date_strings: æ—¥æœŸå­—ç¬¦ä¸²åˆ—è¡¨
            
        Returns:
            x_enc: ç¼–ç å™¨è¾“å…¥ (B, seq_len, C)
            x_mark_enc: ç¼–ç å™¨æ—¶é—´æ ‡è®° (B, seq_len, time_features)
            x_dec: è§£ç å™¨è¾“å…¥ (B, pred_len, C) 
            x_mark_dec: è§£ç å™¨æ—¶é—´æ ‡è®° (B, label_len+pred_len, time_features)
        """
        return self.prepare_standard_inputs(past_data, future_data, date_strings)

def get_unified_model_configs(model_name=None, model_type='standard'):
    """
    è·å–ç»Ÿä¸€çš„æ¨¡å‹é…ç½®ï¼Œæ”¯æŒæ ‡å‡†å’Œ10xæ¨¡å‹
    
    Args:
        model_name: æ¨¡å‹åç§°
        model_type: æ¨¡å‹ç±»å‹ ('standard' æˆ– '10x')
    
    Returns:
        dict: æ¨¡å‹é…ç½®å­—å…¸
    """
    
    # æ ¹æ®æ¨¡å‹ç±»å‹é€‰æ‹©åŸºç¡€é…ç½®
    if model_type == '10x':
        base_config = {
            'seq_len': 15,
            'pred_len': 7,
            'label_len': 0,  # é»˜è®¤ä¸º0ï¼Œç‰¹å®šæ¨¡å‹ä¼šè¦†ç›–
            'd_model': 2048,     # 10x: ä½¿ç”¨åˆç†çš„å‚æ•°è®¾ç½®
            'n_heads': 32,       # 10x: ä½¿ç”¨åˆç†çš„å‚æ•°è®¾ç½®
            'd_ff': 2048,        # 10x: ä½¿ç”¨åˆç†çš„å‚æ•°è®¾ç½®
            'e_layers': 4,       # 10x: ä½¿ç”¨åˆç†çš„å‚æ•°è®¾ç½®
            'd_layers': 4,       # 10x: ä½¿ç”¨åˆç†çš„å‚æ•°è®¾ç½®
            'dropout': 0.1,
            'activation': 'gelu',
            'output_attention': False,
            'enc_in': 39,  # è¾“å…¥ç‰¹å¾ç»´åº¦ï¼Œæ”¹ä¸º39ä¿æŒä¸€è‡´
            'dec_in': 39,  # è§£ç å™¨è¾“å…¥ç‰¹å¾ç»´åº¦ï¼Œæ”¹ä¸º39ä¿æŒä¸€è‡´  
            'c_out': 39,   # è¾“å‡ºç‰¹å¾ç»´åº¦
            'embed': 'timeF',
            'freq': 'd',   # æ—¥é¢‘ç‡
            'factor': 1,
            'moving_avg': 25,  # ç”¨äºAutoformer
            'channel_independence': False,
            'use_norm': True,
            'd_state': 32,       # 10x: 16 -> 32 (ç”¨äºMambaç›¸å…³æ¨¡å‹ï¼Œåˆç†è®¾ç½®)
            'd_conv': 4,         # ç”¨äºMambaç›¸å…³æ¨¡å‹
            'expand': 2,         # ç”¨äºMambaç›¸å…³æ¨¡å‹
            'distil': True,      # ç”¨äºInformer
        }
    else:  # standard
        base_config = {
            'seq_len': 15,
            'pred_len': 7,
            'label_len': 0,  # é»˜è®¤ä¸º0ï¼Œç‰¹å®šæ¨¡å‹ä¼šè¦†ç›–
            'd_model': 512,  # 256
            'n_heads': 8,  # 8
            'd_ff': 2048,  # 512
            'e_layers': 2,
            'd_layers': 2,  # 1
            'dropout': 0.1, 
            'activation': 'gelu',
            'output_attention': False,
            'enc_in': 39,  # è¾“å…¥ç‰¹å¾ç»´åº¦ï¼Œæ”¹ä¸º39ä¿æŒä¸€è‡´
            'dec_in': 39,  # è§£ç å™¨è¾“å…¥ç‰¹å¾ç»´åº¦ï¼Œæ”¹ä¸º39ä¿æŒä¸€è‡´  
            'c_out': 39,   # è¾“å‡ºç‰¹å¾ç»´åº¦
            'embed': 'timeF',
            'freq': 'd',   # æ—¥é¢‘ç‡
            'factor': 1,
            'moving_avg': 25,  # ç”¨äºAutoformer
            'channel_independence': False,
            'use_norm': True,
            'd_state': 16,       # å¤šäºå¤šå˜é‡æ˜¯16ï¼Œå°‘å˜é‡æ˜¯2
            'd_conv': 4,         # é»˜è®¤æ˜¯2
            'expand': 2,         # é»˜è®¤æ˜¯1
            'distil': True,      # ç”¨äºInformer
        }
    
    # ç‰¹å®šæ¨¡å‹çš„é…ç½®
    model_specific_configs = {
        'Autoformer': {'label_len': 3},
        'Autoformer_M': {'label_len': 3},
        'iTransformer': {'class_strategy': 'projection'},
        'iInformer': {'class_strategy': 'projection'},
        'iReformer': {'class_strategy': 'projection'},
        'iFlowformer': {'class_strategy': 'projection'},
        'iFlashformer': {'class_strategy': 'projection'},
        # s_mambaçš„ç‰¹æ®Šé…ç½®
        's_mamba': {
            'd_model': 2048 if model_type == '10x' else 1024,  # ä¿®æ­£ï¼š10xä½¿ç”¨æ›´å¤§å€¼
            'd_ff': 2048 if model_type == '10x' else 1024,     # å‰é¦ˆç½‘ç»œç»´åº¦
            'e_layers': 4 if model_type == '10x' else 2,       # ç¼–ç å™¨å±‚æ•°
            'activation': 'gelu',
            'use_norm': True,
            'embed': 'timeF',
            'freq': 'd'
        },
        # æ·»åŠ ç¼ºå¤±é…ç½®çš„æ¨¡å‹
        'Nonstationary_Transformer': {
            'p_hidden_dims': [128, 128],
            'p_hidden_layers': 2,
            'label_len': 3  # è®¾ç½®åˆé€‚çš„label_len
        },
        'FEDformer': {
            'label_len': 3,  # è®¾ç½®åˆé€‚çš„label_len
            'moving_avg': 25,  # åˆ†è§£çª—å£å¤§å°
            'version': 'fourier',  # é»˜è®¤ä½¿ç”¨fourierç‰ˆæœ¬
            'mode_select': 'random',  # æ¨¡å¼é€‰æ‹©æ–¹æ³•
            'modes': 32  # é€‰æ‹©çš„æ¨¡å¼æ•°
        },
        'TemporalFusionTransformer': {
            'data': 'custom',  # æ·»åŠ æ•°æ®é…ç½®
            'hidden_size': 128,
            'lstm_layers': 1,
            'dropout': 0.1,
            'attn_heads': 4,
            'quantiles': [0.1, 0.5, 0.9]
        },
        'TimeMixer': {
            'seq_len': 30,  # TimeMixeréœ€è¦æ›´é•¿çš„åºåˆ—é•¿åº¦ä»¥é…åˆä¸‹é‡‡æ ·
            'down_sampling_window': 2,
            'down_sampling_layers': 3,
            'down_sampling_method': 'avg',
            'use_future_temporal_feature': True,
            'decomp_method': 'moving_avg',
            'moving_avg_window': 25,
            'channel_independence': False,
            'decomp_kernel': [32],
            'conv_kernel': [24],
            'freq': 'd'  # ä½¿ç”¨æ—¥é¢‘ç‡ï¼Œé…åˆ4ä¸ªæ—¶é—´ç‰¹å¾ï¼šyear, month, day, weekday
        },
        'SCINet': {
            'hidden_size': 1,
            'num_stacks': 1,
            'num_levels': 3,
            'concat_len': 0,
            'groups': 1,
            'kernel': 5,
            'dropout': 0.5,
            'single_step_output_One': 0,
            'input_len_seg': 0,
            'positionalE': False,
            'modified': True,
            'RIN': False
        },
        'Pyraformer': {
            'embed': 'fixed',  # Pyraformerä½¿ç”¨fixed embedding
            'freq': 'd',  # ä½¿ç”¨æ—¥é¢‘ç‡ï¼Œé…åˆ4ä¸ªæ—¶é—´ç‰¹å¾ï¼šyear, month, day, weekday
            'window_size': [2, 2],  # å‡å°çª—å£å¤§å°ä»¥é€‚åº”æ›´çŸ­çš„åºåˆ—
            'inner_size': 3,        # å‡å°inner_sizeä»¥é€‚åº”æ›´çŸ­çš„åºåˆ—
            'CSCM': 'Bottleneck_Construct',
            'truncate': True,
            'use_tvm': False,
            'decoder': 'FC'
        },
        'ETSformer': {
            'top_k': 5,  # ETSformeréœ€è¦çš„top_kå‚æ•°
            'e_layers': 2,  # ç¡®ä¿ç¼–ç å™¨å±‚æ•°
            'd_layers': 2   # ç¡®ä¿è§£ç å™¨å±‚æ•°ç›¸ç­‰
        },
        'TimeXer': {
            'features': 'M',  # TimeXeréœ€è¦çš„featureså‚æ•°
            'patch_len': 16,  # patchç›¸å…³å‚æ•°
            'stride': 8,      # strideå‚æ•°
            'enc_in': 38,     # ç¡®ä¿è¾“å…¥ç»´åº¦
            'c_out': 39       # ç¡®ä¿è¾“å‡ºç»´åº¦
        },
        'CrossLinear': {
            'features': 'M',  # CrossLinearéœ€è¦çš„featureså‚æ•°
            'patch_len': 16,  # patchç›¸å…³å‚æ•°
            'alpha': 0.5,     # CrossLinearçš„alphaå‚æ•°
            'beta': 0.5       # CrossLinearçš„betaå‚æ•°
        },
        'TimesNet': {
            'top_k': 5,       # TimesNetéœ€è¦çš„top_kå‚æ•°
            'num_kernels': 6  # TimesNetçš„num_kernelså‚æ•°
        },
        # å…¶ä»–æ¨¡å‹ä½¿ç”¨é»˜è®¤çš„label_len=0
    }
    
    # åº”ç”¨ç‰¹å®šæ¨¡å‹é…ç½®
    if model_name and model_name in model_specific_configs:
        base_config.update(model_specific_configs[model_name])
    
    return base_config

# å‘åå…¼å®¹çš„åˆ«å
ModelAdapter = UnifiedModelAdapter
get_model_configs = get_unified_model_configs

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # æµ‹è¯•ç»Ÿä¸€é€‚é…å™¨
    batch_size = 4
    seq_len = 30
    pred_len = 7
    d_model = 38
    
    # æ¨¡æ‹Ÿæ•°æ®
    past_data = torch.randn(batch_size, d_model, 365)
    future_data = torch.randn(batch_size, d_model, 30)
    date_strings = ['20240101', '20240102', '20240103', '20240104']
    
    print("ğŸ§ª ç»Ÿä¸€æ¨¡å‹é€‚é…å™¨æµ‹è¯•")
    print("=" * 50)
    
    # æµ‹è¯•æ ‡å‡†æ¨¡å‹é€‚é…
    print("\nğŸ“‹ æ ‡å‡†æ¨¡å‹é…ç½®:")
    standard_config = get_unified_model_configs('Autoformer', 'standard')
    print(f"  d_model: {standard_config['d_model']}")
    print(f"  n_heads: {standard_config['n_heads']}")
    print(f"  d_ff: {standard_config['d_ff']}")
    print(f"  e_layers: {standard_config['e_layers']}")
    
    adapter_std = UnifiedModelAdapter(seq_len=seq_len, pred_len=pred_len, d_model=d_model, label_len=3, model_type='standard')
    x_enc, x_mark_enc, x_dec, x_mark_dec = adapter_std.adapt_inputs(past_data, future_data, date_strings)
    
    print(f"\nğŸ“Š æ ‡å‡†æ¨¡å‹è¾“å…¥:")
    print(f"  x_enc: {x_enc.shape}")
    print(f"  x_mark_enc: {x_mark_enc.shape}")
    print(f"  x_dec: {x_dec.shape}")
    print(f"  x_mark_dec: {x_mark_dec.shape}")
    
    # æµ‹è¯•10xæ¨¡å‹é€‚é…
    print("\nğŸ“‹ 10xæ¨¡å‹é…ç½®:")
    config_10x = get_unified_model_configs('Autoformer', '10x')
    print(f"  d_model: {config_10x['d_model']}")
    print(f"  n_heads: {config_10x['n_heads']}")
    print(f"  d_ff: {config_10x['d_ff']}")
    print(f"  e_layers: {config_10x['e_layers']}")
    
    adapter_10x = UnifiedModelAdapter(seq_len=seq_len, pred_len=pred_len, d_model=d_model, label_len=3, model_type='10x')
    x_enc_10x, x_mark_enc_10x, x_dec_10x, x_mark_dec_10x = adapter_10x.adapt_inputs(past_data, future_data, date_strings)
    
    print(f"\nğŸ“Š 10xæ¨¡å‹è¾“å…¥:")
    print(f"  x_enc: {x_enc_10x.shape}")
    print(f"  x_mark_enc: {x_mark_enc_10x.shape}")
    print(f"  x_dec: {x_dec_10x.shape}")
    print(f"  x_mark_dec: {x_mark_dec_10x.shape}")
    
    print(f"\nğŸ” æ—¶é—´ç‰¹å¾ç¤ºä¾‹: {x_mark_enc[0, 0, :]} (æœˆä»½, æ˜ŸæœŸ, å¹´å†…å¤©æ•°)")
    
    print("\nâœ… ç»Ÿä¸€é€‚é…å™¨æµ‹è¯•å®Œæˆï¼") 