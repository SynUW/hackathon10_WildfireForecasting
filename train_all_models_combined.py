#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é‡ç«é¢„æµ‹æ¨¡å‹å…¨é¢å¯¹æ¯”å®éªŒ - ç»Ÿä¸€ç‰ˆæœ¬
å…ˆè®­ç»ƒæµ‹è¯•model_zooä¸­çš„æ‰€æœ‰æ¨¡å‹ï¼Œå†è®­ç»ƒæµ‹è¯•model_zoo_10xä¸­çš„æ‰€æœ‰10å€å‚æ•°æ¨¡å‹
æ”¯æŒearly stoppingã€F1è¯„ä»·æŒ‡æ ‡ã€æœ€ä½³æ¨¡å‹æµ‹è¯•å’ŒCSVç»“æœå¯¼å‡º
"""

from dataload_year import TimeSeriesDataLoader, TimeSeriesPixelDataset, FullDatasetLoader
from torch.utils.data import Dataset, DataLoader, Subset
import torch
import torch.nn as nn
import torch.optim as optim

import numpy as np
from sklearn.metrics import precision_score, recall_score, f1_score, average_precision_score
import os
import random
import importlib
from datetime import datetime, timedelta
import glob
import torch.nn.functional as F
import warnings
import pandas as pd
import sys
import time
import argparse

# åŠ¨æ€å¯¼å…¥wandb
try:
    import wandb
    WANDB_AVAILABLE = True
except ImportError:
    WANDB_AVAILABLE = False
    print("âš ï¸ wandbæœªå®‰è£…ï¼Œå°†è·³è¿‡wandbç›‘æ§åŠŸèƒ½")

warnings.filterwarnings("ignore")

# =============================================================================
# é…ç½®å‚æ•°
# =============================================================================

# å…¨å±€è®­ç»ƒé…ç½® - ç»Ÿä¸€ç®¡ç†æ‰€æœ‰è®­ç»ƒç›¸å…³å‚æ•°
ENABLE_10X_TRAINING = True        # æ˜¯å¦è®­ç»ƒ10xæ¨¡å‹çš„å…¨å±€å¼€å…³
WANDB_ENABLED = True               # æ˜¯å¦å¯ç”¨WandBç›‘æ§
GLOBAL_SEED = 42                   # å…¨å±€éšæœºç§å­
DEFAULT_PATIENCE = 20              # é»˜è®¤early stopping patience
DEFAULT_MAX_PARALLEL_PER_GPU = 2   # é»˜è®¤æ¯GPUæœ€å¤§å¹¶è¡Œä»»åŠ¡æ•°

# å¤šä»»åŠ¡å­¦ä¹ é…ç½®
MULTITASK_CONFIG = {
    'firms_weight': 1,           # FIRMSé¢„æµ‹çš„æŸå¤±æƒé‡ã€‚å…¸å‹çš„lossç»“åˆï¼ˆother drivers loss*weightä¹‹åï¼‰ï¼šFIRMS loss: 0.3112890124320984, Other drivers loss: 0.0020517727825790644
    'other_drivers_weight': 1.0,   # å…¶ä»–é©±åŠ¨å› ç´ é¢„æµ‹çš„æŸå¤±æƒé‡
    'ignore_zero_values': True,    # æ˜¯å¦å¿½ç•¥å…¶ä»–é©±åŠ¨å› ç´ ä¸­çš„0å€¼
    'loss_function': 'mse',       # æŸå¤±å‡½æ•°ç±»å‹ï¼š'huber', 'mse', 'mae'
    'loss_type': 'focal'          # æŸå¤±å‡½æ•°ç±»å‹é€‰æ‹©ï¼š'focal'(MultiTaskFocalLoss) æˆ– 'kldiv'(MultiTaskKLDivLoss) æˆ– 'multitask'(MultiTaskLoss)
}

# æ•°æ®é›†å¹´ä»½é…ç½®
DEFAULT_TRAIN_YEARS = [2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 
                      2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020]
DEFAULT_VAL_YEARS = [2021, 2022]
DEFAULT_TEST_YEARS = [2023, 2024]

# æ¨¡å‹ç›®å½•é…ç½®
# target_all_channels = target_all_channels.clone()
# target_all_channels[:, :, 0] = (target_all_channels[:, :, 0] > 10).float() åˆ«å¿˜äº†æŠŠè¿™2è¡Œæ¶ˆæ‰
STANDARD_MODEL_DIR = '/mnt/raid/zhengsen/pths/7to1_Focal_woFirms_onlyFirmsLoss_newloadertest'  
MODEL_10X_DIR = '/mnt/raid/zhengsen/pths/model_pth_20epoch_MSE_10x'

def print_config_status():
    """æ‰“å°å½“å‰é…ç½®çŠ¶æ€"""
    print("ğŸ“‹ å½“å‰è®­ç»ƒé…ç½®:")
    print(f"   10xæ¨¡å‹è®­ç»ƒ: {'âœ… å¯ç”¨' if ENABLE_10X_TRAINING else 'âŒ ç¦ç”¨'}")
    print(f"   WandBç›‘æ§: {'âœ… å¯ç”¨' if WANDB_ENABLED else 'âŒ ç¦ç”¨'}")
    print(f"   éšæœºç§å­: {GLOBAL_SEED}")
    print(f"   é»˜è®¤å¹¶è¡Œæ•°: {DEFAULT_MAX_PARALLEL_PER_GPU}/GPU")
    print(f"   Early Stopping patience: {DEFAULT_PATIENCE}")
    print(f"   å¤šä»»åŠ¡Lossç±»å‹: {MULTITASK_CONFIG['loss_type'].upper()}")
    print(f"   FIRMSæƒé‡: {MULTITASK_CONFIG['firms_weight']}")
    print(f"   å…¶ä»–é©±åŠ¨å› ç´ æƒé‡: {MULTITASK_CONFIG['other_drivers_weight']}")
    print(f"   å¿½ç•¥0å€¼: {'âœ…' if MULTITASK_CONFIG['ignore_zero_values'] else 'âŒ'}")
    print(f"   å›å½’æŸå¤±å‡½æ•°: {MULTITASK_CONFIG['loss_function']}")
    if MULTITASK_CONFIG['loss_type'] == 'focal':
        print(f"   Focal Loss Î±: {TRAINING_CONFIG['focal_alpha']}")
        print(f"   Focal Loss Î³: {TRAINING_CONFIG['focal_gamma']}")
    elif MULTITASK_CONFIG['loss_type'] == 'kldiv':
        print(f"   KLæ•£åº¦æ¸©åº¦å‚æ•°: 1.0")
    elif MULTITASK_CONFIG['loss_type'] == 'multitask':
        print(f"   ç»Ÿä¸€æŸå¤±å‡½æ•°: {MULTITASK_CONFIG['loss_function']}")
    
    # ğŸ”¥ æ–°å¢ï¼šä½ç½®ä¿¡æ¯å’Œæ°”è±¡æ•°æ®ç‰¹å¾çŠ¶æ€
    print(f"\nğŸ”§ æ•°æ®ç‰¹å¾é…ç½®:")
    print(f"   ä½ç½®ä¿¡æ¯ç‰¹å¾: {'âœ… å¯ç”¨' if DATA_CONFIG['enable_position_features'] else 'âŒ ç¦ç”¨'}")
    print(f"   æœªæ¥æ°”è±¡æ•°æ®: {'âœ… å¯ç”¨' if DATA_CONFIG['enable_future_weather'] else 'âŒ ç¦ç”¨'}")
    if DATA_CONFIG['enable_future_weather']:
        channels_str = ','.join(map(str, DATA_CONFIG['weather_channels']))
        print(f"   æ°”è±¡é€šé“: [{channels_str}] (å…±{len(DATA_CONFIG['weather_channels'])}ä¸ª)")
    
    # è®¡ç®—æ€»è¾“å…¥é€šé“æ•°
    base_channels = 39
    additional_channels = 0
    if DATA_CONFIG['enable_position_features']:
        additional_channels += 1
    if DATA_CONFIG['enable_future_weather']:
        additional_channels += len(DATA_CONFIG['weather_channels'])
    
    total_channels = base_channels + additional_channels
    if additional_channels > 0:
        print(f"   è¾“å…¥é€šé“æ•°: {base_channels} (åŸºç¡€) + {additional_channels} (ç‰¹å¾) = {total_channels} (æ€»è®¡)")
    else:
        print(f"   è¾“å…¥é€šé“æ•°: {total_channels} (æ ‡å‡†)")

def is_model_trained(model_name, model_type='standard'):
    """
    æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²ç»è®­ç»ƒå®Œæˆ
    é€šè¿‡æ£€æŸ¥final_epochæ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨æ¥åˆ¤æ–­
    """
    model_save_dir = TRAINING_CONFIG[model_type]['model_save_dir']
    final_model_path = os.path.join(model_save_dir, f'{model_name}_final_epoch.pth')
    return os.path.exists(final_model_path)

def get_trained_model_paths(model_name, model_type='standard'):
    """
    è·å–å·²è®­ç»ƒæ¨¡å‹çš„æ‰€æœ‰ä¿å­˜è·¯å¾„
    è¿”å›åŒ…å«metric_nameå’Œpathçš„å­—å…¸
    """
    model_save_dir = TRAINING_CONFIG[model_type]['model_save_dir']
    metric_types = ['f1', 'recall', 'pr_auc', 'mae', 'mse', 'final_epoch']
    
    trained_paths = {}
    for metric_type in metric_types:
        if metric_type == 'final_epoch':
            path = os.path.join(model_save_dir, f'{model_name}_final_epoch.pth')
        else:
            path = os.path.join(model_save_dir, f'{model_name}_best_{metric_type}.pth')
        
        if os.path.exists(path):
            trained_paths[metric_type] = {'path': path, 'score': 0.0}  # scoreä¼šåœ¨æµ‹è¯•æ—¶æ›´æ–°
    
    return trained_paths

def filter_trained_models(model_list, model_type='standard', force_retrain=False):
    """
    è¿‡æ»¤å·²è®­ç»ƒçš„æ¨¡å‹
    è¿”å› (éœ€è¦è®­ç»ƒçš„æ¨¡å‹åˆ—è¡¨, å·²è®­ç»ƒçš„æ¨¡å‹å­—å…¸)
    """
    if force_retrain:
        print(f"ğŸ”„ å¼ºåˆ¶é‡æ–°è®­ç»ƒæ¨¡å¼ï¼šå°†è®­ç»ƒæ‰€æœ‰ {len(model_list)} ä¸ª{model_type}æ¨¡å‹")
        return model_list, {}
    
    models_to_train = []
    trained_models = {}
    
    print(f"ğŸ” æ£€æŸ¥{model_type}æ¨¡å‹è®­ç»ƒçŠ¶æ€...")
    
    for model_name in model_list:
        if is_model_trained(model_name, model_type):
            trained_paths = get_trained_model_paths(model_name, model_type)
            trained_models[model_name] = trained_paths
            print(f"âœ… {model_name}: å·²è®­ç»ƒå®Œæˆ ({len(trained_paths)}ä¸ªä¿å­˜ç‰ˆæœ¬)")
        else:
            models_to_train.append(model_name)
            print(f"âŒ {model_name}: éœ€è¦è®­ç»ƒ")
    
    print(f"\nğŸ“Š {model_type}æ¨¡å‹çŠ¶æ€ç»Ÿè®¡:")
    print(f"   éœ€è¦è®­ç»ƒ: {len(models_to_train)} ä¸ª")
    print(f"   å·²è®­ç»ƒå®Œæˆ: {len(trained_models)} ä¸ª")
    
    if models_to_train:
        print(f"   å°†è®­ç»ƒ: {', '.join(models_to_train)}")
    if trained_models:
        print(f"   è·³è¿‡è®­ç»ƒ: {', '.join(trained_models.keys())}")
    
    return models_to_train, trained_models

def get_all_models(model_zoo_path):
    """è·å–æŒ‡å®šmodel_zooä¸­æ‰€æœ‰å¯ç”¨çš„æ¨¡å‹"""
    model_files = []
    if os.path.exists(model_zoo_path):
        for file in os.listdir(model_zoo_path):
            if file.endswith('.py') and not file.startswith('__') and file != 'trash':
                model_name = file[:-3]  # å»æ‰.pyåç¼€
                model_files.append(model_name)
    return sorted(model_files)

# è·å–æ ‡å‡†æ¨¡å‹å’Œ10xæ¨¡å‹åˆ—è¡¨
MODEL_LIST_STANDARD = get_all_models('model_zoo')

if ENABLE_10X_TRAINING:
    MODEL_LIST_10X = get_all_models('model_zoo_10x')
else:
    MODEL_LIST_10X = []  # ç©ºåˆ—è¡¨ï¼Œè·³è¿‡10xæ¨¡å‹è®­ç»ƒ

print(f"å‘ç°æ ‡å‡†æ¨¡å‹: {MODEL_LIST_STANDARD}")
print(f"å‘ç°10xæ¨¡å‹: {MODEL_LIST_10X}")
if not ENABLE_10X_TRAINING:
    print("âš ï¸  10xæ¨¡å‹è®­ç»ƒå·²ç¦ç”¨")

# è®­ç»ƒé…ç½®
TRAINING_CONFIG = {
    'use_wandb': WANDB_ENABLED,         # ä½¿ç”¨WandBé…ç½®
    'seed': GLOBAL_SEED,                # ä½¿ç”¨éšæœºç§å­
    'patience': DEFAULT_PATIENCE,       # ä½¿ç”¨patienceé…ç½®
    'seq_len': 7,                      # è¾“å…¥åºåˆ—é•¿åº¦
    'pred_len': 1,                      # é¢„æµ‹åºåˆ—é•¿åº¦
    'focal_alpha': 0.5,                 # ä½¿ç”¨æœ€ä½³çš„Focal Lossæ­£æ ·æœ¬æƒé‡
    'focal_gamma': 2.0,                 # Focal Lossèšç„¦å‚æ•°
    
    # æ ‡å‡†æ¨¡å‹é…ç½®
    'standard': {
        'epochs': 20,
        'batch_size': 128,
        'learning_rate': 5e-5,          # é™ä½å­¦ä¹ ç‡ï¼Œä¸10xæ¨¡å‹ä¸€è‡´
        'weight_decay': 1e-4,
        'T_0': 20,
        'T_mult': 2,
        'eta_min':1e-5,
        'max_grad_norm': 0.0,           # å¯ç”¨æ¢¯åº¦è£å‰ªé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸; 0.0è¡¨ç¤ºä¸è£å‰ª
        'model_save_dir': STANDARD_MODEL_DIR,
    },
    
    # 10xæ¨¡å‹é…ç½®ï¼ˆè€ƒè™‘æ˜¾å­˜é™åˆ¶ï¼‰
    '10x': {
        'epochs': 20,
        'batch_size': 128,
        'learning_rate': 5e-5,
        'weight_decay': 1e-4,
        'T_0': 20,
        'T_mult': 2,
        'eta_min': 1e-5,
        'max_grad_norm': 0.0,           # å¯ç”¨æ¢¯åº¦è£å‰ªé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
        'model_save_dir': MODEL_10X_DIR,
    }
}

# æ•°æ®é…ç½®
DATA_CONFIG = {
    'train_years': DEFAULT_TRAIN_YEARS,
    'val_years': DEFAULT_VAL_YEARS,
    'test_years': DEFAULT_TEST_YEARS,
    
    # åº•å±‚æ•°æ®é›†é…ç½®ï¼ˆåŠ è½½å®Œæ•´æ•°æ®ï¼‰
    'positive_ratio': 1.0,           # åº•å±‚åŠ è½½æ‰€æœ‰æ­£æ ·æœ¬
    'pos_neg_ratio': 2.0,            # åº•å±‚æ­£è´Ÿæ ·æœ¬æ¯”ä¾‹1:1
    'resample_each_epoch': False,    # åº•å±‚ç¦ç”¨é‡æ–°æŠ½æ ·ï¼Œæ‰€ä»¥éœ€è¦ä¸€ç›´è®¾ä¸ºFalse
    'firms_min': 0,                  # FIRMSæ•°æ®æœ€å°å€¼ï¼ˆè·³è¿‡ç»Ÿè®¡ï¼‰
    'firms_max': 100,                # FIRMSæ•°æ®æœ€å¤§å€¼ï¼ˆè·³è¿‡ç»Ÿè®¡ï¼‰
    
    # åŠ¨æ€æŠ½æ ·é…ç½®ï¼ˆæ¯epochæŠ½æ ·ï¼‰
    'enable_dynamic_sampling': True,   # æ˜¯å¦å¯ç”¨è®­ç»ƒé›†çš„åŠ¨æ€æŠ½æ ·
    'sampling_ratio': 0.3,            # æ¯epochéšæœºæŠ½æ ·çš„æ•°æ®æ¯”ä¾‹
    
    # ğŸ”¥ æ–°å¢ï¼šä½ç½®ä¿¡æ¯ç‰¹å¾é…ç½®
    'enable_position_features': False,  # æ˜¯å¦å¯ç”¨ä½ç½®ä¿¡æ¯ç‰¹å¾ï¼ˆé»˜è®¤ç¦ç”¨ï¼‰
    'raster_size': (278, 130),         # å›¾åƒå°ºå¯¸ (height, width)ï¼Œç”¨äºä½ç½®å½’ä¸€åŒ–
    
    # ğŸ”¥ æ–°å¢ï¼šæœªæ¥æ°”è±¡æ•°æ®ç‰¹å¾é…ç½®  
    'enable_future_weather': False,    # æ˜¯å¦å¯ç”¨æœªæ¥æ°”è±¡æ•°æ®ç‰¹å¾ï¼ˆé»˜è®¤ç¦ç”¨ï¼‰
    'weather_channels': list(range(1, 13)),  # æ°”è±¡æ•°æ®é€šé“ç´¢å¼•ï¼šç¬¬2-13æ³¢æ®µï¼ˆç´¢å¼•1-12ï¼‰
}

# =============================================================================
# è‡ªå®šä¹‰åŠ¨æ€æŠ½æ ·æ•°æ®é›†ç±»
# =============================================================================

class DynamicSamplingSubset(Dataset):
    """
    æ”¯æŒåŠ¨æ€æŠ½æ ·çš„æ•°æ®é›†å­é›†ï¼ˆç®€åŒ–ç‰ˆï¼‰
    æ¯ä¸ªepochä»å¹³è¡¡çš„æ•°æ®é›†ä¸­éšæœºæŠ½æ ·æŒ‡å®šæ¯”ä¾‹çš„æ•°æ®
    ç”±äºåº•å±‚æ•°æ®é›†å·²ç»æ˜¯1:1å¹³è¡¡çš„ï¼ŒéšæœºæŠ½æ ·ä¼šä¿æŒå¤§è‡´ç›¸åŒçš„æ¯”ä¾‹
    """
    def __init__(self, dataset, full_indices, sampling_ratio=1.0, enable_dynamic_sampling=False):
        """
        Args:
            dataset: åŸå§‹æ•°æ®é›†ï¼ˆå·²ç»æ˜¯1:1å¹³è¡¡çš„ï¼‰
            full_indices: å®Œæ•´çš„ç´¢å¼•åˆ—è¡¨
            sampling_ratio: æ¯epochä½¿ç”¨çš„æ•°æ®æ¯”ä¾‹ (0.0-1.0)
            enable_dynamic_sampling: æ˜¯å¦å¯ç”¨åŠ¨æ€æŠ½æ ·
        """
        self.dataset = dataset
        self.full_indices = full_indices
        self.sampling_ratio = sampling_ratio
        self.enable_dynamic_sampling = enable_dynamic_sampling
        
        # å½“å‰ä½¿ç”¨çš„ç´¢å¼•
        if enable_dynamic_sampling and sampling_ratio < 1.0:
            self.current_indices = self._sample_indices(epoch_seed=42)
        else:
            self.current_indices = full_indices
            
        print(f"ğŸ“Š DynamicSamplingSubsetåˆå§‹åŒ–:")
        print(f"   æ€»ç´¢å¼•: {len(full_indices)}")
        print(f"   å½“å‰ä½¿ç”¨: {len(self.current_indices)}")
        print(f"   æŠ½æ ·æ¯”ä¾‹: {sampling_ratio:.1%}")
        print(f"   åŠ¨æ€æŠ½æ ·: {'å¯ç”¨' if enable_dynamic_sampling else 'ç¦ç”¨'}")
    
    def _sample_indices(self, epoch_seed):
        """æ ¹æ®epochç§å­éšæœºæŠ½æ ·ç´¢å¼•"""
        if not self.enable_dynamic_sampling or self.sampling_ratio >= 1.0:
            return self.full_indices
            
        # è®¾ç½®éšæœºç§å­ç¡®ä¿å¯é‡å¤æ€§
        np.random.seed(epoch_seed)
        random.seed(epoch_seed)
        
        # è®¡ç®—æŠ½æ ·æ•°é‡
        sample_size = int(len(self.full_indices) * self.sampling_ratio)
        sample_size = max(1, sample_size)  # è‡³å°‘ä¿è¯1ä¸ªæ ·æœ¬
        sample_size = min(sample_size, len(self.full_indices))  # ä¸è¶…è¿‡å¯ç”¨æ•°é‡
        
        # éšæœºæŠ½æ ·
        sampled_indices = np.random.choice(self.full_indices, size=sample_size, replace=False)
        return sampled_indices.tolist()
    
    def resample_for_epoch(self, epoch):
        """ä¸ºæ–°epoché‡æ–°æŠ½æ ·"""
        if not self.enable_dynamic_sampling:
            return
            
        # old_size = len(self.current_indices)
        self.current_indices = self._sample_indices(epoch_seed=42 + epoch)
        # new_size = len(self.current_indices)
        
        # print(f"ğŸ”„ Epoch {epoch+1}: é‡æ–°æŠ½æ ·å®Œæˆ {old_size} â†’ {new_size} æ ·æœ¬ (æ¯”ä¾‹: {self.sampling_ratio:.1%})")
    
    def __len__(self):
        return len(self.current_indices)
    
    def __getitem__(self, idx):
        # å°†å½“å‰ç´¢å¼•æ˜ å°„åˆ°åŸå§‹æ•°æ®é›†çš„å®é™…ç´¢å¼•
        actual_idx = self.current_indices[idx]
        return self.dataset[actual_idx]

# =============================================================================
# å·¥å…·å‡½æ•°
# =============================================================================

def set_seed(seed):
    """è®¾ç½®éšæœºç§å­"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    os.environ['PYTHONHASHSEED'] = str(seed)

def worker_init_fn(worker_id):
    """DataLoader workeråˆå§‹åŒ–å‡½æ•°ï¼Œç¡®ä¿å¤šè¿›ç¨‹çš„å¯é‡å¤æ€§"""
    worker_seed = torch.initial_seed() % 2**32
    np.random.seed(worker_seed)
    random.seed(worker_seed)

class FIRMSNormalizer:
    """FIRMSæ•°æ®å½’ä¸€åŒ–å™¨"""
    
    def __init__(self, method='divide_by_100', firms_min=None, firms_max=None):
        self.method = method
        self.firms_min = firms_min
        self.firms_max = firms_max
        self.fitted = False
        
    def fit(self, data_loader):
        """æ‹Ÿåˆå½’ä¸€åŒ–å™¨"""
        if self.firms_min is not None and self.firms_max is not None:
            print(f"ğŸš€ ä½¿ç”¨æŒ‡å®šçš„FIRMSæ•°æ®èŒƒå›´: [{self.firms_min}, {self.firms_max}]")
            if self.method == 'log1p_minmax':
                self.global_min = np.log1p(self.firms_min)
                self.global_max = np.log1p(self.firms_max)
            elif self.method == 'divide_by_100':
                self.global_min = self.firms_min / 100.0
                self.global_max = self.firms_max / 100.0
            else:
                self.global_min = self.firms_min
                self.global_max = self.firms_max
            self.fitted = True
            print(f"âœ… å½’ä¸€åŒ–å™¨åˆå§‹åŒ–å®Œæˆ (å˜æ¢åèŒƒå›´: {self.global_min:.2f}-{self.global_max:.2f})")
            return
            
        print("ğŸ”§ æ”¶é›†FIRMSæ•°æ®è¿›è¡Œå½’ä¸€åŒ–æ‹Ÿåˆ...")
        firms_values = []
        
        # ç®€åŒ–æ•°æ®æ”¶é›†è¿‡ç¨‹ï¼Œå‡å°‘é¢‘ç¹çš„è¿›åº¦æ˜¾ç¤ºä»¥æé«˜æ€§èƒ½
        # progress = SimpleProgressTracker()
        for i, batch in enumerate(data_loader):
            # åªåœ¨å…³é”®èŠ‚ç‚¹æ˜¾ç¤ºè¿›åº¦ï¼Œè€Œä¸æ˜¯æ¯ä¸ªbatch
            if i % max(1, len(data_loader) // 10) == 0:  # æ¯10%æ˜¾ç¤ºä¸€æ¬¡
                print(f"ğŸ“Š æ”¶é›†FIRMSæ•°æ®è¿›åº¦: {i+1}/{len(data_loader)} ({100*(i+1)/len(data_loader):.0f}%)", end='\r')
            # progress.update(i+1, len(data_loader), "ğŸ“Š æ”¶é›†FIRMSæ•°æ®")
            past, future, _ = batch
            firms_data = past[:, 0, :]  # FIRMSé€šé“ (B, T)
            firms_values.append(firms_data.numpy())
        
        print()  # æ¢è¡Œ
        
        all_firms = np.concatenate(firms_values, axis=0).flatten()
        valid_firms = all_firms[all_firms != 255]  # è¿‡æ»¤æ‰NoDataå€¼(255)
        
        if self.method == 'log1p_minmax':
            log_firms = np.log1p(valid_firms)
            self.global_min = log_firms.min()
            self.global_max = log_firms.max()
        elif self.method == 'divide_by_100':
            divide_firms = valid_firms / 100.0
            self.global_min = divide_firms.min()
            self.global_max = divide_firms.max()
        else:
            self.global_min = valid_firms.min()
            self.global_max = valid_firms.max()
            
        self.fitted = True
        print(f"âœ… {self.method.upper()}å½’ä¸€åŒ–å®Œæˆ (èŒƒå›´: {self.global_min:.2f}-{self.global_max:.2f})")
        
    def normalize(self, firms_data):
        """å½’ä¸€åŒ–FIRMSæ•°æ®"""
        if not self.fitted:
            raise ValueError("å½’ä¸€åŒ–å™¨å°šæœªæ‹Ÿåˆï¼Œè¯·å…ˆè°ƒç”¨fit()æ–¹æ³•")
            
        if self.method == 'log1p_minmax':
            log1p_data = torch.log1p(firms_data)
            if self.global_max > self.global_min:
                return (log1p_data - self.global_min) / (self.global_max - self.global_min)
            else:
                return log1p_data
        elif self.method == 'divide_by_100':
            return firms_data / 100.0
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„å½’ä¸€åŒ–æ–¹æ³•: {self.method}")
    
    def transform_tensor(self, tensor_data):
        """ä¸ºtensoræ•°æ®åº”ç”¨å½’ä¸€åŒ–å˜æ¢ï¼ˆå…¼å®¹æ–¹æ³•ï¼‰"""
        return self.normalize(tensor_data)
    
    def inverse_transform_numpy(self, normalized_data):
        """å¯¹å½’ä¸€åŒ–åçš„numpyæ•°æ®è¿›è¡Œåå˜æ¢"""
        if not self.fitted:
            raise ValueError("å½’ä¸€åŒ–å™¨å°šæœªæ‹Ÿåˆï¼Œè¯·å…ˆè°ƒç”¨fit()æ–¹æ³•")
        
        if isinstance(normalized_data, torch.Tensor):
            normalized_data = normalized_data.cpu().numpy()
        
        if self.method == 'log1p_minmax':
            if self.global_max > self.global_min:
                # åå½’ä¸€åŒ–: y = x * (max - min) + min
                log_data = normalized_data * (self.global_max - self.global_min) + self.global_min
            else:
                log_data = normalized_data
            # ålog1på˜æ¢: expm1(log_data)
            return np.expm1(log_data)
        elif self.method == 'divide_by_100':
            return normalized_data * 100.0
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„å½’ä¸€åŒ–æ–¹æ³•: {self.method}")

def add_position_features(data, metadata_list, raster_size):
    """
    ä¸ºæ•°æ®æ·»åŠ ä½ç½®ä¿¡æ¯ç‰¹å¾
    
    Args:
        data: è¾“å…¥æ•°æ® (batch_size, channels, time_steps)
        metadata_list: å…ƒæ•°æ®åˆ—è¡¨ï¼ŒåŒ…å«ä½ç½®ä¿¡æ¯
        raster_size: å›¾åƒå°ºå¯¸ (height, width)
    
    Returns:
        æ·»åŠ ä½ç½®ç‰¹å¾åçš„æ•°æ® (batch_size, channels+1, time_steps)
    """
    batch_size, channels, time_steps = data.shape
    height, width = raster_size
    
    # åˆ›å»ºä½ç½®ç‰¹å¾å¼ é‡
    position_features = torch.zeros(batch_size, 1, time_steps, device=data.device)
    
    for i, metadata in enumerate(metadata_list):
        # ğŸ”¥ ä¿®å¤ï¼šæ­£ç¡®ä»metadataä¸­æå–ä½ç½®ä¿¡æ¯
        try:
            if isinstance(metadata, dict):
                # å¦‚æœmetadataæ˜¯å­—å…¸æ ¼å¼ï¼ˆä»dataload.pyçš„_parse_dataset_keyè¿”å›ï¼‰
                row = metadata.get('row', 0)
                col = metadata.get('col', 0)
            elif hasattr(metadata, '__len__') and len(metadata) >= 3:
                # å¦‚æœmetadataæ˜¯åˆ—è¡¨/å…ƒç»„æ ¼å¼
                if len(metadata) >= 3:
                    # å°è¯•ä¸åŒçš„metadataæ ¼å¼
                    # æ ¼å¼1: [date_int, row, col, ...]
                    try:
                        row, col = int(metadata[1]), int(metadata[2])
                    except (ValueError, IndexError):
                        # æ ¼å¼2: [date_int, firms_value, row, col, ...]
                        try:
                            row, col = int(metadata[2]), int(metadata[3])
                        except (ValueError, IndexError):
                            row, col = 0, 0
                else:
                    row, col = 0, 0
            else:
                # å¦‚æœmetadataæ˜¯å•ä¸ªå€¼ï¼ˆå¯èƒ½æ˜¯date_intï¼‰
                row, col = 0, 0
        except Exception as e:
            print(f"âš ï¸ ä½ç½®ä¿¡æ¯æå–å¤±è´¥: {e}, metadata: {metadata}")
            row, col = 0, 0
        
        # å½’ä¸€åŒ–ä½ç½®åæ ‡åˆ°0-1èŒƒå›´
        norm_row = row / (height - 1) if height > 1 else 0.0
        norm_col = col / (width - 1) if width > 1 else 0.0
        
        # å°†å½’ä¸€åŒ–çš„ä½ç½®ä¿¡æ¯ç¼–ç ä¸ºå•ä¸€å€¼ (å¯ä»¥ä½¿ç”¨ä¸åŒçš„ç¼–ç æ–¹å¼)
        # è¿™é‡Œä½¿ç”¨ç®€å•çš„çº¿æ€§ç»„åˆï¼šrow_weight * norm_row + col_weight * norm_col
        position_value = 0.5 * norm_row + 0.5 * norm_col
        
        # å°†ä½ç½®ç‰¹å¾åº”ç”¨åˆ°æ‰€æœ‰æ—¶é—´æ­¥
        position_features[i, 0, :] = position_value
    
    # å°†ä½ç½®ç‰¹å¾æ‹¼æ¥åˆ°åŸå§‹æ•°æ®
    enhanced_data = torch.cat([data, position_features], dim=1)
    return enhanced_data

def add_weather_features(past_data, future_data, weather_channels):
    """
    ä»futureæ•°æ®ä¸­æå–æ°”è±¡ç‰¹å¾å¹¶æ·»åŠ åˆ°pastæ•°æ®
    
    Args:
        past_data: è¿‡å»æ•°æ® (batch_size, channels, past_time_steps)
        future_data: æœªæ¥æ•°æ® (batch_size, channels, future_time_steps)  
        weather_channels: æ°”è±¡æ•°æ®é€šé“ç´¢å¼•åˆ—è¡¨
    
    Returns:
        æ·»åŠ æ°”è±¡ç‰¹å¾åçš„pastæ•°æ® (batch_size, channels+len(weather_channels), past_time_steps)
    """
    batch_size, channels, past_time_steps = past_data.shape
    future_time_steps = future_data.shape[2]
    
    # æå–æœªæ¥çš„æ°”è±¡æ•°æ® (batch_size, len(weather_channels), future_time_steps)
    future_weather = future_data[:, weather_channels, :]
    
    # å°†æœªæ¥æ°”è±¡æ•°æ®é‡å¤æˆ–æ’å€¼åˆ°pastæ—¶é—´æ­¥é•¿åº¦
    if future_time_steps != past_time_steps:
        # ä½¿ç”¨çº¿æ€§æ’å€¼è°ƒæ•´æ—¶é—´ç»´åº¦
        future_weather = F.interpolate(
            future_weather, 
            size=past_time_steps, 
            mode='linear', 
            align_corners=False
        )
    
    # å°†æ°”è±¡ç‰¹å¾æ‹¼æ¥åˆ°pastæ•°æ®
    enhanced_past = torch.cat([past_data, future_weather], dim=1)
    return enhanced_past

def normalize_batch(past, future, firms_normalizer=None, metadata_list=None):
    """
    å¯¹æ‰¹æ¬¡æ•°æ®è¿›è¡Œå½’ä¸€åŒ–å¤„ç†ï¼Œå¹¶å¯é€‰åœ°æ·»åŠ ä½ç½®ä¿¡æ¯å’Œæ°”è±¡æ•°æ®ç‰¹å¾
    
    Args:
        past: è¿‡å»æ•°æ® (batch_size, channels, past_time_steps)
        future: æœªæ¥æ•°æ® (batch_size, channels, future_time_steps)
        firms_normalizer: FIRMSæ•°æ®å½’ä¸€åŒ–å™¨
        metadata_list: å…ƒæ•°æ®åˆ—è¡¨ï¼Œç”¨äºæå–ä½ç½®ä¿¡æ¯
    
    Returns:
        å¤„ç†åçš„ (past, future) æ•°æ®å…ƒç»„
    """
    # ğŸ”¥ å…³é”®ï¼šå…ˆå¤„ç†æ‰€æœ‰é€šé“çš„NaNå€¼ï¼Œå°†å…¶æ›¿æ¢ä¸º0
    nan_mask_past = torch.isnan(past)
    past[nan_mask_past] = 0.0
    nan_mask_future = torch.isnan(future)
    future[nan_mask_future] = 0.0
    
    # å¯¹ç¬¬0ä¸ªé€šé“ï¼ˆFIRMSï¼‰è¿›è¡Œå½’ä¸€åŒ–ï¼ˆpastå’Œfutureéƒ½è¦ï¼‰
    if firms_normalizer is not None:
        past[:, 0, :] = firms_normalizer.normalize(past[:, 0, :])
        future[:, 0, :] = firms_normalizer.normalize(future[:, 0, :])
    
    # ğŸ”¥ æ–°å¢ï¼šæ·»åŠ ä½ç½®ä¿¡æ¯ç‰¹å¾
    if DATA_CONFIG['enable_position_features'] and metadata_list is not None:
        past = add_position_features(past, metadata_list, DATA_CONFIG['raster_size'])
        # æ³¨æ„ï¼šfutureæ•°æ®é€šå¸¸ä¸éœ€è¦æ·»åŠ ä½ç½®ç‰¹å¾ï¼Œå› ä¸ºä½ç½®ä¿¡æ¯ä¸»è¦ç”¨äºè¾“å…¥
        
    # ğŸ”¥ æ–°å¢ï¼šæ·»åŠ æœªæ¥æ°”è±¡æ•°æ®ç‰¹å¾
    if DATA_CONFIG['enable_future_weather']:
        past = add_weather_features(past, future, DATA_CONFIG['weather_channels'])
    
    return past, future

def load_model(model_name, configs, model_type='standard'):
    """åŠ¨æ€åŠ è½½æ¨¡å‹ï¼ˆç»Ÿä¸€ä½¿ç”¨model_zooï¼‰"""
    try:
        # æ£€æŸ¥ç‰¹æ®Šä¾èµ–
        if model_name in ['Mamba', 'Reformer', 'Transformer', 'iTransformer', 's_mamba']:
            try:
                import mamba_ssm
            except ImportError:
                print(f"âš ï¸ æ¨¡å‹ {model_name} éœ€è¦ mamba_ssm åº“")
                print(f"ğŸ’¡ å»ºè®®ä½¿ç”¨ mamba_env ç¯å¢ƒ: conda activate mamba_env")
                raise ImportError(f"æ¨¡å‹ {model_name} éœ€è¦ mamba_ssm åº“ï¼Œè¯·åœ¨ mamba_env ç¯å¢ƒä¸­è¿è¡Œ")
        
        # ç»Ÿä¸€ä½¿ç”¨model_zooï¼Œé€šè¿‡configsä¸­çš„å‚æ•°åŒºåˆ†æ ‡å‡†/10xæ¨¡å‹
        model_zoo_path = os.path.join(os.getcwd(), 'model_zoo')
        module_name = f'model_zoo.{model_name}'
        
        if model_zoo_path not in sys.path:
            sys.path.insert(0, model_zoo_path)
        
        module = importlib.import_module(module_name)
        Model = getattr(module, 'Model')
        
        return Model(configs), model_type
    except Exception as e:
        print(f"åŠ è½½{model_type}æ¨¡å‹ {model_name} å¤±è´¥: {e}")
        raise

def calculate_detailed_metrics(output, target):
    """è®¡ç®—è¯¦ç»†çš„å›å½’å’ŒäºŒåˆ†ç±»æŒ‡æ ‡ï¼ŒåŒ…æ‹¬MSEã€MAEã€PR-AUC"""
    # åŸå§‹è¾“å‡ºå€¼ç”¨äºå›å½’æŒ‡æ ‡
    output_raw = output.view(-1).cpu().numpy()
    target_np = target.view(-1).cpu().numpy()
    
    # è®¡ç®—MSEå’ŒMAEï¼ˆå›å½’æŒ‡æ ‡ï¼Œä½¿ç”¨åŸå§‹è¾“å‡ºå€¼ï¼‰
    mse = np.mean((output_raw - target_np) ** 2)
    mae = np.mean(np.abs(output_raw - target_np))
    
    # Sigmoidå¤„ç†åçš„æ¦‚ç‡å€¼ç”¨äºåˆ†ç±»æŒ‡æ ‡
    pred_probs = torch.sigmoid(output).view(-1).cpu().numpy()
    pred_binary = (pred_probs > 0.5).astype(int)
    target_binary = (target_np > 0).astype(int)
    
    unique_targets = np.unique(target_binary)
    if len(unique_targets) < 2:
        return 0.0, 0.0, 0.0, 0.0, mse, mae
    
    try:
        precision = precision_score(target_binary, pred_binary, average='binary', zero_division=0)
        recall = recall_score(target_binary, pred_binary, average='binary', zero_division=0)
        f1 = f1_score(target_binary, pred_binary, average='binary', zero_division=0)
        pr_auc = average_precision_score(target_binary, pred_probs)
    except Exception as e:
        print(f"è®¡ç®—æŒ‡æ ‡æ—¶å‡ºé”™: {e}")
        return 0.0, 0.0, 0.0, 0.0, mse, mae
    
    return precision, recall, f1, pr_auc, mse, mae

def calculate_optimal_f1_metrics(output, target):
    """è®¡ç®—F1æœ€ä¼˜é˜ˆå€¼ä¸‹çš„è¯¦ç»†æŒ‡æ ‡ï¼Œç”¨äºæµ‹è¯•é˜¶æ®µ - è°ƒè¯•ç‰ˆæœ¬"""
    # åŸå§‹è¾“å‡ºå€¼ç”¨äºå›å½’æŒ‡æ ‡
    output_raw = output.view(-1).cpu().numpy()
    target_np = target.view(-1).cpu().numpy()
    
    # è®¡ç®—MSEå’ŒMAEï¼ˆå›å½’æŒ‡æ ‡ï¼Œä½¿ç”¨åŸå§‹è¾“å‡ºå€¼ï¼‰
    mse = np.mean((output_raw - target_np) ** 2)
    mae = np.mean(np.abs(output_raw - target_np))
    
    # Sigmoidå¤„ç†åçš„æ¦‚ç‡å€¼ç”¨äºåˆ†ç±»æŒ‡æ ‡
    pred_probs = torch.sigmoid(output).view(-1).cpu().numpy()
    target_binary = (target_np > 0).astype(int)
    
    # ğŸ” è°ƒè¯•ä¿¡æ¯ï¼šåˆ†æè¾“å…¥æ•°æ®ç‰¹æ€§
    print(f"   ğŸ” æ•°æ®ç»Ÿè®¡:")
    print(f"      é¢„æµ‹æ ·æœ¬æ•°: {len(pred_probs)}")
    print(f"      çœŸå®é˜³æ€§æ ·æœ¬æ•°: {np.sum(target_binary)}")
    print(f"      çœŸå®é˜³æ€§æ¯”ä¾‹: {np.sum(target_binary) / len(target_binary):.4f}")
    print(f"      é¢„æµ‹æ¦‚ç‡èŒƒå›´: [{np.min(pred_probs):.4f}, {np.max(pred_probs):.4f}]")
    print(f"      é¢„æµ‹æ¦‚ç‡å‡å€¼: {np.mean(pred_probs):.4f}")
    print(f"      é¢„æµ‹æ¦‚ç‡std: {np.std(pred_probs):.4f}")
    
    unique_targets = np.unique(target_binary)
    if len(unique_targets) < 2:
        return 0.0, 0.0, 0.0, 0.0, mse, mae
    
    try:
        # è®¡ç®—PR-AUC
        pr_auc = average_precision_score(target_binary, pred_probs)
        
        # å¯»æ‰¾F1æœ€ä¼˜é˜ˆå€¼
        thresholds = np.linspace(0, 1, 100)  # ä½¿ç”¨1000ä¸ªé˜ˆå€¼ç‚¹è¿›è¡Œæœç´¢
        best_f1 = 0.0
        best_precision = 0.0
        best_recall = 0.0
        best_threshold = 0.5
        
        # ğŸ” è°ƒè¯•ï¼šè®°å½•æ‰€æœ‰é˜ˆå€¼çš„æŒ‡æ ‡
        all_recalls = []
        all_precisions = []
        all_f1s = []
        
        for threshold in thresholds:
            pred_binary_thresh = (pred_probs > threshold).astype(int)
            
            # ğŸ” é˜²æ­¢é™¤é›¶é”™è¯¯ï¼Œæ·»åŠ æ›´è¯¦ç»†çš„æ£€æŸ¥
            tp = np.sum((pred_binary_thresh == 1) & (target_binary == 1))
            fp = np.sum((pred_binary_thresh == 1) & (target_binary == 0))
            fn = np.sum((pred_binary_thresh == 0) & (target_binary == 1))
            tn = np.sum((pred_binary_thresh == 0) & (target_binary == 0))
            
            if tp + fp > 0:
                precision = tp / (tp + fp)
            else:
                precision = 0.0
                
            if tp + fn > 0:
                recall = tp / (tp + fn)
            else:
                recall = 0.0
                
            if precision + recall > 0:
                f1 = 2 * (precision * recall) / (precision + recall)
            else:
                f1 = 0.0
            
            all_recalls.append(recall)
            all_precisions.append(precision)
            all_f1s.append(f1)
            
            if f1 > best_f1:
                best_f1 = f1
                best_precision = precision
                best_recall = recall
                best_threshold = threshold
        
        # ğŸ” è°ƒè¯•ä¿¡æ¯ï¼šåˆ†ærecallçš„åˆ†å¸ƒ
        all_recalls = np.array(all_recalls)
        unique_recalls = np.unique(all_recalls)
        print(f"      å‘ç°{len(unique_recalls)}ä¸ªä¸åŒçš„recallå€¼")
        print(f"      RecallèŒƒå›´: [{np.min(all_recalls):.4f}, {np.max(all_recalls):.4f}]")
        print(f"      æœ€é«˜recall: {np.max(all_recalls):.6f}")
        print(f"      æœ€ä¼˜F1é˜ˆå€¼: {best_threshold:.3f} (F1={best_f1:.4f})")
        
        # ğŸ” å¦‚æœæ‰€æœ‰recalléƒ½ç›¸åŒï¼Œè¯´æ˜æœ‰é—®é¢˜
        if len(unique_recalls) == 1:
            print(f"      âš ï¸  è­¦å‘Šï¼šæ‰€æœ‰é˜ˆå€¼çš„recalléƒ½ç›¸åŒ = {unique_recalls[0]:.6f}")
            print(f"      å¯èƒ½çš„åŸå› ï¼šæ¨¡å‹é¢„æµ‹è¿‡äºé›†ä¸­æˆ–æ•°æ®åˆ†å¸ƒå¼‚å¸¸")
            
        # ğŸ” åˆ†æé˜ˆå€¼åˆ†å¸ƒ
        recall_counts = {}
        for r in all_recalls:
            r_rounded = round(r, 6)
            recall_counts[r_rounded] = recall_counts.get(r_rounded, 0) + 1
        
        print(f"      Top 5 recallå€¼å‡ºç°é¢‘ç‡:")
        for r, count in sorted(recall_counts.items(), key=lambda x: x[1], reverse=True)[:5]:
            print(f"         {r:.6f}: {count}æ¬¡")
        
    except Exception as e:
        print(f"è®¡ç®—æœ€ä¼˜F1æŒ‡æ ‡æ—¶å‡ºé”™: {e}")
        return 0.0, 0.0, 0.0, 0.0, mse, mae
    
    return best_precision, best_recall, best_f1, pr_auc, mse, mae

class Config:
    """é…ç½®ç±» - ä¿®å¤ç±»å‹å®‰å…¨é—®é¢˜"""
    def __init__(self, model_name, model_type='standard'):
        self.model_name = model_name  # æ·»åŠ æ¨¡å‹åç§°å±æ€§
        self.model_type = model_type
        config = TRAINING_CONFIG[model_type]
        
        # åŸºæœ¬è®­ç»ƒå‚æ•° - ç¡®ä¿ç±»å‹å®‰å…¨
        self.epochs = int(config['epochs'])
        self.batch_size = int(config['batch_size'])
        self.learning_rate = float(config['learning_rate'])
        self.weight_decay = float(config['weight_decay'])
        self.T_0 = int(config['T_0'])
        self.T_mult = int(config['T_mult'])
        self.eta_min = float(config['eta_min'])
        self.max_grad_norm = float(config['max_grad_norm'])
        
        # åºåˆ—å‚æ•° - ç¡®ä¿æ˜¯æ•´æ•°ç±»å‹ï¼Œé¿å…Configå¯¹è±¡é—®é¢˜
        self.seq_len = int(TRAINING_CONFIG['seq_len'])
        self.pred_len = int(TRAINING_CONFIG['pred_len'])
        self.label_len = 0  # é»˜è®¤æ ‡ç­¾é•¿åº¦
        
        # æ ¹æ®æ¨¡å‹ç±»å‹è·å–é…ç½®ï¼ˆä½¿ç”¨ç»Ÿä¸€é€‚é…å™¨ï¼‰
        try:
            from model_adapter_unified import get_unified_model_configs
            model_configs = get_unified_model_configs(model_name, model_type)
            
            # å®‰å…¨åœ°è®¾ç½®é…ç½®ï¼Œç¡®ä¿æ•°å€¼ç±»å‹æ­£ç¡®
            for key, value in model_configs.items():
                if key in ['seq_len', 'pred_len']:
                    continue  # è·³è¿‡ï¼Œä½¿ç”¨æˆ‘ä»¬å·²ç»è®¾ç½®çš„å›ºå®šå€¼
                elif isinstance(value, (int, float, str, bool)):
                    setattr(self, key, value)
                elif value is None:
                    setattr(self, key, None)
                else:
                    # å¯¹äºå¤æ‚ç±»å‹ï¼Œå°è¯•è½¬æ¢ä¸ºåŸºæœ¬ç±»å‹
                    try:
                        if isinstance(value, list):
                            setattr(self, key, value)
                        else:
                            setattr(self, key, value)
                    except:
                        print(f"âš ï¸  è·³è¿‡é…ç½® {key}={value} (ç±»å‹: {type(value)})")
                        
        except Exception as e:
            print(f"âš ï¸  åŠ¨æ€é…ç½®å¯¼å…¥å¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
            # ä½¿ç”¨é»˜è®¤é…ç½®
            if model_type == 'standard':
                self.d_model = 512
                self.n_heads = 8
                self.d_ff = 2048
                self.e_layers = 2
                self.d_layers = 2
                self.d_state = 16
                self.d_conv = 4
                self.expand = 2
            else:  # 10x
                self.d_model = 2048
                self.n_heads = 32
                self.d_ff = 2048
                self.e_layers = 4
                self.d_layers = 4
                self.d_state = 32
                self.d_conv = 8
                self.expand = 4
            
            # é€šç”¨æ¨¡å‹å‚æ•°
            self.dropout = 0.1
            self.activation = 'gelu'
            self.output_attention = False
            self.enc_in = 39
            self.dec_in = 39
            self.c_out = 39
            self.embed = 'timeF'
            self.freq = 'd'
            self.factor = 1
            self.moving_avg = 25
            self.channel_independence = False
            self.use_norm = True
            self.distil = True
            self.label_len = 3 if model_name in ['Autoformer', 'Autoformer_M'] else 0
        
        # æ·»åŠ æ–°æ¨¡å‹éœ€è¦çš„ç‰¹æ®Šé…ç½®
        self.task_name = 'long_term_forecast'  # æ–°æ¨¡å‹æ™®ééœ€è¦è¿™ä¸ªå‚æ•°
        
        # ä¸ºç‰¹å®šæ¨¡å‹æ·»åŠ ç‰¹æ®Šé…ç½®
        if model_name == 'DLinear':
            self.moving_avg = 25  # DLinearéœ€è¦moving_avgç”¨äºseries_decomp
            self.individual = False  # DLinearçš„individualå‚æ•°
            
        elif model_name == 'CrossLinear':
            self.features = 'M'  # CrossLinearéœ€è¦featureså‚æ•°
            self.patch_len = 16  # CrossLinearéœ€è¦patchç›¸å…³å‚æ•°
            self.alpha = 0.5
            self.beta = 0.5
            
        elif model_name == 'TimesNet':
            self.top_k = 5  # TimesNetéœ€è¦çš„å‚æ•°
            self.num_kernels = 6
            
        elif model_name == 'Mamba':
            # Mambaéœ€è¦çš„ç‰¹æ®Šå‚æ•°å·²ç»åœ¨åŸºç¡€é…ç½®ä¸­è®¾ç½®äº†
            pass
        
        # FIRMSæ•°æ®å½’ä¸€åŒ–å‚æ•°
        self.normalize_firms = True
        self.firms_normalization_method = 'divide_by_100'
        self.binarization_threshold = 0.0
        self.firms_min = int(DATA_CONFIG['firms_min'])
        self.firms_max = int(DATA_CONFIG['firms_max'])
        
        # Focal Losså‚æ•°  
        self.focal_alpha = float(TRAINING_CONFIG['focal_alpha'])
        self.focal_gamma = float(TRAINING_CONFIG['focal_gamma'])
        
        # å¤šä»»åŠ¡å­¦ä¹ å‚æ•°
        self.firms_weight = float(MULTITASK_CONFIG['firms_weight'])
        self.other_drivers_weight = float(MULTITASK_CONFIG['other_drivers_weight'])
        self.ignore_zero_values = MULTITASK_CONFIG['ignore_zero_values']
        self.loss_function = MULTITASK_CONFIG['loss_function']
        self.loss_type = MULTITASK_CONFIG['loss_type']  # æ–°å¢ï¼šæŸå¤±å‡½æ•°ç±»å‹é€‰æ‹©
        
        # æ•°æ®é›†åˆ’åˆ†
        self.train_years = DATA_CONFIG['train_years']
        self.val_years = DATA_CONFIG['val_years']
        self.test_years = DATA_CONFIG['test_years']
        
        # ğŸ”¥ æ–°å¢ï¼šåŠ¨æ€æ›´æ–°æ¨¡å‹é€šé“é…ç½®
        self.update_model_channels()
    
    # ğŸ”¥ æ–°å¢ï¼šåŠ¨æ€è®¡ç®—è¾“å…¥é€šé“æ•°
    def calculate_input_channels(self):
        """
        æ ¹æ®é…ç½®åŠ¨æ€è®¡ç®—è¾“å…¥é€šé“æ•°
        åŸºç¡€é€šé“æ•° + ä½ç½®ç‰¹å¾é€šé“æ•° + æ°”è±¡æ•°æ®é€šé“æ•°
        """
        base_channels = 39  # åŸºç¡€é€šé“æ•°
        additional_channels = 0
        
        # ä½ç½®ä¿¡æ¯ç‰¹å¾ (+1 é€šé“) - ä¼˜å…ˆä½¿ç”¨configå¯¹è±¡å±æ€§ï¼Œå¦åˆ™ä½¿ç”¨å…¨å±€é…ç½®
        enable_position = getattr(self, 'enable_position_features', DATA_CONFIG['enable_position_features'])
        if enable_position:
            additional_channels += 1
            
        # æœªæ¥æ°”è±¡æ•°æ®ç‰¹å¾ - ä¼˜å…ˆä½¿ç”¨configå¯¹è±¡å±æ€§ï¼Œå¦åˆ™ä½¿ç”¨å…¨å±€é…ç½®
        enable_weather = getattr(self, 'enable_future_weather', DATA_CONFIG['enable_future_weather'])
        if enable_weather:
            weather_channels = getattr(self, 'weather_channels', DATA_CONFIG['weather_channels'])
            additional_channels += len(weather_channels)
            
        return base_channels + additional_channels
    
    def update_model_channels(self):
        """æ›´æ–°æ¨¡å‹çš„è¾“å…¥/è¾“å‡ºé€šé“é…ç½®"""
        # åŠ¨æ€è®¡ç®—è¾“å…¥é€šé“æ•°
        dynamic_enc_in = self.calculate_input_channels()
        
        # æ›´æ–°ç¼–ç å™¨è¾“å…¥é€šé“æ•°
        self.enc_in = dynamic_enc_in
        
        # è§£ç å™¨è¾“å…¥é€šé“æ•°é€šå¸¸ä¸ç¼–ç å™¨ä¸€è‡´
        self.dec_in = dynamic_enc_in
        
        # è¾“å‡ºé€šé“æ•°ä¿æŒä¸º39ï¼ˆé¢„æµ‹æ‰€æœ‰åŸå§‹é€šé“ï¼‰
        self.c_out = 39
        
        # æ‰“å°é€šé“ä¿¡æ¯ä»¥ä¾¿è°ƒè¯• - ä½¿ç”¨configå¯¹è±¡å±æ€§è€Œä¸æ˜¯å…¨å±€é…ç½®
        features_info = []
        enable_position = getattr(self, 'enable_position_features', DATA_CONFIG['enable_position_features'])
        enable_weather = getattr(self, 'enable_future_weather', DATA_CONFIG['enable_future_weather'])
        
        if enable_position:
            features_info.append("ä½ç½®ä¿¡æ¯(+1)")
        if enable_weather:
            weather_channels = getattr(self, 'weather_channels', DATA_CONFIG['weather_channels'])
            features_info.append(f"æ°”è±¡æ•°æ®(+{len(weather_channels)})")
        
        if features_info:
            print(f"ğŸ”§ {self.model_name} åŠ¨æ€é€šé“é…ç½®: {self.enc_in}è¾“å…¥ -> {self.c_out}è¾“å‡º (é¢å¤–ç‰¹å¾: {', '.join(features_info)})")
        else:
            print(f"ğŸ”§ {self.model_name} æ ‡å‡†é€šé“é…ç½®: {self.enc_in}è¾“å…¥ -> {self.c_out}è¾“å‡º")

class FocalLoss(nn.Module):
    """Focal Loss for addressing class imbalance"""
    def __init__(self, alpha=0.25, gamma=2.0, reduction='mean'):
        super(FocalLoss, self).__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.reduction = reduction
        
    def forward(self, inputs, targets):
        p = torch.sigmoid(inputs)
        ce_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')
        p_t = p * targets + (1 - p) * (1 - targets)
        alpha_t = self.alpha * targets + (1 - self.alpha) * (1 - targets)
        focal_loss = alpha_t * (1 - p_t) ** self.gamma * ce_loss
        
        if self.reduction == 'mean':
            return focal_loss.mean()
        elif self.reduction == 'sum':
            return focal_loss.sum()
        else:
            return focal_loss

class MultiTaskFocalLoss(nn.Module):
    """
    å¤šä»»åŠ¡Focal Lossï¼š
    - å¯¹FIRMSé€šé“ï¼ˆç¬¬0é€šé“ï¼‰ä½¿ç”¨Focal Lossè¿›è¡ŒäºŒåˆ†ç±»
    - å¯¹å…¶ä»–é©±åŠ¨å› ç´ ä½¿ç”¨å›å½’æŸå¤±ï¼ˆMSE/Huber/MAEï¼‰
    - æ”¯æŒæƒé‡è°ƒèŠ‚å’Œå¿½ç•¥0å€¼åŠŸèƒ½
    """
    def __init__(self, firms_weight=1.0, other_drivers_weight=0.1, 
                 focal_alpha=0.25, focal_gamma=2.0,
                 ignore_zero_values=True, regression_loss='mse'):
        super(MultiTaskFocalLoss, self).__init__()
        self.firms_weight = firms_weight
        self.other_drivers_weight = other_drivers_weight
        self.ignore_zero_values = ignore_zero_values
        
        # FIRMSçš„Focal Loss
        # self.focal_loss = FocalLoss(alpha=focal_alpha, gamma=focal_gamma, reduction='mean')
        self.focal_loss = nn.BCELoss()
        # å…¶ä»–é©±åŠ¨å› ç´ çš„å›å½’æŸå¤±å‡½æ•°
        if regression_loss == 'huber':
            self.regression_loss_fn = nn.HuberLoss(reduction='none')
        elif regression_loss == 'mse':
            self.regression_loss_fn = nn.MSELoss(reduction='none')
        elif regression_loss == 'mae':
            self.regression_loss_fn = nn.L1Loss(reduction='none')
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„å›å½’æŸå¤±å‡½æ•°ç±»å‹: {regression_loss}")
        
        self.regression_loss_type = regression_loss
    
    def forward(self, predictions, targets):
        """
        è®¡ç®—å¤šä»»åŠ¡æŸå¤±
        
        Args:
            predictions: (B, T, C) æ¨¡å‹é¢„æµ‹ç»“æœï¼ŒCå¯èƒ½å¤§äº39ï¼ˆå¦‚æœæœ‰é¢å¤–ç‰¹å¾ï¼‰
            targets: (B, T, 39) çœŸå®æ ‡ç­¾ï¼Œå§‹ç»ˆæ˜¯39ä¸ªé€šé“
            
        Returns:
            total_loss: æ€»æŸå¤±
            loss_components: å„ç»„ä»¶æŸå¤±å­—å…¸
        """
        batch_size, seq_len, pred_channels = predictions.shape
        _, _, target_channels = targets.shape
        
        # ğŸ”¥ å…³é”®ä¿®å¤ï¼šå¦‚æœé¢„æµ‹é€šé“æ•°å¤§äºç›®æ ‡é€šé“æ•°ï¼Œåªå–å‰target_channelsä¸ªé€šé“
        # è¿™æ˜¯å› ä¸ºé¢å¤–çš„é€šé“ï¼ˆå¦‚æ°”è±¡æ•°æ®ï¼‰å·²ç»ä½œä¸ºè¾“å…¥ç‰¹å¾ï¼Œä¸åº”è¯¥è®¡ç®—æŸå¤±
        if pred_channels > target_channels:
            predictions = predictions[:, :, :target_channels]
        #   print(f"ğŸ”§ æŸå¤±è®¡ç®—ï¼šé¢„æµ‹é€šé“æ•°({pred_channels}) > ç›®æ ‡é€šé“æ•°({target_channels})ï¼Œåªè®¡ç®—å‰{target_channels}ä¸ªé€šé“çš„æŸå¤±")
        
        # åˆ†ç¦»FIRMSå’Œå…¶ä»–é©±åŠ¨å› ç´ 
        firms_pred = predictions[:, :, 0]      # (B, T) - FIRMSé€šé“ç”¨äºäºŒåˆ†ç±»
        firms_target = targets[:, :, 0]        # (B, T)
        other_pred = predictions[:, :, 1:]     # (B, T, 38) - å…¶ä»–é€šé“ç”¨äºå›å½’
        other_target = targets[:, :, 1:]       # (B, T, 38)
        
        # 1. è®¡ç®—FIRMSçš„Focal Lossï¼ˆäºŒåˆ†ç±»ï¼‰
        # å°†FIRMSç›®æ ‡è½¬æ¢ä¸ºäºŒåˆ†ç±»æ ‡ç­¾ï¼ˆ>0ä¸º1ï¼Œ=0ä¸º0ï¼‰
        firms_binary_target = (firms_target > 0).float()
        firms_pred = torch.sigmoid(firms_pred)  # ä½¿ç”¨focal lossçš„æ—¶å€™ä¸éœ€è¦sigmoidï¼Œå› ä¸ºsigmoidå·²ç»å†…ç½®åˆ°focal lossä¸­
        firms_loss = self.focal_loss(firms_pred, firms_binary_target) * self.firms_weight
        
        # 2. è®¡ç®—å…¶ä»–é©±åŠ¨å› ç´ çš„å›å½’æŸå¤±
        other_loss = self.regression_loss_fn(other_pred, other_target)  # (B, T, 38)
        
        if self.ignore_zero_values:
            # åˆ›å»ºéé›¶å€¼æ©ç ï¼Œå¿½ç•¥0å€¼
            non_zero_mask = (other_target != 0.0).float()  # (B, T, 38)
            
            # è®¡ç®—æœ‰æ•ˆæ ·æœ¬æ•°
            valid_samples = non_zero_mask.sum()
            
            if valid_samples > 0:
                # åªå¯¹éé›¶å€¼è®¡ç®—æŸå¤±
                masked_loss = other_loss * non_zero_mask
                other_loss = masked_loss.sum() / valid_samples
            else:
                # å¦‚æœæ²¡æœ‰æœ‰æ•ˆæ ·æœ¬ï¼ŒæŸå¤±ä¸º0
                other_loss = torch.tensor(0.0, device=predictions.device)
        else:
            # ä¸å¿½ç•¥0å€¼ï¼Œç›´æ¥è®¡ç®—å¹³å‡æŸå¤±
            other_loss = other_loss.mean()
        
        other_loss = other_loss * self.other_drivers_weight
        
        # æ€»æŸå¤±
        total_loss = firms_loss + other_loss
        
        # è¿”å›æŸå¤±ç»„ä»¶ä¿¡æ¯
        loss_components = {
            'total_loss': total_loss.item(),
            'firms_loss': firms_loss.item(),
            'other_drivers_loss': other_loss.item(),
            'firms_weight': self.firms_weight,
            'other_drivers_weight': self.other_drivers_weight,
            # 'focal_alpha': self.focal_loss.alpha,  # ä½¿ç”¨BCELossçš„æ—¶å€™ä¸éœ€è¦focal_alphaå’Œfocal_gamma
            # 'focal_gamma': self.focal_loss.gamma,
            'regression_loss_type': self.regression_loss_type,
            'loss_type': 'focal'  # æ–°å¢ï¼šæŸå¤±å‡½æ•°ç±»å‹æ ‡è¯†
        }
        # print(firms_loss, other_loss)
        return firms_loss, loss_components  # total_loss, loss_components

class MultiTaskKLDivLoss(nn.Module):
    """
    å¤šä»»åŠ¡KLæ•£åº¦Lossï¼š
    - å¯¹FIRMSé€šé“ï¼ˆç¬¬0é€šé“ï¼‰ä½¿ç”¨KLæ•£åº¦è¿›è¡Œåˆ†ç±»
    - å¯¹å…¶ä»–é©±åŠ¨å› ç´ ä½¿ç”¨KLæ•£åº¦è¿›è¡Œå›å½’
    - æ”¯æŒæƒé‡è°ƒèŠ‚å’Œå¿½ç•¥0å€¼åŠŸèƒ½
    """
    def __init__(self, firms_weight=1.0, other_drivers_weight=0.1, 
                 ignore_zero_values=True, temperature=1.0, epsilon=1e-8):
        super(MultiTaskKLDivLoss, self).__init__()
        self.firms_weight = firms_weight
        self.other_drivers_weight = other_drivers_weight
        self.ignore_zero_values = ignore_zero_values
        self.temperature = temperature  # æ¸©åº¦å‚æ•°ï¼Œç”¨äºæ§åˆ¶åˆ†å¸ƒçš„å¹³æ»‘åº¦
        self.epsilon = epsilon  # é˜²æ­¢æ•°å€¼ä¸ç¨³å®šçš„å°å¸¸æ•°
        
        # KLæ•£åº¦æŸå¤±å‡½æ•°ï¼ˆreduction='none'ä»¥ä¾¿æ‰‹åŠ¨å¤„ç†ï¼‰
        self.kldiv_loss = nn.KLDivLoss(reduction='none')
    
    def _to_probability_distribution(self, x, is_classification=False):
        """
        å°†è¾“å…¥è½¬æ¢ä¸ºæ¦‚ç‡åˆ†å¸ƒ
        
        Args:
            x: è¾“å…¥å¼ é‡
            is_classification: æ˜¯å¦ä¸ºåˆ†ç±»ä»»åŠ¡ï¼ˆFIRMSé€šé“ï¼‰
            
        Returns:
            æ¦‚ç‡åˆ†å¸ƒå¼ é‡
        """
        if is_classification:
            # å¯¹äºåˆ†ç±»ä»»åŠ¡ï¼Œä½¿ç”¨sigmoid+å½’ä¸€åŒ–
            # x shape: (...,) æˆ– (..., 1)
            if x.dim() > 0 and x.shape[-1] == 1:
                x = x.squeeze(-1)  # ç§»é™¤æœ€åä¸€ç»´å¦‚æœæ˜¯1
            
            prob = torch.sigmoid(x / self.temperature)
            # åˆ›å»ºäºŒé¡¹åˆ†å¸ƒï¼š[1-p, p]
            prob_neg = 1 - prob
            prob_dist = torch.stack([prob_neg, prob], dim=-1)  # (..., 2)
            # å½’ä¸€åŒ–ç¡®ä¿æ˜¯æ¦‚ç‡åˆ†å¸ƒ
            prob_dist = prob_dist / (prob_dist.sum(dim=-1, keepdim=True) + self.epsilon)
        else:
            # å¯¹äºå›å½’ä»»åŠ¡ï¼Œå°†å€¼è½¬æ¢ä¸ºæ­£å€¼ç„¶åå½’ä¸€åŒ–
            # ä½¿ç”¨softplusç¡®ä¿æ­£å€¼ï¼šsoftplus(x) = log(1 + exp(x))
            positive_vals = F.softplus(x / self.temperature)
            # å½’ä¸€åŒ–ä¸ºæ¦‚ç‡åˆ†å¸ƒ
            prob_dist = positive_vals / (positive_vals.sum(dim=-1, keepdim=True) + self.epsilon)
        
        # æ·»åŠ å°å¸¸æ•°é˜²æ­¢log(0)
        prob_dist = prob_dist + self.epsilon
        prob_dist = prob_dist / prob_dist.sum(dim=-1, keepdim=True)
        
        return prob_dist
    
    def forward(self, predictions, targets):
        """
        è®¡ç®—å¤šä»»åŠ¡KLæ•£åº¦æŸå¤±
        
        Args:
            predictions: (B, T, C) æ¨¡å‹é¢„æµ‹ç»“æœï¼ŒCå¯èƒ½å¤§äº39ï¼ˆå¦‚æœæœ‰é¢å¤–ç‰¹å¾ï¼‰
            targets: (B, T, 39) çœŸå®æ ‡ç­¾ï¼Œå§‹ç»ˆæ˜¯39ä¸ªé€šé“
            
        Returns:
            total_loss: æ€»æŸå¤±
            loss_components: å„ç»„ä»¶æŸå¤±å­—å…¸
        """
        batch_size, seq_len, pred_channels = predictions.shape
        _, _, target_channels = targets.shape
        
        # ğŸ”¥ å…³é”®ä¿®å¤ï¼šå¦‚æœé¢„æµ‹é€šé“æ•°å¤§äºç›®æ ‡é€šé“æ•°ï¼Œåªå–å‰target_channelsä¸ªé€šé“
        # è¿™æ˜¯å› ä¸ºé¢å¤–çš„é€šé“ï¼ˆå¦‚æ°”è±¡æ•°æ®ï¼‰å·²ç»ä½œä¸ºè¾“å…¥ç‰¹å¾ï¼Œä¸åº”è¯¥è®¡ç®—æŸå¤±
        if pred_channels > target_channels:
            predictions = predictions[:, :, :target_channels]
            print(f"ğŸ”§ KLæ•£åº¦æŸå¤±è®¡ç®—ï¼šé¢„æµ‹é€šé“æ•°({pred_channels}) > ç›®æ ‡é€šé“æ•°({target_channels})ï¼Œåªè®¡ç®—å‰{target_channels}ä¸ªé€šé“çš„æŸå¤±")
        
        # åˆ†ç¦»FIRMSå’Œå…¶ä»–é©±åŠ¨å› ç´ 
        firms_pred = predictions[:, :, 0]      # (B, T) - FIRMSé€šé“
        firms_target = targets[:, :, 0]        # (B, T)
        other_pred = predictions[:, :, 1:]     # (B, T, 38) - å…¶ä»–é€šé“
        other_target = targets[:, :, 1:]       # (B, T, 38)
        
        # 1. è®¡ç®—FIRMSçš„KLæ•£åº¦æŸå¤±ï¼ˆåˆ†ç±»ä»»åŠ¡ï¼‰
        # å°†FIRMSç›®æ ‡è½¬æ¢ä¸ºäºŒåˆ†ç±»æ ‡ç­¾ï¼ˆ>0ä¸º1ï¼Œ=0ä¸º0ï¼‰
        firms_binary_target = (firms_target > 0).float()
        
        # è½¬æ¢ä¸ºæ¦‚ç‡åˆ†å¸ƒ
        firms_pred_dist = self._to_probability_distribution(firms_pred, is_classification=True)  # (B, T, 2)
        firms_target_dist = self._to_probability_distribution(firms_binary_target, is_classification=True)  # (B, T, 2)
        
        # è®¡ç®—KLæ•£åº¦ï¼šKL(target || pred)
        firms_kl = self.kldiv_loss(firms_pred_dist.log(), firms_target_dist)  # (B, T, 2)
        firms_loss = firms_kl.sum(dim=-1).mean() * self.firms_weight  # å¯¹åˆ†å¸ƒç»´åº¦æ±‚å’Œï¼Œç„¶åå¹³å‡
        
        # 2. è®¡ç®—å…¶ä»–é©±åŠ¨å› ç´ çš„KLæ•£åº¦æŸå¤±ï¼ˆå›å½’ä»»åŠ¡ï¼‰
        # è½¬æ¢ä¸ºæ¦‚ç‡åˆ†å¸ƒ
        other_pred_dist = self._to_probability_distribution(other_pred, is_classification=False)  # (B, T, 38)
        other_target_dist = self._to_probability_distribution(other_target, is_classification=False)  # (B, T, 38)
        
        # è®¡ç®—KLæ•£åº¦
        other_kl = self.kldiv_loss(other_pred_dist.log(), other_target_dist)  # (B, T, 38)
        
        if self.ignore_zero_values:
            # åˆ›å»ºéé›¶å€¼æ©ç ï¼Œå¿½ç•¥0å€¼
            non_zero_mask = (other_target != 0.0).float()  # (B, T, 38)
            
            # è®¡ç®—æœ‰æ•ˆæ ·æœ¬æ•°
            valid_samples = non_zero_mask.sum()
            
            if valid_samples > 0:
                # åªå¯¹éé›¶å€¼è®¡ç®—æŸå¤±
                masked_kl = other_kl * non_zero_mask
                other_loss = masked_kl.sum() / valid_samples
            else:
                # å¦‚æœæ²¡æœ‰æœ‰æ•ˆæ ·æœ¬ï¼ŒæŸå¤±ä¸º0
                other_loss = torch.tensor(0.0, device=predictions.device)
        else:
            # ä¸å¿½ç•¥0å€¼ï¼Œç›´æ¥è®¡ç®—å¹³å‡æŸå¤±
            other_loss = other_kl.mean()
        
        other_loss = other_loss * self.other_drivers_weight
        
        # æ€»æŸå¤±
        total_loss = firms_loss + other_loss
        
        # è¿”å›æŸå¤±ç»„ä»¶ä¿¡æ¯
        loss_components = {
            'total_loss': total_loss.item(),
            'firms_loss': firms_loss.item(),
            'other_drivers_loss': other_loss.item(),
            'firms_weight': self.firms_weight,
            'other_drivers_weight': self.other_drivers_weight,
            'temperature': self.temperature,
            'loss_type': 'kldiv'
        }
        
        return total_loss, loss_components

class MultiMetricEarlyStopping:
    """
    å¤šæŒ‡æ ‡Early Stoppingï¼šåŒæ—¶ç›‘æ§F1ã€Recallã€PR-AUC
    ä»»ä½•ä¸€ä¸ªæŒ‡æ ‡æå‡éƒ½ä¼šé‡ç½®è®¡æ•°å™¨
    """
    def __init__(self, patience=7, min_delta=0.0001, restore_best_weights=True):
        self.patience = patience
        self.min_delta = min_delta
        self.restore_best_weights = restore_best_weights
        self.counter = 0
        self.best_metrics = {
            'f1': 0.0,
            'recall': 0.0,
            'pr_auc': 0.0,
            'mae': float('inf')  # MAEè¶Šå°è¶Šå¥½
        }
        self.best_weights = None
        self.should_stop = False
    
    def __call__(self, metrics, model):
        """
        æ£€æŸ¥æ˜¯å¦åº”è¯¥åœæ­¢è®­ç»ƒ
        Args:
            metrics: dictåŒ…å«'f1', 'recall', 'pr_auc', 'mae'
            model: æ¨¡å‹å®ä¾‹
        Returns:
            bool: æ˜¯å¦åº”è¯¥åœæ­¢è®­ç»ƒ
        """
        f1_improved = metrics['f1'] > (self.best_metrics['f1'] + self.min_delta)
        recall_improved = metrics['recall'] > (self.best_metrics['recall'] + self.min_delta)
        pr_auc_improved = metrics['pr_auc'] > (self.best_metrics['pr_auc'] + self.min_delta)
        mae_improved = metrics['mae'] < (self.best_metrics['mae'] - self.min_delta)  # MAEè¶Šå°è¶Šå¥½
        
        # ä»»ä½•ä¸€ä¸ªæŒ‡æ ‡æå‡å°±é‡ç½®è®¡æ•°å™¨
        if f1_improved or recall_improved or pr_auc_improved or mae_improved:
            # æ›´æ–°æœ€ä½³æŒ‡æ ‡
            if f1_improved:
                self.best_metrics['f1'] = metrics['f1']
            if recall_improved:
                self.best_metrics['recall'] = metrics['recall']
            if pr_auc_improved:
                self.best_metrics['pr_auc'] = metrics['pr_auc']
            if mae_improved:
                self.best_metrics['mae'] = metrics['mae']
                
            self.counter = 0
            if self.restore_best_weights:
                self.save_checkpoint(model)
            print(f"ğŸ“ˆ æŒ‡æ ‡æå‡! F1: {metrics['f1']:.4f}, Recall: {metrics['recall']:.4f}, PR-AUC: {metrics['pr_auc']:.4f}, MAE: {metrics['mae']:.6f}")
        else:
            self.counter += 1
            print(f"â³ æ— æ”¹å–„ ({self.counter}/{self.patience}): F1: {metrics['f1']:.4f}, Recall: {metrics['recall']:.4f}, PR-AUC: {metrics['pr_auc']:.4f}, MAE: {metrics['mae']:.6f}")
        
        if self.counter >= self.patience:
            self.should_stop = True
            if self.restore_best_weights and self.best_weights is not None:
                model.load_state_dict(self.best_weights)
                print("ğŸ”„ æ¢å¤æœ€ä½³æƒé‡")
        
        return self.should_stop
    
    def save_checkpoint(self, model):
        """ä¿å­˜æœ€ä½³æƒé‡"""
        self.best_weights = model.state_dict().copy()

class MultiTaskLoss(nn.Module):
    """å¤šä»»åŠ¡æŸå¤±å‡½æ•°ï¼Œæ”¯æŒå¯¹ä¸åŒé€šé“çš„é¢„æµ‹ç»“æœè¿›è¡ŒåŠ æƒæŸå¤±è®¡ç®—"""
    
    def __init__(self, firms_weight=1.0, other_drivers_weight=0.1, 
                 ignore_zero_values=True, loss_function='huber'):
        super(MultiTaskLoss, self).__init__()
        self.firms_weight = firms_weight
        self.other_drivers_weight = other_drivers_weight
        self.ignore_zero_values = ignore_zero_values
        
        # é€‰æ‹©æŸå¤±å‡½æ•°
        if loss_function == 'huber':
            self.loss_fn = nn.HuberLoss(reduction='none')
        elif loss_function == 'mse':
            self.loss_fn = nn.MSELoss(reduction='none')
        elif loss_function == 'mae':
            self.loss_fn = nn.L1Loss(reduction='none')
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æŸå¤±å‡½æ•°ç±»å‹: {loss_function}")
    
    def forward(self, predictions, targets):
        """
        è®¡ç®—å¤šä»»åŠ¡æŸå¤±
        
        Args:
            predictions: (B, T, C) æ¨¡å‹é¢„æµ‹ç»“æœï¼ŒCå¯èƒ½å¤§äº39ï¼ˆå¦‚æœæœ‰é¢å¤–ç‰¹å¾ï¼‰
            targets: (B, T, 39) çœŸå®æ ‡ç­¾ï¼Œå§‹ç»ˆæ˜¯39ä¸ªé€šé“
            
        Returns:
            total_loss: æ€»æŸå¤±
            loss_components: å„ç»„ä»¶æŸå¤±å­—å…¸
        """
        batch_size, seq_len, pred_channels = predictions.shape
        _, _, target_channels = targets.shape
        
        # ğŸ”¥ å…³é”®ä¿®å¤ï¼šå¦‚æœé¢„æµ‹é€šé“æ•°å¤§äºç›®æ ‡é€šé“æ•°ï¼Œåªå–å‰target_channelsä¸ªé€šé“
        # è¿™æ˜¯å› ä¸ºé¢å¤–çš„é€šé“ï¼ˆå¦‚æ°”è±¡æ•°æ®ï¼‰å·²ç»ä½œä¸ºè¾“å…¥ç‰¹å¾ï¼Œä¸åº”è¯¥è®¡ç®—æŸå¤±
        if pred_channels > target_channels:
            predictions = predictions[:, :, :target_channels]
            print(f"ğŸ”§ å¤šä»»åŠ¡æŸå¤±è®¡ç®—ï¼šé¢„æµ‹é€šé“æ•°({pred_channels}) > ç›®æ ‡é€šé“æ•°({target_channels})ï¼Œåªè®¡ç®—å‰{target_channels}ä¸ªé€šé“çš„æŸå¤±")
        
        # åˆ†ç¦»FIRMSå’Œå…¶ä»–é©±åŠ¨å› ç´ 
        firms_pred = predictions[:, :, 0:1]  # (B, T, 1)
        firms_target = targets[:, :, 0:1]    # (B, T, 1)
        other_pred = predictions[:, :, 1:]   # (B, T, 38)
        other_target = targets[:, :, 1:]     # (B, T, 38)
        
        # è®¡ç®—FIRMSæŸå¤±
        firms_loss = self.loss_fn(firms_pred, firms_target)  # (B, T, 1)
        firms_loss = firms_loss.mean() * self.firms_weight
        
        # è®¡ç®—å…¶ä»–é©±åŠ¨å› ç´ æŸå¤±
        other_loss = self.loss_fn(other_pred, other_target)  # (B, T, 38)
        
        if self.ignore_zero_values:
            # åˆ›å»ºéé›¶å€¼æ©ç ï¼Œå¿½ç•¥0å€¼
            non_zero_mask = (other_target != 0.0).float()  # (B, T, 38)
            
            # è®¡ç®—æœ‰æ•ˆæ ·æœ¬æ•°
            valid_samples = non_zero_mask.sum()
            
            if valid_samples > 0:
                # åªå¯¹éé›¶å€¼è®¡ç®—æŸå¤±
                masked_loss = other_loss * non_zero_mask
                other_loss = masked_loss.sum() / valid_samples
            else:
                # å¦‚æœæ²¡æœ‰æœ‰æ•ˆæ ·æœ¬ï¼ŒæŸå¤±ä¸º0
                other_loss = torch.tensor(0.0, device=predictions.device)
        else:
            # ä¸å¿½ç•¥0å€¼ï¼Œç›´æ¥è®¡ç®—å¹³å‡æŸå¤±
            other_loss = other_loss.mean()
        
        other_loss = other_loss * self.other_drivers_weight
        
        # æ€»æŸå¤±
        total_loss = firms_loss + other_loss
        
        # è¿”å›æŸå¤±ç»„ä»¶ä¿¡æ¯
        loss_components = {
            'total_loss': total_loss.item(),
            'firms_loss': firms_loss.item(),
            'other_drivers_loss': other_loss.item(),
            'firms_weight': self.firms_weight,
            'other_drivers_weight': self.other_drivers_weight,
            'loss_type': 'multitask'  # æ–°å¢ï¼šæŸå¤±å‡½æ•°ç±»å‹æ ‡è¯†
        }
        # print(firms_loss, other_loss)
        return total_loss, loss_components

# =============================================================================
# è¿›åº¦æ˜¾ç¤ºå·¥å…·å‡½æ•°
# =============================================================================

class SimpleProgressTracker:
    """ç®€åŒ–çš„è¿›åº¦è·Ÿè¸ªå™¨ï¼Œæ¨¡ä»¿tqdmé»˜è®¤æ•ˆæœä½†å»æ‰è¿›åº¦æ¡"""
    def __init__(self):
        self.start_time = None
        
    def update(self, current, total, prefix="Progress", clear_on_complete=True):
        """
        æ›´æ–°è¿›åº¦æ˜¾ç¤º - tqdmé£æ ¼ä½†æ— è¿›åº¦æ¡
        """
        if self.start_time is None:
            self.start_time = time.time()
            
        current_time = time.time()
        elapsed_time = current_time - self.start_time
        
        # è®¡ç®—é€Ÿåº¦ (items/second)
        speed = current / elapsed_time if elapsed_time > 0 else 0
        
        # è®¡ç®—ç™¾åˆ†æ¯”
        percent = int((current / total) * 100)
        
        # tqdmé£æ ¼çš„æ˜¾ç¤ºæ ¼å¼
        if current == total:
            # å®Œæˆæ—¶çš„æ ¼å¼
            progress_text = f"\r{prefix}: {percent:3d}%|{current}/{total} [{self._format_time(elapsed_time)}, {speed:.2f}it/s]"
        else:
            # è¿›è¡Œä¸­çš„æ ¼å¼ï¼Œè®¡ç®—é¢„ä¼°å‰©ä½™æ—¶é—´
            if speed > 0:
                remaining_time = (total - current) / speed
                progress_text = f"\r{prefix}: {percent:3d}%|{current}/{total} [{self._format_time(elapsed_time)}<{self._format_time(remaining_time)}, {speed:.2f}it/s]"
            else:
                # å¦‚æœé€Ÿåº¦ä¸º0ï¼Œä½¿ç”¨ç®€åŒ–æ ¼å¼
                progress_text = f"\r{prefix}: {percent:3d}%|{current}/{total} [{self._format_time(elapsed_time)}<?, ?it/s]"
        
        print(progress_text, end='', flush=True)
        
        # å®Œæˆåå¤„ç†
        if current == total:
            if clear_on_complete:
                # æ¸…é™¤è¿›åº¦æ¡
                print('\r' + ' ' * len(progress_text) + '\r', end='', flush=True)
            else:
                print()  # ä¿ç•™æœ€ç»ˆçŠ¶æ€å¹¶æ¢è¡Œ
    
    def _format_time(self, seconds):
        """æ ¼å¼åŒ–æ—¶é—´æ˜¾ç¤º - tqdmé£æ ¼"""
        if seconds < 0:
            return "00s"
        elif seconds < 60:
            return f"{int(seconds):02d}s"
        elif seconds < 3600:
            minutes = int(seconds // 60)
            secs = int(seconds % 60)
            return f"{minutes:02d}:{secs:02d}"
        else:
            hours = int(seconds // 3600)
            minutes = int((seconds % 3600) // 60)
            return f"{hours}:{minutes:02d}:00"

def print_dynamic_progress(current, total, prefix="Progress", show_percent=True):
    """
    å…¼å®¹æ€§å‡½æ•° - ä¿æŒç®€å•çš„åŠ¨æ€è¿›åº¦æ˜¾ç¤º
    """
    if show_percent:
        percent = (current / total) * 100
        progress_text = f"\r{prefix}: {current}/{total} ({percent:.1f}%)"
    else:
        progress_text = f"\r{prefix}: {current}/{total}"
    
    print(progress_text, end='', flush=True)
    
    # å®Œæˆåæ¸…é™¤è¿›åº¦æ¡
    if current == total:
        print('\r' + ' ' * len(progress_text) + '\r', end='', flush=True)

def save_epoch_metrics_to_log(epoch_metrics, log_file, model_name, model_type):
    """
    å°†æ¯ä¸ªepochçš„è®­ç»ƒå’ŒéªŒè¯æŒ‡æ ‡ä¿å­˜åˆ°æ—¥å¿—æ–‡ä»¶
    """
    try:
        with open(log_file, 'a', encoding='utf-8') as f:
            f.write(f"\n{'='*80}\n")
            f.write(f"è¯¦ç»†è®­ç»ƒæ—¥å¿— - {model_name} ({model_type})\n")
            f.write(f"{'='*80}\n")
            f.write(f"è®°å½•æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            # å†™å…¥è¡¨å¤´
            f.write(f"{'Epoch':<6} {'Train_Loss':<11} {'Train_P':<8} {'Train_R':<8} {'Train_F1':<9} {'Train_PRAUC':<11} {'Train_MSE':<10} {'Train_MAE':<10} ")
            f.write(f"{'Val_Loss':<9} {'Val_P':<6} {'Val_R':<6} {'Val_F1':<7} {'Val_PRAUC':<9} {'Val_MSE':<8} {'Val_MAE':<8} {'LR':<10}\n")
            f.write("-" * 150 + "\n")
            
            # å†™å…¥æ¯ä¸ªepochçš„æ•°æ®
            for metrics in epoch_metrics:
                f.write(f"{metrics['epoch']:<6} ")
                f.write(f"{metrics['train_loss']:<11.6f} ")
                f.write(f"{metrics['train_precision']:<8.4f} ")
                f.write(f"{metrics['train_recall']:<8.4f} ")
                f.write(f"{metrics['train_f1']:<9.4f} ")
                f.write(f"{metrics['train_pr_auc']:<11.4f} ")
                f.write(f"{metrics['train_mse']:<10.6f} ")
                f.write(f"{metrics['train_mae']:<10.6f} ")
                f.write(f"{metrics['val_loss']:<9.6f} ")
                f.write(f"{metrics['val_precision']:<6.4f} ")
                f.write(f"{metrics['val_recall']:<6.4f} ")
                f.write(f"{metrics['val_f1']:<7.4f} ")
                f.write(f"{metrics['val_pr_auc']:<9.4f} ")
                f.write(f"{metrics['val_mse']:<8.6f} ")
                f.write(f"{metrics['val_mae']:<8.6f} ")
                f.write(f"{metrics['learning_rate']:<10.2e}\n")
            
            f.write("\n")
            
        print(f"ğŸ“ è¯¦ç»†epochæ—¥å¿—å·²ä¿å­˜åˆ°: {log_file}")
        
    except Exception as e:
        print(f"âš ï¸ ä¿å­˜epochæ—¥å¿—å¤±è´¥: {e}")

def save_structured_results_to_csv(structured_results, model_type):
    """
    å°†ç»“æ„åŒ–æµ‹è¯•ç»“æœä¿å­˜ä¸ºåˆ†ç±»çš„CSVæ–‡ä»¶
    åˆ†åˆ«ä¿å­˜ï¼šbest_f1.csv, best_recall.csv, final_epoch.csv
    æ¯ä¸ªCSVåŒ…å«ï¼šModel, precision, recall, f1, pr_auc
    """
    if not structured_results:
        print("âš ï¸  æ²¡æœ‰ç»“æœå¯ä»¥ä¿å­˜")
        return
    
    # ç¡®å®šä¿å­˜ç›®å½•
    if model_type == 'standard':
        save_dir = STANDARD_MODEL_DIR
    else:
        save_dir = MODEL_10X_DIR
    
    # åˆ›å»ºä¿å­˜ç›®å½•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
    import os
    os.makedirs(save_dir, exist_ok=True)
    
    # è¦ä¿å­˜çš„3ç§æ¨¡å‹ç±»å‹
    model_categories = ['f1', 'recall', 'final_epoch']
    classification_metrics = ['precision', 'recall', 'f1', 'pr_auc']  # åªä¿å­˜åˆ†ç±»æŒ‡æ ‡
    
    saved_files = []
    
    for category in model_categories:
        # å‡†å¤‡CSVæ•°æ®
        csv_data = []
        columns = ['Model'] + classification_metrics
        
        # æ·»åŠ æ•°æ®è¡Œ
        for model_name, model_results in structured_results.items():
            if category in model_results and model_results[category]['precision'] is not None:
                row = [model_name]
                
                for metric_name in classification_metrics:
                    value = model_results[category][metric_name]
                    if value is not None:
                        row.append(f"{value:.6f}")
                    else:
                        row.append("N/A")
                
                csv_data.append(row)
        
        # ç”Ÿæˆæ–‡ä»¶åå¹¶ä¿å­˜
        if category == 'final_epoch':
            filename = f"final_epoch.csv"
        else:
            filename = f"best_{category}.csv"
        
        csv_filepath = os.path.join(save_dir, filename)
        
        if csv_data:  # åªåœ¨æœ‰æ•°æ®æ—¶ä¿å­˜
            df = pd.DataFrame(csv_data, columns=columns)
            df.to_csv(csv_filepath, index=False)
            saved_files.append(csv_filepath)
            
            print(f"ğŸ“Š {filename}: {len(csv_data)} ä¸ªæ¨¡å‹ç»“æœå·²ä¿å­˜")
        else:
            print(f"âš ï¸  {filename}: æ²¡æœ‰å¯ç”¨æ•°æ®")
    
    # æ€»ç»“ä¿å­˜æƒ…å†µ
    print(f"\nâœ… å…±ä¿å­˜ {len(saved_files)} ä¸ªCSVæ–‡ä»¶åˆ°: {save_dir}")
    for filepath in saved_files:
        print(f"   ğŸ“„ {os.path.basename(filepath)}")
    
    print(f"\nğŸ“‹ CSVæ–‡ä»¶ç»“æ„è¯´æ˜:")
    print(f"   best_f1.csv: F1æœ€ä½³æ¨¡å‹çš„æ€§èƒ½è¯„ä»·")
    print(f"   best_recall.csv: Recallæœ€ä½³æ¨¡å‹çš„æ€§èƒ½è¯„ä»·")
    print(f"   final_epoch.csv: æœ€åepochæ¨¡å‹çš„æ€§èƒ½è¯„ä»·")
    print(f"   æ¯ä¸ªæ–‡ä»¶åŒ…å«: Model, precision, recall, f1, pr_auc")

# =============================================================================
# æ ¸å¿ƒè®­ç»ƒå’Œæµ‹è¯•å‡½æ•°
# =============================================================================

def train_single_model(model_name, device, train_loader, val_loader, test_loader, firms_normalizer, model_type='standard', log_file=None):
    """è®­ç»ƒå•ä¸ªæ¨¡å‹"""
    print(f"\nğŸ”¥ è®­ç»ƒ{model_type}æ¨¡å‹: {model_name}")
    
    config = Config(model_name, model_type)
    
    # åˆ›å»ºè¯¦ç»†æ—¥å¿—è®°å½•å™¨
    epoch_metrics = []  # è®°å½•æ¯ä¸ªepochçš„æŒ‡æ ‡
    
    # åˆå§‹åŒ–wandb (å¦‚æœå¯ç”¨)
    wandb_run = None
    if TRAINING_CONFIG['use_wandb'] and WANDB_AVAILABLE:
        wandb_run = wandb.init(
            project="wildfire-forecasting-0708",
            name=f"{model_name}_{model_type}",
            config={
                "model_name": model_name,
                "model_type": model_type,
                "seq_len": config.seq_len,
                "pred_len": config.pred_len,
                "learning_rate": config.learning_rate,
                "batch_size": config.batch_size,
                "epochs": config.epochs,
                "focal_alpha": config.focal_alpha,
                "focal_gamma": config.focal_gamma,
                # å¤šä»»åŠ¡å­¦ä¹ é…ç½®
                "multitask_enabled": True,
                "firms_weight": config.firms_weight,
                "other_drivers_weight": config.other_drivers_weight,
                "ignore_zero_values": config.ignore_zero_values,
                "loss_function": config.loss_function,
            },
            reinit=True
        )
        print(f"âœ… WandBåˆå§‹åŒ–å®Œæˆ: {wandb_run.name}")
    
    # ä½¿ç”¨ç»Ÿä¸€é€‚é…å™¨
    from model_adapter_unified import UnifiedModelAdapter
    adapter = UnifiedModelAdapter(config)
    
    try:
        model, _ = load_model(model_name, config, model_type)
        model = model.to(device)
    except Exception as e:
        print(f"âŒ {model_type}æ¨¡å‹ {model_name} åŠ è½½å¤±è´¥: {e}")
        if wandb_run:
            wandb_run.finish()
        return None
    
    # ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
    optimizer = optim.AdamW(model.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay)
    
    # æ ¹æ®é…ç½®é€‰æ‹©æŸå¤±å‡½æ•°ç±»å‹
    if config.loss_type == 'focal':
        # ä½¿ç”¨å¤šä»»åŠ¡Focal Loss
        criterion = MultiTaskFocalLoss(
            firms_weight=config.firms_weight,
            other_drivers_weight=config.other_drivers_weight,
            focal_alpha=config.focal_alpha,
            focal_gamma=config.focal_gamma,
            ignore_zero_values=config.ignore_zero_values,
            regression_loss=config.loss_function  # 'mse', 'huber', 'mae'
        )
        
        print(f"ğŸ” å¤šä»»åŠ¡Focal Lossé…ç½®:")
        print(f"   FIRMSæƒé‡: {config.firms_weight}, å…¶ä»–é©±åŠ¨å› ç´ æƒé‡: {config.other_drivers_weight}")
        print(f"   Focal Î±: {config.focal_alpha}, Focal Î³: {config.focal_gamma}")
        print(f"   å›å½’æŸå¤±: {config.loss_function}, å¿½ç•¥0å€¼: {config.ignore_zero_values}")
        
    elif config.loss_type == 'kldiv':
        # ä½¿ç”¨å¤šä»»åŠ¡KLæ•£åº¦Loss
        criterion = MultiTaskKLDivLoss(
            firms_weight=config.firms_weight,
            other_drivers_weight=config.other_drivers_weight,
            ignore_zero_values=config.ignore_zero_values,
            temperature=1.0,  # å¯ä»¥åç»­æ·»åŠ åˆ°é…ç½®ä¸­
            epsilon=1e-8
        )
        
        print(f"ğŸ” å¤šä»»åŠ¡KLæ•£åº¦Lossé…ç½®:")
        print(f"   FIRMSæƒé‡: {config.firms_weight}, å…¶ä»–é©±åŠ¨å› ç´ æƒé‡: {config.other_drivers_weight}")
        print(f"   æ¸©åº¦å‚æ•°: 1.0, å¿½ç•¥0å€¼: {config.ignore_zero_values}")
        
    elif config.loss_type == 'multitask':
        # ä½¿ç”¨å¤šä»»åŠ¡æŸå¤±å‡½æ•°
        criterion = MultiTaskLoss(
            firms_weight=config.firms_weight,
            other_drivers_weight=config.other_drivers_weight,
            ignore_zero_values=config.ignore_zero_values,
            loss_function=config.loss_function
        )
        
        print(f"ğŸ” å¤šä»»åŠ¡æŸå¤±å‡½æ•°é…ç½®:")
        print(f"   FIRMSæƒé‡: {config.firms_weight}, å…¶ä»–é©±åŠ¨å› ç´ æƒé‡: {config.other_drivers_weight}")
        print(f"   å¿½ç•¥0å€¼: {config.ignore_zero_values}")
        print(f"   æŸå¤±å‡½æ•°: {config.loss_function}")
    
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æŸå¤±å‡½æ•°ç±»å‹: {config.loss_type}ã€‚æ”¯æŒçš„ç±»å‹: 'focal', 'kldiv', 'multitask'")
    
    print(f"ğŸ¯ å½“å‰ä½¿ç”¨æŸå¤±å‡½æ•°: {config.loss_type.upper()}")
    
    lr_scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(
        optimizer, T_0=config.T_0, T_mult=config.T_mult, eta_min=config.eta_min
    )
    
    # Early stopping
    early_stopping = MultiMetricEarlyStopping(patience=TRAINING_CONFIG['patience'], min_delta=0.0001, restore_best_weights=True)
    
    # è¿½è¸ªå„æŒ‡æ ‡çš„æœ€ä½³å€¼å’Œæ¨¡å‹è·¯å¾„
    best_metrics = {
        'f1': {'score': 0.0, 'path': None},
        'recall': {'score': 0.0, 'path': None},
        'pr_auc': {'score': 0.0, 'path': None},
        'mae': {'score': float('inf'), 'path': None},  # MAEè¶Šå°è¶Šå¥½ï¼Œåˆå§‹åŒ–ä¸ºæ— ç©·å¤§
        'mse': {'score': float('inf'), 'path': None}   # MSEè¶Šå°è¶Šå¥½ï¼Œåˆå§‹åŒ–ä¸ºæ— ç©·å¤§
    }
    
    print(f"ğŸš€ å¼€å§‹è®­ç»ƒ {config.epochs} ä¸ªepochs...")
    
    for epoch in range(config.epochs):
        # æ¯epoché‡æ–°æŠ½æ ·è®­ç»ƒé›†ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if hasattr(train_loader.dataset, 'resample_for_epoch'):
            train_loader.dataset.resample_for_epoch(epoch)
        
        model.train()
        train_loss = 0
        train_preds = []
        train_targets = []
        
        # è®­ç»ƒé˜¶æ®µ - ç®€åŒ–è¿›åº¦æ˜¾ç¤ºä»¥æé«˜æ€§èƒ½
        # train_progress = SimpleProgressTracker()
        for i, batch in enumerate(train_loader):
            # æ³¨é‡Šæ‰è¯¦ç»†çš„è®­ç»ƒè¿›åº¦æ˜¾ç¤ºä»¥å‡å°‘CPUå¼€é”€
            # train_progress.update(i+1, len(train_loader), f"ğŸ”¥ Epoch {epoch+1}/{config.epochs} Training")
            
            past, future, metadata_list = batch
            past, future = past.to(device), future.to(device)
            
            # ğŸ”¥ ä¿®æ”¹ï¼šä¸åˆ é™¤ç¬¬0ä¸ªé€šé“ï¼Œè€Œæ˜¯å°†å…¶æ•°æ®ç½®é›¶ï¼Œä¿æŒ39ä¸ªé€šé“çš„å®Œæ•´æ€§
            # past[:, 0, :] = 0.0  # å°†ç¬¬0ä¸ªé€šé“ï¼ˆFIRMSï¼‰ç½®é›¶ï¼Œè€Œä¸æ˜¯åˆ é™¤
            
            if firms_normalizer is not None:
                past, future = normalize_batch(past, future, firms_normalizer, metadata_list)
            
            date_strings = [str(int(metadata[0])) for metadata in metadata_list]
            
            future_truncated = future[:, :, :config.pred_len].transpose(1, 2)
            target = future_truncated[:, :, 0]  # å¦‚æœæ˜¯focal lossçš„å•é€šé“é¢„æµ‹ï¼Œåˆ™ä½¿ç”¨[:, :, 0]
            # target = (target > config.binarization_threshold).float()
            
            # å‰å‘ä¼ æ’­
            if model_name == 's_mamba':
                past_transposed = past.transpose(1, 2)
                past_truncated = past_transposed[:, -config.seq_len:, :]
                
                output = model(past_truncated, date_strings)
            else:
                x_enc, x_mark_enc, x_dec, x_mark_dec = adapter.adapt_inputs(past, future, date_strings)
                x_enc, x_mark_enc, x_dec, x_mark_dec = x_enc.to(device), x_mark_enc.to(device), x_dec.to(device), x_mark_dec.to(device)
                output = model(x_enc, x_mark_enc, x_dec, x_mark_dec)  # B T C
                        
            # å¤šä»»åŠ¡å­¦ä¹ ï¼šé¢„æµ‹æ‰€æœ‰39ä¸ªé€šé“
            # output shape: (B, T, C) where C=39
            # target shape: (B, T, C) where C=39
            target_all_channels = future_truncated  # ä½¿ç”¨æ‰€æœ‰é€šé“ä½œä¸ºç›®æ ‡

            # è®¡ç®—å¤šä»»åŠ¡FocalæŸå¤±
            # è®¾ç½®é«˜ç½®ä¿¡åº¦é˜ˆå€¼ï¼Œé¿å…ä½ç½®ä¿¡åº¦æ ·æœ¬å¯¹æŸå¤±å‡½æ•°çš„å½±å“
            # åªå¯¹FIRMSé€šé“äºŒå€¼åŒ–ï¼Œå…¶å®ƒé€šé“ä¿æŒåŸå€¼
            # target_all_channels = target_all_channels.clone()
            # target_all_channels[:, :, 0] = (target_all_channels[:, :, 0] > 10).float()
            loss, loss_components = criterion(output, target_all_channels)
            # print(f"FIRMS loss: {loss_components['firms_loss']}, Other drivers loss: {loss_components['other_drivers_loss']}")
            optimizer.zero_grad()
            loss.backward()
            
            if config.max_grad_norm > 0:
                torch.nn.utils.clip_grad_norm_(model.parameters(), config.max_grad_norm)
            optimizer.step()
            
            train_loss += loss.item()
            # åªä¿å­˜FIRMSé€šé“çš„é¢„æµ‹ç»“æœç”¨äºæŒ‡æ ‡è®¡ç®—
            train_preds.append(output[:, :, 0].detach())
            train_targets.append(target_all_channels[:, :, 0].detach())
        
        # è®¡ç®—è®­ç»ƒæŒ‡æ ‡
        train_loss /= len(train_loader)
        train_preds = torch.cat(train_preds, dim=0)
        train_targets = torch.cat(train_targets, dim=0)
        train_precision, train_recall, train_f1, train_pr_auc, train_mse, train_mae = calculate_detailed_metrics(train_preds, train_targets)
        
        # éªŒè¯é˜¶æ®µ
        model.eval()
        val_loss = 0
        val_preds = []
        val_targets = []
        
        with torch.no_grad():
            # éªŒè¯é˜¶æ®µ - ç®€åŒ–è¿›åº¦æ˜¾ç¤ºä»¥æé«˜æ€§èƒ½
            # val_progress = SimpleProgressTracker()
            for i, batch in enumerate(val_loader):
                # æ³¨é‡Šæ‰è¯¦ç»†çš„éªŒè¯è¿›åº¦æ˜¾ç¤ºä»¥å‡å°‘CPUå¼€é”€
                # val_progress.update(i+1, len(val_loader), f"ğŸ“Š Epoch {epoch+1}/{config.epochs} Validation")
                
                past, future, metadata_list = batch
                past, future = past.to(device), future.to(device)
                
                # ğŸ”¥ ä¿®æ”¹ï¼šä¸åˆ é™¤ç¬¬0ä¸ªé€šé“ï¼Œè€Œæ˜¯å°†å…¶æ•°æ®ç½®é›¶ï¼Œä¿æŒ39ä¸ªé€šé“çš„å®Œæ•´æ€§
                # past[:, 0, :] = 0.0  # å°†ç¬¬0ä¸ªé€šé“ï¼ˆFIRMSï¼‰ç½®é›¶ï¼Œè€Œä¸æ˜¯åˆ é™¤
                
                # ä¸ºä»€ä¹ˆè¦å¯¹æœªæ¥æ•°æ®ä¹Ÿå½’ä¸€åŒ–ï¼ï¼ï¼
                if firms_normalizer is not None:
                    past, future = normalize_batch(past, future, firms_normalizer, metadata_list)
                
                date_strings = [str(int(metadata[0])) for metadata in metadata_list]
                
                future_truncated = future[:, :, :config.pred_len].transpose(1, 2)
                target = future_truncated[:, :, 0]  # å¦‚æœæ˜¯focal lossçš„å•é€šé“é¢„æµ‹ï¼Œåˆ™ä½¿ç”¨[:, :, 0]
                # target = (target > config.binarization_threshold).float()
                
                if model_name == 's_mamba':
                    past_transposed = past.transpose(1, 2)
                    past_truncated = past_transposed[:, -config.seq_len:, :]
                    output = model(past_truncated, date_strings)
                else:
                    x_enc, x_mark_enc, x_dec, x_mark_dec = adapter.adapt_inputs(past, future, date_strings)
                    x_enc, x_mark_enc, x_dec, x_mark_dec = x_enc.to(device), x_mark_enc.to(device), x_dec.to(device), x_mark_dec.to(device)
                    output = model(x_enc, x_mark_enc, x_dec, x_mark_dec)
                
                # å¤šä»»åŠ¡å­¦ä¹ ï¼šé¢„æµ‹æ‰€æœ‰39ä¸ªé€šé“
                target_all_channels = future_truncated  # ä½¿ç”¨æ‰€æœ‰é€šé“ä½œä¸ºç›®æ ‡
                
                # è®¡ç®—å¤šä»»åŠ¡FocalæŸå¤±
                # target_all_channels = target_all_channels.clone()
                # target_all_channels[:, :, 0] = (target_all_channels[:, :, 0] > 10).float()
                loss, loss_components = criterion(output, target_all_channels)
                val_loss += loss.item()
                
                # åªä¿å­˜FIRMSé€šé“çš„é¢„æµ‹ç»“æœç”¨äºæŒ‡æ ‡è®¡ç®—
                val_preds.append(output[:, :, 0].detach())
                val_targets.append(target_all_channels[:, :, 0].detach())
        
        # è®¡ç®—éªŒè¯æŒ‡æ ‡
        val_loss /= len(val_loader)
        val_preds = torch.cat(val_preds, dim=0)
        val_targets = torch.cat(val_targets, dim=0)
        val_precision, val_recall, val_f1, val_pr_auc, val_mse, val_mae = calculate_detailed_metrics(val_preds, val_targets)
        
        # è®°å½•å½“å‰epochçš„æŒ‡æ ‡
        epoch_data = {
                "epoch": epoch + 1,
                "train_loss": train_loss,
                "train_precision": train_precision,
                "train_recall": train_recall,
                "train_f1": train_f1,
                "train_pr_auc": train_pr_auc,
                "train_mse": train_mse,
                "train_mae": train_mae,
                "val_loss": val_loss,
                "val_precision": val_precision,
                "val_recall": val_recall,
                "val_f1": val_f1,
                "val_pr_auc": val_pr_auc,
                "val_mse": val_mse,
                "val_mae": val_mae,
                "learning_rate": optimizer.param_groups[0]["lr"],
                # æ·»åŠ å¤šä»»åŠ¡æŸå¤±ç»„ä»¶ä¿¡æ¯
                "firms_weight": config.firms_weight,
                "other_drivers_weight": config.other_drivers_weight,
                "loss_function": config.loss_function,
                "ignore_zero_values": config.ignore_zero_values
        }
        epoch_metrics.append(epoch_data)
        
        # è®°å½•åˆ°wandb
        if wandb_run:
            wandb.log(epoch_data)
        
        # æ˜¾ç¤ºè®­ç»ƒè¿›åº¦
        print(f"Epoch {epoch+1}/{config.epochs} - "
              f"Train Loss: {train_loss:.4f} (F1: {train_f1:.4f}) - "
              f"Val Loss: {val_loss:.4f} (F1: {val_f1:.4f}) - "
              f"LR: {optimizer.param_groups[0]['lr']:.2e}")
        
        # æ˜¾ç¤ºå¤šä»»åŠ¡æŸå¤±ç»„ä»¶ä¿¡æ¯ï¼ˆæ¯5ä¸ªepochæ˜¾ç¤ºä¸€æ¬¡ï¼‰
        if (epoch + 1) % 5 == 0:
            print(f"   å¤šä»»åŠ¡æŸå¤±ç»„ä»¶ - FIRMS: {config.firms_weight:.1f}, "
                  f"å…¶ä»–é©±åŠ¨å› ç´ : {config.other_drivers_weight:.1f}, "
                  f"æŸå¤±å‡½æ•°: {config.loss_function}")
        
        # ä¿å­˜å„æŒ‡æ ‡çš„æœ€ä½³æ¨¡å‹
        model_save_dir = TRAINING_CONFIG[model_type]['model_save_dir']
        os.makedirs(model_save_dir, exist_ok=True)
        
        metrics_to_save = {
            'f1': val_f1,
            'recall': val_recall,
            'pr_auc': val_pr_auc,
            'mae': val_mae,
            'mse': val_mse
        }
        
        for metric_name, score in metrics_to_save.items():
            # MAEå’ŒMSEæ˜¯è¶Šå°è¶Šå¥½ï¼Œå…¶ä»–æŒ‡æ ‡æ˜¯è¶Šå¤§è¶Šå¥½
            if metric_name in ['mae', 'mse']:
                if score <= best_metrics[metric_name]['score']:
                    best_metrics[metric_name]['score'] = score
                    model_path = os.path.join(model_save_dir, f'{model_name}_best_{metric_name}.pth')
                    torch.save(model.state_dict(), model_path)
                    best_metrics[metric_name]['path'] = model_path
            else:
                if score >= best_metrics[metric_name]['score']:
                    best_metrics[metric_name]['score'] = score
                    model_path = os.path.join(model_save_dir, f'{model_name}_best_{metric_name}.pth')
                    torch.save(model.state_dict(), model_path)
                    best_metrics[metric_name]['path'] = model_path
        
        # æ‰“å°epochæ€»ç»“
        print(f'Epoch {epoch+1:3d}/{config.epochs} | Train: Loss={train_loss:.4f}, F1={train_f1:.4f}, MSE={train_mse:.6f}, MAE={train_mae:.6f} | '
              f'Val: Loss={val_loss:.4f}, P={val_precision:.4f}, R={val_recall:.4f}, F1={val_f1:.4f}, PR-AUC={val_pr_auc:.4f}, MSE={val_mse:.6f}, MAE={val_mae:.6f} | '
              f'LR={optimizer.param_groups[0]["lr"]:.2e}')
        
        # Early stoppingæ£€æŸ¥ (ä½¿ç”¨å¤šæŒ‡æ ‡)
        if early_stopping({'f1': val_f1, 'recall': val_recall, 'pr_auc': val_pr_auc, 'mae': val_mae, 'mse': val_mse}, model):
            print(f"â¹ï¸  Early stopping triggered at epoch {epoch+1} (patience={TRAINING_CONFIG['patience']}, counter={early_stopping.counter})")
            break
        
        lr_scheduler.step()
    
    # ä¿å­˜æœ€åä¸€ä¸ªepochçš„æ¨¡å‹å‚æ•°
    final_model_path = os.path.join(model_save_dir, f'{model_name}_final_epoch.pth')
    torch.save(model.state_dict(), final_model_path)
    print(f"ğŸ’¾ æœ€åä¸€ä¸ªepochæ¨¡å‹å·²ä¿å­˜: {final_model_path}")
    
    # å°†æœ€åepochè·¯å¾„æ·»åŠ åˆ°è¿”å›ç»“æœä¸­
    best_metrics['final_epoch'] = {
        'score': epoch + 1,  # è®°å½•æœ€ç»ˆepochæ•°
        'path': final_model_path
    }
    
    # ä¿å­˜è¯¦ç»†çš„epochè®­ç»ƒæ—¥å¿—
    if log_file:
        save_epoch_metrics_to_log(epoch_metrics, log_file, model_name, model_type)
    
    # å…³é—­wandb
    if wandb_run:
        wandb.finish()
    
    return best_metrics

def test_model(model_name, model_path, device, test_loader, firms_normalizer, model_type='standard'):
    """æµ‹è¯•æ¨¡å‹"""
    print(f"\nğŸ“Š æµ‹è¯•{model_type}æ¨¡å‹: {model_name}")
    
    config = Config(model_name, model_type)
    
    # ä½¿ç”¨ç»Ÿä¸€é€‚é…å™¨
    from model_adapter_unified import UnifiedModelAdapter
    adapter = UnifiedModelAdapter(config)
    
    # æ ¹æ®é…ç½®é€‰æ‹©æŸå¤±å‡½æ•°ç±»å‹
    if config.loss_type == 'focal':
        # åˆ›å»ºå¤šä»»åŠ¡FocalæŸå¤±å‡½æ•°ç”¨äºæµ‹è¯•
        criterion = MultiTaskFocalLoss(
            firms_weight=config.firms_weight,
            other_drivers_weight=config.other_drivers_weight,
            focal_alpha=config.focal_alpha,
            focal_gamma=config.focal_gamma,
            ignore_zero_values=config.ignore_zero_values,
            regression_loss=config.loss_function
        )
        
        print(f"ğŸ” æµ‹è¯•é˜¶æ®µå¤šä»»åŠ¡Focal Lossé…ç½®:")
        print(f"   FIRMSæƒé‡: {config.firms_weight}, å…¶ä»–é©±åŠ¨å› ç´ æƒé‡: {config.other_drivers_weight}")
        print(f"   Focal Î±: {config.focal_alpha}, Focal Î³: {config.focal_gamma}")
        print(f"   å›å½’æŸå¤±: {config.loss_function}, å¿½ç•¥0å€¼: {config.ignore_zero_values}")
        
    elif config.loss_type == 'kldiv':
        # åˆ›å»ºå¤šä»»åŠ¡KLæ•£åº¦æŸå¤±å‡½æ•°ç”¨äºæµ‹è¯•
        criterion = MultiTaskKLDivLoss(
            firms_weight=config.firms_weight,
            other_drivers_weight=config.other_drivers_weight,
            ignore_zero_values=config.ignore_zero_values,
            temperature=1.0,
            epsilon=1e-8
        )
        
        print(f"ğŸ” æµ‹è¯•é˜¶æ®µå¤šä»»åŠ¡KLæ•£åº¦Lossé…ç½®:")
        print(f"   FIRMSæƒé‡: {config.firms_weight}, å…¶ä»–é©±åŠ¨å› ç´ æƒé‡: {config.other_drivers_weight}")
        print(f"   æ¸©åº¦å‚æ•°: 1.0, å¿½ç•¥0å€¼: {config.ignore_zero_values}")
        
    elif config.loss_type == 'multitask':
        # åˆ›å»ºå¤šä»»åŠ¡æŸå¤±å‡½æ•°ç”¨äºæµ‹è¯•
        criterion = MultiTaskLoss(
            firms_weight=config.firms_weight,
            other_drivers_weight=config.other_drivers_weight,
            ignore_zero_values=config.ignore_zero_values,
            loss_function=config.loss_function
        )
        
        print(f"ğŸ” æµ‹è¯•é˜¶æ®µå¤šä»»åŠ¡æŸå¤±å‡½æ•°é…ç½®:")
        print(f"   FIRMSæƒé‡: {config.firms_weight}, å…¶ä»–é©±åŠ¨å› ç´ æƒé‡: {config.other_drivers_weight}")
        print(f"   å¿½ç•¥0å€¼: {config.ignore_zero_values}")
        print(f"   æŸå¤±å‡½æ•°: {config.loss_function}")
    
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æŸå¤±å‡½æ•°ç±»å‹: {config.loss_type}ã€‚æ”¯æŒçš„ç±»å‹: 'focal', 'kldiv', 'multitask'")
    
    print(f"ğŸ¯ æµ‹è¯•é˜¶æ®µä½¿ç”¨æŸå¤±å‡½æ•°: {config.loss_type.upper()}")
    
    try:
        model, _ = load_model(model_name, config, model_type)
        model.load_state_dict(torch.load(model_path))
        model = model.to(device)
        model.eval()
    except Exception as e:
        print(f"âŒ {model_type}æ¨¡å‹ {model_name} æµ‹è¯•åŠ è½½å¤±è´¥: {e}")
        return None
    
    test_preds = []
    test_targets = []
    total_test_loss = 0.0
    
    with torch.no_grad():
        # ç®€åŒ–æµ‹è¯•è¿›åº¦æ˜¾ç¤ºä»¥æé«˜æ€§èƒ½
        # test_progress = SimpleProgressTracker()
        for i, batch in enumerate(test_loader):
            # æ³¨é‡Šæ‰è¯¦ç»†çš„æµ‹è¯•è¿›åº¦æ˜¾ç¤ºä»¥å‡å°‘CPUå¼€é”€
            # test_progress.update(i+1, len(test_loader), f"ğŸ§ª Testing {model_name}")
            past, future, metadata_list = batch
            past, future = past.to(device), future.to(device)
            
            # ğŸ”¥ ä¿®æ”¹ï¼šä¸åˆ é™¤ç¬¬0ä¸ªé€šé“ï¼Œè€Œæ˜¯å°†å…¶æ•°æ®ç½®é›¶ï¼Œä¿æŒ39ä¸ªé€šé“çš„å®Œæ•´æ€§
            # past[:, 0, :] = 0.0  # å°†ç¬¬0ä¸ªé€šé“ï¼ˆFIRMSï¼‰ç½®é›¶ï¼Œè€Œä¸æ˜¯åˆ é™¤
            
            if firms_normalizer is not None:
                past, future = normalize_batch(past, future, firms_normalizer, metadata_list)
            
            date_strings = [str(int(metadata[0])) for metadata in metadata_list]
            
            future_truncated = future[:, :, :config.pred_len].transpose(1, 2)
            target = future_truncated[:, :, 0]  # å¦‚æœæ˜¯focal lossçš„å•é€šé“é¢„æµ‹ï¼Œåˆ™ä½¿ç”¨[:, :, 0]
            # target = (target > config.binarization_threshold).float()
            
            if model_name == 's_mamba':
                past_transposed = past.transpose(1, 2)
                past_truncated = past_transposed[:, -config.seq_len:, :]
                output = model(past_truncated, date_strings)
            else:
                x_enc, x_mark_enc, x_dec, x_mark_dec = adapter.adapt_inputs(past, future, date_strings)
                x_enc, x_mark_enc, x_dec, x_mark_dec = x_enc.to(device), x_mark_enc.to(device), x_dec.to(device), x_mark_dec.to(device)
                output = model(x_enc, x_mark_enc, x_dec, x_mark_dec)
            
            # å¤šä»»åŠ¡å­¦ä¹ ï¼šé¢„æµ‹æ‰€æœ‰39ä¸ªé€šé“
            target_all_channels = future_truncated  # ä½¿ç”¨æ‰€æœ‰é€šé“ä½œä¸ºç›®æ ‡
            
            # è®¡ç®—å¤šä»»åŠ¡FocalæŸå¤±
            # target_all_channels = target_all_channels.clone()
            # target_all_channels[:, :, 0] = (target_all_channels[:, :, 0] > 10).float()
            loss, loss_components = criterion(output, target_all_channels)
            total_test_loss += loss.item()
            
            # åªä¿å­˜FIRMSé€šé“çš„é¢„æµ‹ç»“æœç”¨äºæŒ‡æ ‡è®¡ç®—
            test_preds.append(output[:, :, 0].detach())
            test_targets.append((target_all_channels[:, :, 0].detach()).float())
    
    # è®¡ç®—æµ‹è¯•æŒ‡æ ‡ - ä½¿ç”¨F1æœ€ä¼˜é˜ˆå€¼
    test_preds = torch.cat(test_preds, dim=0)
    test_targets = torch.cat(test_targets, dim=0)
    precision, recall, f1, pr_auc, mse, mae = calculate_optimal_f1_metrics(test_preds, test_targets)
    
    avg_test_loss = total_test_loss / len(test_loader)
    
    print(f"âœ… {model_name} {model_type}æ¨¡å‹æµ‹è¯•ç»“æœ: P={precision:.4f}, R={recall:.4f}, F1={f1:.4f}, PR-AUC={pr_auc:.4f}, MSE={mse:.6f}, MAE={mae:.6f}")
    print(f"   å¤šä»»åŠ¡æŸå¤±: {avg_test_loss:.6f} (FIRMSæƒé‡: {config.firms_weight:.1f}, å…¶ä»–é©±åŠ¨å› ç´ æƒé‡: {config.other_drivers_weight:.1f})")
    
    return {
        'model': model_name,
        'model_type': model_type,
        'precision': precision,
        'recall': recall,
        'f1': f1,
        'pr_auc': pr_auc,
        'mse': mse,
        'mae': mae,
        'test_loss': avg_test_loss,
        'firms_weight': config.firms_weight,
        'other_drivers_weight': config.other_drivers_weight,
        'loss_function': config.loss_function,
        'loss_type': config.loss_type,  # æ–°å¢ï¼šæŸå¤±å‡½æ•°ç±»å‹ä¿¡æ¯
        'ignore_zero_values': config.ignore_zero_values
    }

def train_and_test_models(model_list, model_type, device, train_loader, val_loader, test_loader, firms_normalizer, force_retrain=False):
    """è®­ç»ƒå’Œæµ‹è¯•ä¸€ç±»æ¨¡å‹"""
    print(f"\nğŸ”¥ å¼€å§‹è®­ç»ƒ{model_type}æ¨¡å‹ç»„")
    print(f"ğŸ“‹ åŸå§‹æ¨¡å‹åˆ—è¡¨: {len(model_list)} ä¸ª{model_type}æ¨¡å‹")
    print(f"ğŸ“Š {model_type}æ¨¡å‹åˆ—è¡¨: {', '.join(model_list)}")
    
    # è¿‡æ»¤å·²è®­ç»ƒçš„æ¨¡å‹
    models_to_train, trained_models = filter_trained_models(model_list, model_type, force_retrain)
    
    # è®­ç»ƒæ–°æ¨¡å‹
    model_results = []
    failed_models = []
    
    if models_to_train:
        print(f"\nğŸš€ å¼€å§‹è®­ç»ƒ {len(models_to_train)} ä¸ªéœ€è¦è®­ç»ƒçš„{model_type}æ¨¡å‹...")
        for i, model_name in enumerate(models_to_train):
            print(f"\nğŸ”„ {model_type}è®­ç»ƒè¿›åº¦: {i+1}/{len(models_to_train)} (æ€»ä½“: {i+1+len(trained_models)}/{len(model_list)})")
        try:
            result = train_single_model(
                model_name, device, train_loader, val_loader, test_loader, firms_normalizer, model_type
            )
            if result is not None:
                best_metrics = result
                print(f"âœ… {model_name} {model_type}æ¨¡å‹è®­ç»ƒå®Œæˆï¼Œä¿å­˜çš„æ¨¡å‹:")
                for metric_name, metric_info in best_metrics.items():
                        if metric_name == 'final_epoch':
                            print(f"  æœ€åepochæ¨¡å‹ (epoch {metric_info['score']}): {metric_info['path']}")
                        else:
                            print(f"  æœ€ä½³{metric_name}æ¨¡å‹ ({metric_info['score']:.4f}): {metric_info['path']}")
                model_results.append((model_name, best_metrics))
            else:
                failed_models.append(model_name)
        except Exception as e:
            print(f"âŒ {model_name} {model_type}æ¨¡å‹è®­ç»ƒå¤±è´¥: {e}")
            failed_models.append(model_name)
            # æ¸…ç†GPUå†…å­˜
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
    else:
        print(f"\nâœ… æ‰€æœ‰{model_type}æ¨¡å‹éƒ½å·²è®­ç»ƒå®Œæˆï¼Œè·³è¿‡è®­ç»ƒé˜¶æ®µ")
    
    # å°†å·²è®­ç»ƒçš„æ¨¡å‹æ·»åŠ åˆ°ç»“æœä¸­
    for model_name, trained_paths in trained_models.items():
        model_results.append((model_name, trained_paths))
        print(f"ğŸ“‹ åŠ è½½å·²è®­ç»ƒæ¨¡å‹: {model_name} ({len(trained_paths)}ä¸ªä¿å­˜ç‰ˆæœ¬)")
    
    print(f"\nğŸ“ˆ {model_type}æ¨¡å‹å‡†å¤‡å®Œæˆ!")
    print(f"   æ–°è®­ç»ƒ: {len(models_to_train)} ä¸ª")
    print(f"   å·²è®­ç»ƒ: {len(trained_models)} ä¸ª") 
    print(f"   è®­ç»ƒå¤±è´¥: {len(failed_models)} ä¸ª")
    print(f"   æ€»å¯ç”¨: {len(model_results)} ä¸ª")
    
    if failed_models:
        print(f"âŒ å¤±è´¥çš„{model_type}æ¨¡å‹: {', '.join(failed_models)}")
    
    # æµ‹è¯•é˜¶æ®µ
    print("\n" + "="*60)
    print("ğŸ§ª æµ‹è¯•é˜¶æ®µ - è¯„ä¼°è®­ç»ƒå¥½çš„æ¨¡å‹")
    print("="*60)
    
    # ç”¨äºå­˜å‚¨ç»“æ„åŒ–æµ‹è¯•ç»“æœçš„å­—å…¸
    structured_results = {}
    
    for model_name, metrics in model_results:
        print(f"\nğŸ“‹ æµ‹è¯•æ¨¡å‹: {model_name}")
        print("-" * 40)
        
        # åˆå§‹åŒ–è¯¥æ¨¡å‹çš„ç»“æœå­—å…¸
        structured_results[model_name] = {
            'f1': {'precision': None, 'recall': None, 'f1': None, 'pr_auc': None, 'mse': None, 'mae': None},
            'recall': {'precision': None, 'recall': None, 'f1': None, 'pr_auc': None, 'mse': None, 'mae': None},
            'pr_auc': {'precision': None, 'recall': None, 'f1': None, 'pr_auc': None, 'mse': None, 'mae': None},
            'mae': {'precision': None, 'recall': None, 'f1': None, 'pr_auc': None, 'mse': None, 'mae': None},
            'final_epoch': {'precision': None, 'recall': None, 'f1': None, 'pr_auc': None, 'mse': None, 'mae': None}
        }
        
        # åˆ†åˆ«æµ‹è¯•æ‰€æœ‰ä¿å­˜çš„æ¨¡å‹ï¼ˆåŒ…æ‹¬æœ€ä½³æ¨¡å‹å’Œæœ€åepochæ¨¡å‹ï¼‰
        for metric_name, metric_info in metrics.items():
            if metric_info['path'] is not None:
                if metric_name == 'final_epoch':
                    print(f"\nğŸ¯ æµ‹è¯•æœ€åä¸€ä¸ªepochçš„æ¨¡å‹ (epoch: {metric_info['score']})")
                else:
                    print(f"\nğŸ¯ æµ‹è¯•åŸºäº {metric_name.upper()} çš„æœ€ä½³æ¨¡å‹ (åˆ†æ•°: {metric_info['score']:.4f})")
                try:
                    result = test_model(model_name, metric_info['path'], device, test_loader, firms_normalizer, model_type)
                    if result:
                        # ä¿å­˜åˆ°ç»“æ„åŒ–ç»“æœä¸­
                        structured_results[model_name][metric_name] = {
                            'precision': result['precision'],
                            'recall': result['recall'],
                            'f1': result['f1'],
                            'pr_auc': result['pr_auc'],
                            'mse': result['mse'],
                            'mae': result['mae']
                        }
                        print(f"âœ… {model_name} ({metric_name}) æµ‹è¯•å®Œæˆ")
                except Exception as e:
                    print(f"âŒ {model_name} ({metric_name}) æµ‹è¯•å¤±è´¥: {str(e)}")
    
    if not structured_results:
        print("âš ï¸  æ²¡æœ‰æ¨¡å‹é€šè¿‡æµ‹è¯•ï¼")
        return None
    
    # ä¿å­˜ç»“æ„åŒ–ç»“æœåˆ°CSV
    save_structured_results_to_csv(structured_results, model_type)
    
    # è¾“å‡ºæœ€ç»ˆç»“æœæ€»ç»“
    print("\n" + "="*80)
    print("ğŸ“Š æœ€ç»ˆæµ‹è¯•ç»“æœæ€»ç»“")
    print("="*80)
    
    # æ˜¾ç¤ºè¡¨æ ¼å½¢å¼çš„ç»“æœ
    for model_name, model_results in structured_results.items():
        print(f"\nğŸ”¥ æ¨¡å‹: {model_name}")
        print("-" * 80)
        print(f"{'æŒ‡æ ‡ç±»å‹':<12} {'ç²¾ç¡®ç‡':<8} {'å¬å›ç‡':<8} {'F1åˆ†æ•°':<8} {'PR-AUC':<8} {'MSE':<10} {'MAE':<10}")
        print("-" * 80)
        for metric_type, metrics in model_results.items():
            if metrics['precision'] is not None:
                display_type = "FINAL" if metric_type == 'final_epoch' else metric_type.upper()
                print(f"{display_type:<12} {metrics['precision']:<8.4f} {metrics['recall']:<8.4f} {metrics['f1']:<8.4f} {metrics['pr_auc']:<8.4f} {metrics['mse']:<10.6f} {metrics['mae']:<10.6f}")
    
    print(f"\nğŸ‰ è®­ç»ƒå’Œæµ‹è¯•å®Œæˆï¼å…±è®­ç»ƒäº† {len(model_results)} ä¸ªæ¨¡å‹")
    
    if failed_models:
        print(f"\nâš ï¸  å¤±è´¥çš„æ¨¡å‹: {failed_models}")
    
    print("\nğŸ“ æ‰€æœ‰æ¨¡å‹å·²ä¿å­˜åˆ°ç›¸åº”ç›®å½•ä¸­")
    save_dir = STANDARD_MODEL_DIR if model_type == 'standard' else MODEL_10X_DIR
    print(f"ğŸ“Š æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ°ç›®å½•: {save_dir}")
    
    return structured_results

def prepare_data_loaders():
    """å‡†å¤‡æ•°æ®åŠ è½½å™¨"""
    print("ğŸ“‚ åŠ è½½æ•°æ®...")
    data_loader = TimeSeriesDataLoader(
        # h5_dir='/mnt/raid/zhengsen/wildfire_dataset/self_built_materials/pixel_samples_merged',
        h5_dir='/mnt/raid/zhengsen/wildfire_dataset/self_built_materials/full_datasets',
        positive_ratio=DATA_CONFIG['positive_ratio'],
        pos_neg_ratio=DATA_CONFIG['pos_neg_ratio'],
        resample_each_epoch=False  # åœ¨åº•å±‚ç¦ç”¨ï¼Œæ”¹ç”¨åŠ¨æ€æŠ½æ ·
    )
    
    # æ•°æ®é›†åˆ’åˆ†
    train_indices, val_indices, test_indices = data_loader.get_year_based_split(
        train_years=DATA_CONFIG['train_years'],
        val_years=DATA_CONFIG['val_years'],
        test_years=DATA_CONFIG['test_years']
    )
    
    # ä½¿ç”¨è‡ªå®šä¹‰çš„åŠ¨æ€æŠ½æ ·æ•°æ®é›†æ›¿ä»£æ ‡å‡†Subset
    train_dataset = DynamicSamplingSubset(
        dataset=data_loader.dataset,
        full_indices=train_indices,
        sampling_ratio=DATA_CONFIG['sampling_ratio'],
        enable_dynamic_sampling=DATA_CONFIG['enable_dynamic_sampling']
    )
    
    # éªŒè¯é›†å’Œæµ‹è¯•é›†ä½¿ç”¨å®Œæ•´æ•°æ®ï¼Œä¸è¿›è¡ŒåŠ¨æ€æŠ½æ ·
    val_dataset = Subset(data_loader.dataset, val_indices)
    test_dataset = Subset(data_loader.dataset, test_indices)
    
    print(f"ğŸ“Š æ•°æ®é›†å¤§å°:")
    print(f"   è®­ç»ƒé›†: {len(train_dataset)} (å®Œæ•´: {len(train_indices)})")
    print(f"   éªŒè¯é›†: {len(val_dataset)} (å®Œæ•´æ•°æ®)")
    print(f"   æµ‹è¯•é›†: {len(test_dataset)} (å®Œæ•´æ•°æ®)")
    print(f"   åŠ¨æ€æŠ½æ ·: {'å¯ç”¨' if DATA_CONFIG['enable_dynamic_sampling'] else 'ç¦ç”¨'}")
    if DATA_CONFIG['enable_dynamic_sampling']:
        print(f"   æŠ½æ ·é…ç½®: æ¯epochéšæœºä½¿ç”¨ {DATA_CONFIG['sampling_ratio']:.1%} çš„è®­ç»ƒæ•°æ®")
    
    return train_dataset, val_dataset, test_dataset, data_loader

def main():
    """ä¸»å‡½æ•° - ä¾æ¬¡è®­ç»ƒæ ‡å‡†æ¨¡å‹å’Œ10xæ¨¡å‹"""
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description='é‡ç«é¢„æµ‹æ¨¡å‹è®­ç»ƒè„šæœ¬')
    parser.add_argument('--skip-10x', action='store_true', 
                       help='è·³è¿‡10xæ¨¡å‹è®­ç»ƒï¼Œåªè®­ç»ƒæ ‡å‡†æ¨¡å‹')
    parser.add_argument('--only-10x', action='store_true',
                       help='åªè®­ç»ƒ10xæ¨¡å‹ï¼Œè·³è¿‡æ ‡å‡†æ¨¡å‹')
    parser.add_argument('--force-retrain', action='store_true',
                       help='å¼ºåˆ¶é‡æ–°è®­ç»ƒæ‰€æœ‰æ¨¡å‹ï¼Œå¿½ç•¥å·²å­˜åœ¨çš„æ¨¡å‹æ–‡ä»¶')
    
    # å¤šä»»åŠ¡å­¦ä¹ å‚æ•°
    parser.add_argument('--firms-weight', type=float, default=1,  # 0.005 for focal loss, 0.1 for multitask 
                       help='FIRMSé¢„æµ‹çš„æŸå¤±æƒé‡ (é»˜è®¤: 1.0)')
    parser.add_argument('--other-drivers-weight', type=float, default=1.0,
                       help='å…¶ä»–é©±åŠ¨å› ç´ é¢„æµ‹çš„æŸå¤±æƒé‡ (é»˜è®¤: 1.0)')
    parser.add_argument('--loss-function', type=str, default='mse',
                       choices=['huber', 'mse', 'mae'],
                       help='å…¶ä»–é©±åŠ¨å› ç´ çš„å›å½’æŸå¤±å‡½æ•°ç±»å‹ (é»˜è®¤: mse)')
    parser.add_argument('--no-ignore-zero', action='store_true',
                       help='ä¸å¿½ç•¥å…¶ä»–é©±åŠ¨å› ç´ ä¸­çš„0å€¼')
    
    # Focal Losså‚æ•°
    parser.add_argument('--focal-alpha', type=float, default=0.5,
                       help='Focal Lossçš„alphaå‚æ•° (é»˜è®¤: 0.5)')
    parser.add_argument('--focal-gamma', type=float, default=2.0,
                       help='Focal Lossçš„gammaå‚æ•° (é»˜è®¤: 2.0)')
    
    # æŸå¤±å‡½æ•°ç±»å‹é€‰æ‹©å‚æ•°
    parser.add_argument('--loss-type', type=str, default='focal',  #######################################
                       choices=['focal', 'kldiv', 'multitask'],
                       help='æŸå¤±å‡½æ•°ç±»å‹é€‰æ‹© (é»˜è®¤: focal)')
    
    # ğŸ”¥ æ–°å¢ï¼šä½ç½®ä¿¡æ¯å’Œæ°”è±¡æ•°æ®ç‰¹å¾å‚æ•°
    parser.add_argument('--enable-position-features', action='store_true',
                       help='å¯ç”¨ä½ç½®ä¿¡æ¯ç‰¹å¾ (é»˜è®¤: ç¦ç”¨)')
    parser.add_argument('--enable-future-weather', action='store_true', 
                       help='å¯ç”¨æœªæ¥æ°”è±¡æ•°æ®ç‰¹å¾ (é»˜è®¤: ç¦ç”¨)')
    parser.add_argument('--weather-channels', type=str, default='1-12',
                       help='æ°”è±¡æ•°æ®é€šé“èŒƒå›´ï¼Œæ ¼å¼å¦‚"1-12"æˆ–"1,3,5-8" (é»˜è®¤: 1-12)')
    
    args = parser.parse_args()
    
    # ğŸ”¥ æ–°å¢ï¼šè§£ææ°”è±¡æ•°æ®é€šé“èŒƒå›´
    def parse_channel_range(channel_str):
        """è§£æé€šé“èŒƒå›´å­—ç¬¦ä¸²ï¼Œè¿”å›é€šé“ç´¢å¼•åˆ—è¡¨"""
        channels = []
        for part in channel_str.split(','):
            if '-' in part:
                start, end = map(int, part.split('-'))
                channels.extend(range(start, end + 1))
            else:
                channels.append(int(part))
        return channels
    
    # æ›´æ–°æ•°æ®é…ç½®
    global DATA_CONFIG
    DATA_CONFIG['enable_position_features'] = args.enable_position_features
    DATA_CONFIG['enable_future_weather'] = args.enable_future_weather
    
    if args.enable_future_weather:
        try:
            DATA_CONFIG['weather_channels'] = parse_channel_range(args.weather_channels)
        except ValueError as e:
            print(f"âŒ æ°”è±¡é€šé“èŒƒå›´æ ¼å¼é”™è¯¯: {args.weather_channels}")
            print(f"   é”™è¯¯ä¿¡æ¯: {e}")
            print(f"   æ­£ç¡®æ ¼å¼ç¤ºä¾‹: '1-12' æˆ– '1,3,5-8'")
            return
    
    # æ›´æ–°å¤šä»»åŠ¡å­¦ä¹ é…ç½®
    global MULTITASK_CONFIG, TRAINING_CONFIG
    MULTITASK_CONFIG['firms_weight'] = args.firms_weight
    MULTITASK_CONFIG['other_drivers_weight'] = args.other_drivers_weight
    MULTITASK_CONFIG['loss_function'] = args.loss_function
    MULTITASK_CONFIG['ignore_zero_values'] = not args.no_ignore_zero
    MULTITASK_CONFIG['loss_type'] = args.loss_type  # æ–°å¢ï¼šæŸå¤±å‡½æ•°ç±»å‹é…ç½®
    
    # æ›´æ–°Focal Lossé…ç½®
    TRAINING_CONFIG['focal_alpha'] = args.focal_alpha
    TRAINING_CONFIG['focal_gamma'] = args.focal_gamma
    
    print("ğŸ”¥ é‡ç«é¢„æµ‹æ¨¡å‹å…¨é¢å¯¹æ¯”å®éªŒ - ç»Ÿä¸€ç‰ˆæœ¬")
    
    # æ˜¾ç¤ºå…±äº«é…ç½®çŠ¶æ€
    print_config_status()
    print()
    
    # æ ¹æ®å‘½ä»¤è¡Œå‚æ•°å†³å®šè®­ç»ƒå“ªäº›æ¨¡å‹
    train_standard = not args.only_10x
    train_10x = ENABLE_10X_TRAINING and not args.skip_10x
    
    if args.skip_10x:
        print("ğŸ“‹ å·²é€‰æ‹©è·³è¿‡10xæ¨¡å‹è®­ç»ƒ")
        train_10x = False
    elif args.only_10x:
        print("ğŸ“‹ å·²é€‰æ‹©åªè®­ç»ƒ10xæ¨¡å‹")
        train_standard = False
    
    print(f"ğŸ“‹ è®­ç»ƒè®¡åˆ’: æ ‡å‡†æ¨¡å‹={'âœ…' if train_standard else 'âŒ'}, 10xæ¨¡å‹={'âœ…' if train_10x else 'âŒ'}")
    if args.force_retrain:
        print("ğŸ”„ å¼ºåˆ¶é‡æ–°è®­ç»ƒæ¨¡å¼å·²å¯ç”¨ï¼Œå°†å¿½ç•¥å·²å­˜åœ¨çš„æ¨¡å‹æ–‡ä»¶")
    
    # åˆå§‹åŒ–
    set_seed(TRAINING_CONFIG['seed'])
    # ä½¿ç”¨å½“å‰å¯è§çš„ç¬¬ä¸€ä¸ªCUDAè®¾å¤‡ï¼ˆé€šè¿‡CUDA_VISIBLE_DEVICESæ§åˆ¶ï¼‰
    device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')
    
    # æ˜¾ç¤ºå®é™…ä½¿ç”¨çš„GPUä¿¡æ¯
    if torch.cuda.is_available():
        actual_gpu = torch.cuda.current_device()
        gpu_name = torch.cuda.get_device_name(actual_gpu)
        print(f"ğŸ–¥ï¸  ä½¿ç”¨è®¾å¤‡: cuda:0 (å®é™…GPU: {gpu_name})")
        print(f"ğŸ” CUDA_VISIBLE_DEVICES: {os.environ.get('CUDA_VISIBLE_DEVICES', 'æœªè®¾ç½®')}")
    else:
        print(f"ğŸ–¥ï¸  ä½¿ç”¨è®¾å¤‡: {device}")
    
    # WandBé…ç½®æ£€æŸ¥
    if TRAINING_CONFIG['use_wandb']:
        if WANDB_AVAILABLE:
            print("âœ… WandBç›‘æ§å·²å¯ç”¨")
        else:
            print("âš ï¸ WandBç›‘æ§å·²é…ç½®ä½†wandbæœªå®‰è£…ï¼Œå°†è·³è¿‡ç›‘æ§åŠŸèƒ½")
    else:
        print("â„¹ï¸ WandBç›‘æ§å·²ç¦ç”¨")
    
    # æ£€æŸ¥GPUå†…å­˜
    if torch.cuda.is_available():
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
        print(f"ğŸ’¾ GPUå†…å­˜: {gpu_memory:.1f} GB")
    
    # å‡†å¤‡æ•°æ®
    train_dataset, val_dataset, test_dataset, data_loader_obj = prepare_data_loaders()
    
    # åˆå§‹åŒ–FIRMSå½’ä¸€åŒ–å™¨
    print("ğŸ”§ åˆå§‹åŒ–FIRMSå½’ä¸€åŒ–å™¨...")
    firms_normalizer = FIRMSNormalizer(
        method='divide_by_100',
        firms_min=DATA_CONFIG['firms_min'],
        firms_max=DATA_CONFIG['firms_max']
    )
    
    # ä¸ºå½’ä¸€åŒ–æ‹Ÿåˆåˆ›å»ºä¸´æ—¶æ•°æ®åŠ è½½å™¨
    temp_loader = DataLoader(
        train_dataset, batch_size=512, shuffle=False, 
        num_workers=2, collate_fn=data_loader_obj.dataset.custom_collate_fn
    )
    firms_normalizer.fit(temp_loader)
    
    all_results = {}
    
    # ========== ç¬¬ä¸€é˜¶æ®µï¼šè®­ç»ƒæ ‡å‡†æ¨¡å‹ ==========
    if train_standard and MODEL_LIST_STANDARD:
        print(f"\n{'='*80}")
        print("ğŸš€ ç¬¬ä¸€é˜¶æ®µï¼šè®­ç»ƒæ ‡å‡†model_zooæ¨¡å‹")
        print(f"{'='*80}")
        
        # åˆ›å»ºæ ‡å‡†æ¨¡å‹æ•°æ®åŠ è½½å™¨
        standard_config = TRAINING_CONFIG['standard']
        train_loader = DataLoader(
            train_dataset, batch_size=standard_config['batch_size'], shuffle=True, 
            num_workers=4, collate_fn=data_loader_obj.dataset.custom_collate_fn, worker_init_fn=worker_init_fn
        )
        val_loader = DataLoader(
            val_dataset, batch_size=standard_config['batch_size'], shuffle=False,
            num_workers=4, collate_fn=data_loader_obj.dataset.custom_collate_fn, worker_init_fn=worker_init_fn
        )
        test_loader = DataLoader(
            test_dataset, batch_size=standard_config['batch_size'], shuffle=False,
            num_workers=4, collate_fn=data_loader_obj.dataset.custom_collate_fn, worker_init_fn=worker_init_fn
        )
        
        standard_results = train_and_test_models(
            MODEL_LIST_STANDARD, 'standard', device, train_loader, val_loader, test_loader, firms_normalizer, args.force_retrain
        )
        all_results['standard'] = standard_results
    
    # ========== ç¬¬äºŒé˜¶æ®µï¼šè®­ç»ƒ10xæ¨¡å‹ ==========
    if train_10x and MODEL_LIST_10X:
        print(f"\n{'='*80}")
        print("ğŸš€ ç¬¬äºŒé˜¶æ®µï¼šè®­ç»ƒ10xå‚æ•°model_zoo_10xæ¨¡å‹")
        print(f"{'='*80}")
        
        # åˆ›å»º10xæ¨¡å‹æ•°æ®åŠ è½½å™¨ï¼ˆè¾ƒå°batch sizeï¼‰
        config_10x = TRAINING_CONFIG['10x']
        train_loader_10x = DataLoader(
            train_dataset, batch_size=config_10x['batch_size'], shuffle=True, 
            num_workers=6, collate_fn=data_loader_obj.dataset.custom_collate_fn, worker_init_fn=worker_init_fn
        )
        val_loader_10x = DataLoader(
            val_dataset, batch_size=config_10x['batch_size'], shuffle=False,
            num_workers=4, collate_fn=data_loader_obj.dataset.custom_collate_fn, worker_init_fn=worker_init_fn
        )
        test_loader_10x = DataLoader(
            test_dataset, batch_size=config_10x['batch_size'], shuffle=False,
            num_workers=4, collate_fn=data_loader_obj.dataset.custom_collate_fn, worker_init_fn=worker_init_fn
        )
        
        results_10x = train_and_test_models(
            MODEL_LIST_10X, '10x', device, train_loader_10x, val_loader_10x, test_loader_10x, firms_normalizer, args.force_retrain
        )
        all_results['10x'] = results_10x
    
    # ========== æœ€ç»ˆæ€»ç»“ ==========
    print(f"\n{'='*80}")
    print("ğŸ‰ æ‰€æœ‰æ¨¡å‹å®éªŒå®Œæˆï¼")
    print(f"{'='*80}")
    
    for model_type, results in all_results.items():
        if results:
            df = pd.DataFrame(results)
            df = df.sort_values('f1', ascending=False)
            best_model = df.iloc[0]
            print(f"\nğŸ† æœ€ä½³{model_type}æ¨¡å‹: {best_model['model']}")
            print(f"   F1-Score: {best_model['f1']:.4f}")
            print(f"   Precision: {best_model['precision']:.4f}")
            print(f"   Recall: {best_model['recall']:.4f}")
            print(f"   PR-AUC: {best_model['pr_auc']:.4f}")
    
    print("\nğŸ“Š æ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ°ç›¸åº”çš„CSVæ–‡ä»¶ä¸­ï¼")

if __name__ == "__main__":
    main() 