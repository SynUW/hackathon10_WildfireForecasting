#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é‡ç«é¢„æµ‹æ¨¡å‹å…¨é¢å¯¹æ¯”å®éªŒ - 10å€å‚æ•°æ¨¡å‹ç‰ˆæœ¬
æ”¯æŒmodel_zoo_10xä¸­æ‰€æœ‰æ¨¡å‹çš„è®­ç»ƒã€early stoppingã€F1è¯„ä»·æŒ‡æ ‡ã€æœ€ä½³æ¨¡å‹æµ‹è¯•å’ŒCSVç»“æœå¯¼å‡º
"""

from dataload import TimeSeriesDataLoader, TimeSeriesPixelDataset, FullDatasetLoader
from torch.utils.data import Dataset, DataLoader, Subset
from model_adapter_10x import ModelAdapter, get_model_configs  # ä½¿ç”¨10xç‰ˆæœ¬çš„é€‚é…å™¨
import torch
import torch.nn as nn
import torch.optim as optim
from tqdm import tqdm
import wandb
import numpy as np
from sklearn.metrics import precision_score, recall_score, f1_score, average_precision_score
import os
import random
import importlib
from datetime import datetime, timedelta
import glob
import torch.nn.functional as F
import warnings
import pandas as pd
warnings.filterwarnings("ignore")

# =============================================================================
# é…ç½®å‚æ•°
# =============================================================================

# ä»model_zoo_10xæ–‡ä»¶å¤¹è·å–æ‰€æœ‰å¯ç”¨æ¨¡å‹
def get_all_models_10x():
    """è·å–model_zoo_10xä¸­æ‰€æœ‰å¯ç”¨çš„æ¨¡å‹"""
    model_files = []
    model_zoo_path = 'model_zoo_10x'
    if os.path.exists(model_zoo_path):
        for file in os.listdir(model_zoo_path):
            if file.endswith('.py') and not file.startswith('__') and file != 'trash':
                model_name = file[:-3]  # å»æ‰.pyåç¼€
                model_files.append(model_name)
    return sorted(model_files)

# æ‰€æœ‰å¯ç”¨10xæ¨¡å‹åˆ—è¡¨
MODEL_LIST_10X = get_all_models_10x()
print(f"å‘ç°å¯ç”¨10xæ¨¡å‹: {MODEL_LIST_10X}")

# è®­ç»ƒé…ç½® - é’ˆå¯¹10xæ¨¡å‹è°ƒæ•´
TRAINING_CONFIG = {
    'models': MODEL_LIST_10X,        # ä½¿ç”¨æ‰€æœ‰å¯ç”¨10xæ¨¡å‹
    'use_wandb': False,              # æ˜¯å¦ä½¿ç”¨wandb
    'seed': 42,                      # éšæœºç§å­
    'epochs': 50,                    # 10xæ¨¡å‹å‚æ•°å¤šï¼Œå‡å°‘è®­ç»ƒè½®æ•°
    'patience': 15,                  # Early stopping patienceï¼Œç¨å¾®å‡å°‘
    'batch_size': 256,               # 10xæ¨¡å‹æ˜¾å­˜å ç”¨å¤§ï¼Œå‡å°æ‰¹æ¬¡å¤§å°
    'learning_rate': 5e-4,          # å¤§æ¨¡å‹ä½¿ç”¨è¾ƒå°å­¦ä¹ ç‡
    'seq_len': 30,                   # è¾“å…¥åºåˆ—é•¿åº¦
    'pred_len': 7,                   # é¢„æµ‹åºåˆ—é•¿åº¦
    'weight_decay': 3e-4,           # æƒé‡è¡°å‡
    'T_0': 10,                      # ä½™å¼¦é€€ç«å‘¨æœŸ
    'T_mult': 1,                    # ä½™å¼¦é€€ç«å‘¨æœŸå€å¢å› å­
    'eta_min': 1e-6,                # æœ€å°å­¦ä¹ ç‡
    'max_grad_norm': 1.0,           # å¤§æ¨¡å‹ä½¿ç”¨æ¢¯åº¦è£å‰ª
    'focal_alpha': 0.5,             # ä½¿ç”¨æœ€ä½³çš„Focal Lossæ­£æ ·æœ¬æƒé‡
    'focal_gamma': 2.0,             # Focal Lossèšç„¦å‚æ•°
    'model_save_dir': '/mnt/raid/zhengsen/wildfire_dataset/self_built_materials/model_pth_10x',
    'results_save_path': 'model_comparison_results_10x.csv',
}

# æ•°æ®é…ç½®
DATA_CONFIG = {
    'train_years': [2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 
                   2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020],
    'val_years': [2021, 2022],
    'test_years': [2023, 2024],
    'positive_ratio': 1.0,          # ä½¿ç”¨å…¨éƒ¨æ­£æ ·æœ¬
    'pos_neg_ratio': 1.0,           # æ­£è´Ÿæ ·æœ¬æ¯”ä¾‹
    'resample_each_epoch': False,    # æ˜¯å¦æ¯epoché‡æ–°æŠ½æ ·
    'firms_min': 0,                 # FIRMSæ•°æ®æœ€å°å€¼ï¼ˆè·³è¿‡ç»Ÿè®¡ï¼‰
    'firms_max': 100,               # FIRMSæ•°æ®æœ€å¤§å€¼ï¼ˆè·³è¿‡ç»Ÿè®¡ï¼‰
}

# =============================================================================
# å·¥å…·å‡½æ•°
# =============================================================================

def set_seed(seed):
    """è®¾ç½®éšæœºç§å­"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    os.environ['PYTHONHASHSEED'] = str(seed)

def worker_init_fn(worker_id):
    """DataLoader workeråˆå§‹åŒ–å‡½æ•°ï¼Œç¡®ä¿å¤šè¿›ç¨‹çš„å¯é‡å¤æ€§"""
    worker_seed = torch.initial_seed() % 2**32
    np.random.seed(worker_seed)
    random.seed(worker_seed)

class FIRMSNormalizer:
    """FIRMSæ•°æ®å½’ä¸€åŒ–å™¨"""
    
    def __init__(self, method='log1p_minmax', firms_min=None, firms_max=None):
        self.method = method
        self.firms_min = firms_min
        self.firms_max = firms_max
        self.fitted = False
        
    def fit(self, data_loader):
        """æ‹Ÿåˆå½’ä¸€åŒ–å™¨"""
        if self.firms_min is not None and self.firms_max is not None:
            print(f"ğŸš€ ä½¿ç”¨æŒ‡å®šçš„FIRMSæ•°æ®èŒƒå›´: [{self.firms_min}, {self.firms_max}]")
            if self.method == 'log1p_minmax':
                self.global_min = np.log1p(self.firms_min)
                self.global_max = np.log1p(self.firms_max)
            else:
                self.global_min = self.firms_min
                self.global_max = self.firms_max
            self.fitted = True
            print(f"âœ… å½’ä¸€åŒ–å™¨åˆå§‹åŒ–å®Œæˆ (å˜æ¢åèŒƒå›´: {self.global_min:.2f}-{self.global_max:.2f})")
            return
            
        print("ğŸ”§ æ”¶é›†FIRMSæ•°æ®è¿›è¡Œå½’ä¸€åŒ–æ‹Ÿåˆ...")
        firms_values = []
        
        for batch in tqdm(data_loader, desc="æ”¶é›†FIRMSæ•°æ®"):
            past, future, _ = batch
            firms_data = past[:, 0, :]  # FIRMSé€šé“ (B, T)
            firms_values.append(firms_data.numpy())
        
        all_firms = np.concatenate(firms_values, axis=0).flatten()
        valid_firms = all_firms[all_firms != 255]  # è¿‡æ»¤æ‰NoDataå€¼(255)
        
        if self.method == 'log1p_minmax':
            log_firms = np.log1p(valid_firms)
            self.global_min = log_firms.min()
            self.global_max = log_firms.max()
        else:
            self.global_min = valid_firms.min()
            self.global_max = valid_firms.max()
            
        self.fitted = True
        print(f"âœ… {self.method.upper()}å½’ä¸€åŒ–å®Œæˆ (èŒƒå›´: {self.global_min:.2f}-{self.global_max:.2f})")
        
    def normalize(self, firms_data):
        """å½’ä¸€åŒ–FIRMSæ•°æ®"""
        if not self.fitted:
            raise ValueError("å½’ä¸€åŒ–å™¨å°šæœªæ‹Ÿåˆï¼Œè¯·å…ˆè°ƒç”¨fit()æ–¹æ³•")
            
        if self.method == 'log1p_minmax':
            log1p_data = torch.log1p(firms_data)
            if self.global_max > self.global_min:
                return (log1p_data - self.global_min) / (self.global_max - self.global_min)
            else:
                return log1p_data
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„å½’ä¸€åŒ–æ–¹æ³•: {self.method}")

def normalize_batch(past, future, firms_normalizer=None):
    """å¯¹æ‰¹æ¬¡æ•°æ®è¿›è¡Œå½’ä¸€åŒ–å¤„ç†"""
    if firms_normalizer is not None:
        past[:, 0, :] = firms_normalizer.normalize(past[:, 0, :])
    return past, future

def load_model_10x(model_name, configs):
    """åŠ¨æ€åŠ è½½10xæ¨¡å‹"""
    try:
        import sys
        model_zoo_path = os.path.join(os.getcwd(), 'model_zoo_10x')
        if model_zoo_path not in sys.path:
            sys.path.insert(0, model_zoo_path)
        
        module = importlib.import_module(f'model_zoo_10x.{model_name}')
        Model = getattr(module, 'Model')
        
        return Model(configs), 'standard'
    except Exception as e:
        print(f"åŠ è½½10xæ¨¡å‹ {model_name} å¤±è´¥: {e}")
        raise

def calculate_detailed_metrics(output, target):
    """è®¡ç®—è¯¦ç»†çš„äºŒåˆ†ç±»æŒ‡æ ‡ï¼ŒåŒ…æ‹¬PR-AUC"""
    pred_probs = torch.sigmoid(output).view(-1).cpu().numpy()
    pred_binary = (pred_probs > 0.5).astype(int)
    target_np = target.view(-1).cpu().numpy()
    
    unique_targets = np.unique(target_np)
    if len(unique_targets) < 2:
        return 0.0, 0.0, 0.0, 0.0
    
    try:
        precision = precision_score(target_np, pred_binary, average='binary', zero_division=0)
        recall = recall_score(target_np, pred_binary, average='binary', zero_division=0)
        f1 = f1_score(target_np, pred_binary, average='binary', zero_division=0)
        pr_auc = average_precision_score(target_np, pred_probs)
    except Exception as e:
        print(f"è®¡ç®—æŒ‡æ ‡æ—¶å‡ºé”™: {e}")
        return 0.0, 0.0, 0.0, 0.0
    
    return precision, recall, f1, pr_auc

class Config:
    """é…ç½®ç±» - ä½¿ç”¨10xé…ç½®"""
    def __init__(self, model_name):
        self.epochs = TRAINING_CONFIG['epochs']
        self.batch_size = TRAINING_CONFIG['batch_size']
        self.learning_rate = TRAINING_CONFIG['learning_rate']
        self.seq_len = TRAINING_CONFIG['seq_len']
        self.pred_len = TRAINING_CONFIG['pred_len']
        
        # è·å–10xæ¨¡å‹é…ç½®
        model_configs = get_model_configs(model_name)
        for key, value in model_configs.items():
            setattr(self, key, value)
        
        # å¼ºåˆ¶ä½¿ç”¨ç»Ÿä¸€é…ç½®
        self.seq_len = TRAINING_CONFIG['seq_len']
        self.pred_len = TRAINING_CONFIG['pred_len']
        
        # è®­ç»ƒå‚æ•°
        self.weight_decay = TRAINING_CONFIG['weight_decay']
        self.T_0 = TRAINING_CONFIG['T_0']
        self.T_mult = TRAINING_CONFIG['T_mult']
        self.eta_min = TRAINING_CONFIG['eta_min']
        self.max_grad_norm = TRAINING_CONFIG['max_grad_norm']
        
        # FIRMSæ•°æ®å½’ä¸€åŒ–å‚æ•°
        self.normalize_firms = True
        self.firms_normalization_method = 'log1p_minmax'
        self.binarization_threshold = 0.0
        self.firms_min = DATA_CONFIG['firms_min']
        self.firms_max = DATA_CONFIG['firms_max']
        
        # Focal Losså‚æ•°  
        self.focal_alpha = TRAINING_CONFIG['focal_alpha']
        self.focal_gamma = TRAINING_CONFIG['focal_gamma']
        
        # æ•°æ®é›†åˆ’åˆ†
        self.train_years = DATA_CONFIG['train_years']
        self.val_years = DATA_CONFIG['val_years']
        self.test_years = DATA_CONFIG['test_years']

class FocalLoss(nn.Module):
    """Focal Loss for addressing class imbalance"""
    def __init__(self, alpha=0.25, gamma=2.0, reduction='mean'):
        super(FocalLoss, self).__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.reduction = reduction
        
    def forward(self, inputs, targets):
        p = torch.sigmoid(inputs)
        ce_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')
        p_t = p * targets + (1 - p) * (1 - targets)
        alpha_t = self.alpha * targets + (1 - self.alpha) * (1 - targets)
        focal_weight = (1 - p_t) ** self.gamma
        focal_loss = alpha_t * focal_weight * ce_loss
        
        if self.reduction == 'mean':
            return focal_loss.mean()
        elif self.reduction == 'sum':
            return focal_loss.sum()
        else:
            return focal_loss

class EarlyStopping:
    """Early stopping utility"""
    def __init__(self, patience=7, min_delta=0, restore_best_weights=True):
        self.patience = patience
        self.min_delta = min_delta
        self.restore_best_weights = restore_best_weights
        self.best_score = None
        self.counter = 0
        self.best_weights = None
        
    def __call__(self, score, model):
        if self.best_score is None:
            self.best_score = score
            self.save_checkpoint(model)
        elif score < self.best_score + self.min_delta:
            self.counter += 1
            if self.counter >= self.patience:
                if self.restore_best_weights:
                    model.load_state_dict(self.best_weights)
                return True
        else:
            self.best_score = score
            self.counter = 0
            self.save_checkpoint(model)
        return False
    
    def save_checkpoint(self, model):
        self.best_weights = model.state_dict().copy()

# =============================================================================
# æ ¸å¿ƒè®­ç»ƒå’Œæµ‹è¯•å‡½æ•°
# =============================================================================

def train_single_model_10x(model_name, device, train_loader, val_loader, test_loader, firms_normalizer):
    """è®­ç»ƒå•ä¸ª10xæ¨¡å‹"""
    print(f"\n{'='*60}")
    print(f"ğŸ”¥ å¼€å§‹è®­ç»ƒ10xæ¨¡å‹: {model_name}")
    print(f"{'='*60}")
    
    # åˆ›å»ºé…ç½®å’Œæ¨¡å‹
    config = Config(model_name)
    adapter = ModelAdapter(config)
    
    try:
        model, model_type = load_model_10x(model_name, config)
        model = model.to(device)
    except Exception as e:
        print(f"âŒ 10xæ¨¡å‹ {model_name} åŠ è½½å¤±è´¥: {e}")
        return None
    
    # æ‰“å°æ¨¡å‹ä¿¡æ¯
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"ğŸ“ˆ 10xæ¨¡å‹å‚æ•°: æ€»è®¡ {total_params:,}, å¯è®­ç»ƒ {trainable_params:,}")
    
    # åˆå§‹åŒ–è®­ç»ƒç»„ä»¶
    criterion = FocalLoss(alpha=config.focal_alpha, gamma=config.focal_gamma)
    optimizer = optim.AdamW(model.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay)
    lr_scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(
        optimizer, T_0=config.T_0, T_mult=config.T_mult, eta_min=config.eta_min
    )
    early_stopping = EarlyStopping(patience=TRAINING_CONFIG['patience'], min_delta=0.001)
    
    best_f1 = 0.0
    best_model_path = None
    
    print("ğŸš€ å¼€å§‹è®­ç»ƒ...")
    
    # è®­ç»ƒå¾ªç¯
    for epoch in range(config.epochs):
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        train_loss = 0
        train_preds = []
        train_targets = []
        
        for batch in tqdm(train_loader, desc=f'Train {epoch+1}/{config.epochs}', leave=False):
            past, future, metadata_list = batch
            past, future = past.to(device), future.to(device)
            
            if firms_normalizer is not None:
                past, future = normalize_batch(past, future, firms_normalizer)
            
            date_strings = [str(int(metadata[0])) for metadata in metadata_list]
            
            future_truncated = future[:, :, :config.pred_len].transpose(1, 2)
            target = future_truncated[:, :, 0]
            target = (target > config.binarization_threshold).float()
            
            # å‰å‘ä¼ æ’­ - 10xæ¨¡å‹æ”¯æŒ
            if model_name == 's_mamba':
                past_transposed = past.transpose(1, 2)
                past_truncated = past_transposed[:, -config.seq_len:, :]
                output = model(past_truncated, date_strings)
            else:
                x_enc, x_mark_enc, x_dec, x_mark_dec = adapter.adapt_inputs(past, future, date_strings)
                x_enc, x_mark_enc, x_dec, x_mark_dec = x_enc.to(device), x_mark_enc.to(device), x_dec.to(device), x_mark_dec.to(device)
                output = model(x_enc, x_mark_enc, x_dec, x_mark_dec)
            
            output_channel_0 = output[:, :, 0]
            loss = criterion(output_channel_0, target)
            
            optimizer.zero_grad()
            loss.backward()
            if config.max_grad_norm > 0:
                torch.nn.utils.clip_grad_norm_(model.parameters(), config.max_grad_norm)
            optimizer.step()
            
            train_loss += loss.item()
            train_preds.append(output_channel_0.detach())
            train_targets.append(target.detach())
        
        # è®¡ç®—è®­ç»ƒæŒ‡æ ‡
        train_loss /= len(train_loader)
        train_preds = torch.cat(train_preds, dim=0)
        train_targets = torch.cat(train_targets, dim=0)
        train_precision, train_recall, train_f1, train_pr_auc = calculate_detailed_metrics(train_preds, train_targets)
        
        # éªŒè¯é˜¶æ®µ
        model.eval()
        val_loss = 0
        val_preds = []
        val_targets = []
        
        with torch.no_grad():
            for batch in tqdm(val_loader, desc=f'Val {epoch+1}/{config.epochs}', leave=False):
                past, future, metadata_list = batch
                past, future = past.to(device), future.to(device)
                
                if firms_normalizer is not None:
                    past, future = normalize_batch(past, future, firms_normalizer)
                
                date_strings = [str(int(metadata[0])) for metadata in metadata_list]
                
                future_truncated = future[:, :, :config.pred_len].transpose(1, 2)
                target = future_truncated[:, :, 0]
                target = (target > config.binarization_threshold).float()
                
                if model_name == 's_mamba':
                    past_transposed = past.transpose(1, 2)
                    past_truncated = past_transposed[:, -config.seq_len:, :]
                    output = model(past_truncated, date_strings)
                else:
                    x_enc, x_mark_enc, x_dec, x_mark_dec = adapter.adapt_inputs(past, future, date_strings)
                    x_enc, x_mark_enc, x_dec, x_mark_dec = x_enc.to(device), x_mark_enc.to(device), x_dec.to(device), x_mark_dec.to(device)
                    output = model(x_enc, x_mark_enc, x_dec, x_mark_dec)
                
                output_channel_0 = output[:, :, 0]
                loss = criterion(output_channel_0, target)
                val_loss += loss.item()
                
                val_preds.append(output_channel_0.detach())
                val_targets.append(target.detach())
        
        # è®¡ç®—éªŒè¯æŒ‡æ ‡
        val_loss /= len(val_loader)
        val_preds = torch.cat(val_preds, dim=0)
        val_targets = torch.cat(val_targets, dim=0)
        val_precision, val_recall, val_f1, val_pr_auc = calculate_detailed_metrics(val_preds, val_targets)
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_f1 > best_f1:
            best_f1 = val_f1
            model_path = os.path.join(TRAINING_CONFIG['model_save_dir'], f'{model_name}_best_f1.pth')
            os.makedirs(TRAINING_CONFIG['model_save_dir'], exist_ok=True)
            torch.save(model.state_dict(), model_path)
            best_model_path = model_path
        
        # æ‰“å°è¿›åº¦
        print(f'Epoch {epoch+1:3d}/{config.epochs} | Train: Loss={train_loss:.4f}, F1={train_f1:.4f} | '
              f'Val: Loss={val_loss:.4f}, P={val_precision:.4f}, R={val_recall:.4f}, F1={val_f1:.4f}, PR-AUC={val_pr_auc:.4f} | '
              f'LR={optimizer.param_groups[0]["lr"]:.2e}')
        
        # Early stoppingæ£€æŸ¥
        if early_stopping(val_f1, model):
            print(f"â¹ï¸  Early stopping triggered at epoch {epoch+1}")
            break
        
        lr_scheduler.step()
    
    return best_model_path, best_f1

def test_model_10x(model_name, model_path, device, test_loader, firms_normalizer):
    """æµ‹è¯•10xæ¨¡å‹"""
    print(f"\nğŸ“Š æµ‹è¯•10xæ¨¡å‹: {model_name}")
    
    config = Config(model_name)
    adapter = ModelAdapter(config)
    
    try:
        model, _ = load_model_10x(model_name, config)
        model.load_state_dict(torch.load(model_path))
        model = model.to(device)
        model.eval()
    except Exception as e:
        print(f"âŒ 10xæ¨¡å‹ {model_name} æµ‹è¯•åŠ è½½å¤±è´¥: {e}")
        return None
    
    test_preds = []
    test_targets = []
    
    with torch.no_grad():
        for batch in tqdm(test_loader, desc=f'Testing {model_name}', leave=False):
            past, future, metadata_list = batch
            past, future = past.to(device), future.to(device)
            
            if firms_normalizer is not None:
                past, future = normalize_batch(past, future, firms_normalizer)
            
            date_strings = [str(int(metadata[0])) for metadata in metadata_list]
            
            future_truncated = future[:, :, :config.pred_len].transpose(1, 2)
            target = future_truncated[:, :, 0]
            target = (target > config.binarization_threshold).float()
            
            if model_name == 's_mamba':
                past_transposed = past.transpose(1, 2)
                past_truncated = past_transposed[:, -config.seq_len:, :]
                output = model(past_truncated, date_strings)
            else:
                x_enc, x_mark_enc, x_dec, x_mark_dec = adapter.adapt_inputs(past, future, date_strings)
                x_enc, x_mark_enc, x_dec, x_mark_dec = x_enc.to(device), x_mark_enc.to(device), x_dec.to(device), x_mark_dec.to(device)
                output = model(x_enc, x_mark_enc, x_dec, x_mark_dec)
            
            output_channel_0 = output[:, :, 0]
            test_preds.append(output_channel_0.detach())
            test_targets.append(target.detach())
    
    # è®¡ç®—æµ‹è¯•æŒ‡æ ‡
    test_preds = torch.cat(test_preds, dim=0)
    test_targets = torch.cat(test_targets, dim=0)
    precision, recall, f1, pr_auc = calculate_detailed_metrics(test_preds, test_targets)
    
    print(f"âœ… {model_name} 10xæ¨¡å‹æµ‹è¯•ç»“æœ: P={precision:.4f}, R={recall:.4f}, F1={f1:.4f}, PR-AUC={pr_auc:.4f}")
    
    return {
        'model': model_name,
        'precision': precision,
        'recall': recall,
        'f1': f1,
        'pr_auc': pr_auc
    }

def main():
    """ä¸»å‡½æ•° - è®­ç»ƒæ‰€æœ‰10xæ¨¡å‹å¹¶æµ‹è¯•"""
    print("ğŸ”¥ é‡ç«é¢„æµ‹10xæ¨¡å‹å…¨é¢å¯¹æ¯”å®éªŒ")
    print(f"ğŸ“‹ å°†è®­ç»ƒ {len(MODEL_LIST_10X)} ä¸ª10xæ¨¡å‹")
    print(f"ğŸ“Š 10xæ¨¡å‹åˆ—è¡¨: {', '.join(MODEL_LIST_10X)}")
    
    # åˆå§‹åŒ–
    set_seed(TRAINING_CONFIG['seed'])
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ–¥ï¸  ä½¿ç”¨è®¾å¤‡: {device}")
    
    # æ£€æŸ¥GPUå†…å­˜
    if torch.cuda.is_available():
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
        print(f"ğŸ’¾ GPUå†…å­˜: {gpu_memory:.1f} GB (10xæ¨¡å‹éœ€è¦æ›´å¤šå†…å­˜)")
    
    # æ•°æ®åŠ è½½
    print("ğŸ“‚ åŠ è½½æ•°æ®...")
    data_loader = TimeSeriesDataLoader(
        h5_dir='/mnt/raid/zhengsen/wildfire_dataset/self_built_materials/pixel_samples_merged',
        positive_ratio=DATA_CONFIG['positive_ratio'],
        pos_neg_ratio=DATA_CONFIG['pos_neg_ratio'],
        resample_each_epoch=DATA_CONFIG['resample_each_epoch']
    )
    
    # æ•°æ®é›†åˆ’åˆ†
    train_indices, val_indices, test_indices = data_loader.get_year_based_split(
        train_years=DATA_CONFIG['train_years'],
        val_years=DATA_CONFIG['val_years'],
        test_years=DATA_CONFIG['test_years']
    )
    
    train_dataset = Subset(data_loader.dataset, train_indices)
    val_dataset = Subset(data_loader.dataset, val_indices)
    test_dataset = Subset(data_loader.dataset, test_indices)
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨ - 10xæ¨¡å‹ä½¿ç”¨è¾ƒå°‘çš„worker
    train_loader = DataLoader(
        train_dataset, batch_size=TRAINING_CONFIG['batch_size'], shuffle=True, 
        num_workers=8, collate_fn=data_loader.dataset.custom_collate_fn, worker_init_fn=worker_init_fn
    )
    val_loader = DataLoader(
        val_dataset, batch_size=TRAINING_CONFIG['batch_size'], shuffle=False,
        num_workers=4, collate_fn=data_loader.dataset.custom_collate_fn, worker_init_fn=worker_init_fn
    )
    test_loader = DataLoader(
        test_dataset, batch_size=TRAINING_CONFIG['batch_size'], shuffle=False,
        num_workers=4, collate_fn=data_loader.dataset.custom_collate_fn, worker_init_fn=worker_init_fn
    )
    
    print(f"ğŸ“Š æ•°æ®é›†å¤§å°: è®­ç»ƒé›† {len(train_dataset)}, éªŒè¯é›† {len(val_dataset)}, æµ‹è¯•é›† {len(test_dataset)}")
    
    # åˆå§‹åŒ–FIRMSå½’ä¸€åŒ–å™¨
    print("ğŸ”§ åˆå§‹åŒ–FIRMSå½’ä¸€åŒ–å™¨...")
    firms_normalizer = FIRMSNormalizer(
        method='log1p_minmax',
        firms_min=DATA_CONFIG['firms_min'],
        firms_max=DATA_CONFIG['firms_max']
    )
    firms_normalizer.fit(train_loader)
    
    # è®­ç»ƒæ‰€æœ‰10xæ¨¡å‹
    model_results = []
    failed_models = []
    
    for i, model_name in enumerate(MODEL_LIST_10X):
        print(f"\nğŸ”„ è¿›åº¦: {i+1}/{len(MODEL_LIST_10X)}")
        try:
            result = train_single_model_10x(
                model_name, device, train_loader, val_loader, test_loader, firms_normalizer
            )
            if result is not None:
                best_model_path, best_f1 = result
                print(f"âœ… {model_name} 10xæ¨¡å‹è®­ç»ƒå®Œæˆï¼Œæœ€ä½³éªŒè¯F1: {best_f1:.4f}")
                model_results.append((model_name, best_model_path))
            else:
                failed_models.append(model_name)
        except Exception as e:
            print(f"âŒ {model_name} 10xæ¨¡å‹è®­ç»ƒå¤±è´¥: {e}")
            failed_models.append(model_name)
            # æ¸…ç†GPUå†…å­˜
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
    
    print(f"\nğŸ“ˆ 10xæ¨¡å‹è®­ç»ƒå®Œæˆ! æˆåŠŸ: {len(model_results)}, å¤±è´¥: {len(failed_models)}")
    if failed_models:
        print(f"âŒ å¤±è´¥çš„10xæ¨¡å‹: {', '.join(failed_models)}")
    
    # æµ‹è¯•æ‰€æœ‰æˆåŠŸè®­ç»ƒçš„10xæ¨¡å‹
    print(f"\nğŸ§ª å¼€å§‹æµ‹è¯• {len(model_results)} ä¸ª10xæ¨¡å‹...")
    test_results = []
    
    for model_name, model_path in model_results:
        try:
            result = test_model_10x(model_name, model_path, device, test_loader, firms_normalizer)
            if result:
                test_results.append(result)
        except Exception as e:
            print(f"âŒ {model_name} 10xæ¨¡å‹æµ‹è¯•å¤±è´¥: {e}")
            # æ¸…ç†GPUå†…å­˜
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
    
    # ä¿å­˜ç»“æœåˆ°CSV
    if test_results:
        df = pd.DataFrame(test_results)
        df = df.sort_values('f1', ascending=False)  # æŒ‰F1åˆ†æ•°æ’åº
        df.to_csv(TRAINING_CONFIG['results_save_path'], index=False)
        
        print(f"\nğŸ“Š 10xæ¨¡å‹æœ€ç»ˆæµ‹è¯•ç»“æœå·²ä¿å­˜åˆ°: {TRAINING_CONFIG['results_save_path']}")
        print("\nğŸ† 10xæ¨¡å‹æ’è¡Œæ¦œ (æŒ‰F1åˆ†æ•°æ’åº):")
        print(df.to_string(index=False, float_format='%.4f'))
        
        # æ˜¾ç¤ºæœ€ä½³10xæ¨¡å‹
        best_model = df.iloc[0]
        print(f"\nğŸ¥‡ æœ€ä½³10xæ¨¡å‹: {best_model['model']}")
        print(f"   F1-Score: {best_model['f1']:.4f}")
        print(f"   Precision: {best_model['precision']:.4f}")
        print(f"   Recall: {best_model['recall']:.4f}")
        print(f"   PR-AUC: {best_model['pr_auc']:.4f}")
    
    print("\nğŸ‰ æ‰€æœ‰10xæ¨¡å‹å®éªŒå®Œæˆ!")

if __name__ == "__main__":
    main() 