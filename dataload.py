import os
import h5py
import torch
import numpy as np
from torch.utils.data import Dataset, DataLoader
from tqdm import tqdm
import logging
import re
from datetime import datetime
from torch.nn.utils.rnn import pad_sequence
import glob
import random
import math

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class TimeSeriesPixelDataset(Dataset):
    """
    é€‚é…merge_pixel_samples.pyç”Ÿæˆçš„H5æ–‡ä»¶çš„æ•°æ®åŠ è½½å™¨
    
    æ•°æ®æ ¼å¼:
    - æ–‡ä»¶å: {year}_samples.h5 (æŠ½æ ·æ•°æ®) æˆ– {year}_full.h5 (å®Œæ•´æ•°æ®)
    - æ•°æ®é›†å: YYYYMMDD_{past/future}_{firms_value}_row_col (åˆå¹¶åçš„æ•°æ®)
    - æ•°æ®å½¢çŠ¶: (total_bands, total_time_steps) å…¶ä¸­ total_time_steps = past_days + future_days
    - æ•°æ®ç±»å‹: float32 (å·²æ ‡å‡†åŒ–)
    - é»˜è®¤é…ç½®: past_days=365, future_days=30, total_bands=39
    
    ç‰¹æ€§: 
    - é»˜è®¤åˆ†ç¦»è¿‡å»å’Œæœªæ¥æ•°æ®ï¼Œè¿”å› (past_data, future_data)
    - æ•°æ®å·²ç»æ ‡å‡†åŒ–ï¼Œæ— éœ€é¢å¤–å¤„ç†
    - æ”¯æŒæŠ½æ ·æ•°æ®å’Œå®Œæ•´æ•°æ®çš„åŒºåˆ†
    - æ”¯æŒæŒ‰å¹´ä»½è¿›è¡Œè®­ç»ƒ/éªŒè¯/æµ‹è¯•åˆ’åˆ†
    - è¦æ±‚æ‰€æœ‰æ•°æ®å°ºå¯¸ä¸€è‡´ï¼Œä¸ä¸€è‡´åˆ™æŠ¥é”™
    - æ”¯æŒæ¯epoché‡æ–°æŠ½æ ·çš„åŠ¨æ€æ ·æœ¬é€‰æ‹©
    """
    
    def __init__(self, h5_dir, years=None, firms_values=None, return_metadata=True, 
                 use_full_data=False, positive_ratio=1.0, pos_neg_ratio=1.0, 
                 resample_each_epoch=False, epoch_seed=None, verbose_sampling=True):
        """
        åˆå§‹åŒ–æ—¶é—´åºåˆ—åƒç´ æ•°æ®é›†
        
        Args:
            h5_dir: H5æ–‡ä»¶ç›®å½•
            years: è¦åŠ è½½çš„å¹´ä»½åˆ—è¡¨ï¼ŒNoneè¡¨ç¤ºåŠ è½½æ‰€æœ‰å¹´ä»½
            firms_values: è¦åŠ è½½çš„FIRMSå€¼åˆ—è¡¨ï¼ŒNoneè¡¨ç¤ºåŠ è½½æ‰€æœ‰å€¼
            return_metadata: æ˜¯å¦è¿”å›å…ƒæ•°æ®ï¼ˆæ—¥æœŸã€åæ ‡ã€FIRMSå€¼ç­‰ï¼‰
            use_full_data: æ˜¯å¦ä½¿ç”¨å®Œæ•´æ•°æ®ï¼ˆTrue: {year}_full.h5, False: {year}_samples.h5ï¼‰
            positive_ratio: æ­£æ ·æœ¬ä½¿ç”¨æ¯”ä¾‹ï¼Œæ§åˆ¶é€‰å–çš„æ­£æ ·æœ¬æ•°å æ€»æ­£æ ·æœ¬æ•°çš„æ¯”ä¾‹ (0.0-1.0)
            pos_neg_ratio: æ­£è´Ÿæ ·æœ¬æ¯”ä¾‹ï¼Œå³è´Ÿæ ·æœ¬æ•° = æ­£æ ·æœ¬æ•° Ã— pos_neg_ratio
            resample_each_epoch: æ˜¯å¦åœ¨æ¯ä¸ªepoché‡æ–°è¿›è¡Œæ ·æœ¬æŠ½æ ·
            epoch_seed: å½“å‰epochçš„éšæœºç§å­ï¼Œç”¨äºæ§åˆ¶é‡é‡‡æ ·
        """
        self.h5_dir = h5_dir
        self.years = years
        self.firms_values = firms_values
        self.return_metadata = return_metadata
        self.use_full_data = use_full_data
        self.positive_ratio = positive_ratio
        self.pos_neg_ratio = pos_neg_ratio
        self.resample_each_epoch = resample_each_epoch
        self.epoch_seed = epoch_seed
        self.verbose_sampling = verbose_sampling
        
        # è·å–H5æ–‡ä»¶åˆ—è¡¨
        self.h5_files = self._get_h5_files()
        
        # æ„å»ºæ ·æœ¬ç´¢å¼•
        self.full_sample_index = []  # å­˜å‚¨æ‰€æœ‰æ ·æœ¬çš„å®Œæ•´ç´¢å¼•
        self.sample_index = []  # å½“å‰ä½¿ç”¨çš„æ ·æœ¬ç´¢å¼•
        self.dataset_info = {}  # å­˜å‚¨æ•°æ®é›†ä¿¡æ¯
        
        self._build_index()
        
        # åˆå§‹çš„æ ·æœ¬æ¯”ä¾‹ç­›é€‰
        if positive_ratio < 1.0 or pos_neg_ratio != 1.0:
            if resample_each_epoch:
                if self.verbose_sampling:
                    logger.info("å¯ç”¨æ¯epoché‡æ–°æŠ½æ ·æ¨¡å¼")
                # å¦‚æœå¯ç”¨æ¯epoché‡æ–°æŠ½æ ·ï¼Œä¿å­˜å®Œæ•´ç´¢å¼•å¹¶è¿›è¡Œåˆå§‹æŠ½æ ·
                self.full_sample_index = self.sample_index.copy()
                self._apply_sample_ratio_filtering()
            else:
                # ä¼ ç»Ÿæ¨¡å¼ï¼šä¸€æ¬¡æ€§æŠ½æ ·
                self._apply_sample_ratio_filtering()
        
        logger.info(f"æ•°æ®é›†åˆå§‹åŒ–å®Œæˆï¼Œå…± {len(self.sample_index)} ä¸ªæ ·æœ¬")
        logger.info(f"æ­£æ ·æœ¬ä½¿ç”¨æ¯”ä¾‹: {positive_ratio:.2f}, æ­£è´Ÿæ ·æœ¬æ¯”ä¾‹: 1:{pos_neg_ratio:.2f}")
        if resample_each_epoch:
            logger.info(f"å¯ç”¨æ¯epoché‡æ–°æŠ½æ ·ï¼Œæ€»æ ·æœ¬æ± : {len(self.full_sample_index)} ä¸ªæ ·æœ¬")
    
    def resample_for_epoch(self, epoch_seed):
        """
        ä¸ºæ–°çš„epoché‡æ–°è¿›è¡Œæ ·æœ¬æŠ½æ ·
        
        Args:
            epoch_seed: å½“å‰epochçš„éšæœºç§å­
        """
        if not self.resample_each_epoch:
            return
        
        if not hasattr(self, 'full_sample_index') or not self.full_sample_index:
            logger.warning("æ— æ³•é‡æ–°æŠ½æ ·ï¼šæ²¡æœ‰å®Œæ•´æ ·æœ¬ç´¢å¼•")
            return
        
        self.epoch_seed = epoch_seed
        
        # ä¸´æ—¶ä¿å­˜å½“å‰æ ·æœ¬ç´¢å¼•ï¼Œç”¨å®Œæ•´æ ·æœ¬ç´¢å¼•æ›¿æ¢
        temp_sample_index = self.sample_index
        self.sample_index = self.full_sample_index.copy()
        
        # ä½¿ç”¨æ–°çš„éšæœºç§å­é‡æ–°æŠ½æ ·
        self._apply_sample_ratio_filtering(seed=epoch_seed)
        
        # é™é»˜æ¨¡å¼ï¼Œä¸è¾“å‡ºæŠ½æ ·å®Œæˆä¿¡æ¯
        # if self.verbose_sampling:
        #     logger.info(f"Epoch {epoch_seed}: é‡æ–°æŠ½æ ·å®Œæˆï¼Œå½“å‰æ ·æœ¬æ•°: {len(self.sample_index)}")
    
    def get_current_sample_stats(self):
        """è·å–å½“å‰æ ·æœ¬çš„ç»Ÿè®¡ä¿¡æ¯"""
        positive_count = 0
        negative_count = 0
        
        for _, _, metadata in self.sample_index:
            if metadata['firms_value'] > 0:
                positive_count += 1
            else:
                negative_count += 1
        
        return {
            'total_samples': len(self.sample_index),
            'positive_samples': positive_count,
            'negative_samples': negative_count,
            'positive_ratio': positive_count / len(self.sample_index) if len(self.sample_index) > 0 else 0,
            'pos_neg_ratio': negative_count / positive_count if positive_count > 0 else 0
        }
    
    def custom_collate_fn(self, batch):
        """
        è‡ªå®šä¹‰çš„collateå‡½æ•°ï¼Œè¦æ±‚æ‰€æœ‰æ•°æ®å°ºå¯¸ä¸€è‡´ï¼Œä¸ä¸€è‡´åˆ™æŠ¥é”™
        é»˜è®¤åˆ†ç¦»è¿‡å»å’Œæœªæ¥æ•°æ®
        """
        if self.return_metadata:
            past_data_list, future_data_list, metadata_list = zip(*batch)
        else:
            past_data_list, future_data_list = zip(*batch)
        
        # æ£€æŸ¥æ‰€æœ‰tensorçš„å½¢çŠ¶
        past_shapes = [data.shape for data in past_data_list]
        future_shapes = [data.shape for data in future_data_list]
        
        # æ£€æŸ¥pastæ•°æ®å½¢çŠ¶ä¸€è‡´æ€§
        if len(set(past_shapes)) > 1:
            raise ValueError(f"Pastæ•°æ®å½¢çŠ¶ä¸ä¸€è‡´: {set(past_shapes)}")
        
        # æ£€æŸ¥futureæ•°æ®å½¢çŠ¶ä¸€è‡´æ€§
        if len(set(future_shapes)) > 1:
            raise ValueError(f"Futureæ•°æ®å½¢çŠ¶ä¸ä¸€è‡´: {set(future_shapes)}")
        
        # ç›´æ¥å †å ï¼Œå½¢çŠ¶å¿…é¡»ä¸€è‡´
        past_batch = torch.stack(past_data_list, dim=0)
        future_batch = torch.stack(future_data_list, dim=0)
        
        if self.return_metadata:
            return past_batch, future_batch, metadata_list
        else:
            return past_batch, future_batch
    
    def _get_h5_files(self):
        """è·å–ç¬¦åˆæ¡ä»¶çš„H5æ–‡ä»¶åˆ—è¡¨"""
        h5_files = []
        
        # æ ¹æ®use_full_dataå‚æ•°é€‰æ‹©æ–‡ä»¶ç±»å‹
        file_suffix = '_full.h5' if self.use_full_data else '_samples.h5'
        
        # æ”¯æŒä¸¤ç§æ–‡ä»¶åæ ¼å¼:
        # 1. æ ‡å‡†æ ¼å¼: YYYY_samples.h5 æˆ– YYYY_full.h5
        # 2. æ‰©å±•æ ¼å¼: YYYY_MMDD_MMDD_full.h5 (ç”¨äºå®Œæ•´æ•°æ®)
        if self.use_full_data:
            patterns = [
                r'(\d{4})_full\.h5',           # æ ‡å‡†æ ¼å¼: 2024_full.h5
                r'(\d{4})_\d{4}_\d{4}_full\.h5'  # æ‰©å±•æ ¼å¼: 2024_0719_0725_full.h5
            ]
        else:
            patterns = [r'(\d{4})_samples\.h5']  # æŠ½æ ·æ•°æ®åªæ”¯æŒæ ‡å‡†æ ¼å¼
        
        for filename in os.listdir(self.h5_dir):
            if not filename.endswith(file_suffix):
                continue
                
            # å°è¯•åŒ¹é…æ‰€æœ‰æ¨¡å¼
            year = None
            for pattern in patterns:
                year_match = re.match(pattern, filename)
                if year_match:
                    year = int(year_match.group(1))
                    break
            
            if year is None:
                continue
            
            # æ£€æŸ¥å¹´ä»½è¿‡æ»¤æ¡ä»¶
            if self.years is not None and year not in self.years:
                continue
                
            h5_path = os.path.join(self.h5_dir, filename)
            h5_files.append((h5_path, year))
        
        data_type = "å®Œæ•´æ•°æ®" if self.use_full_data else "æŠ½æ ·æ•°æ®"
        logger.info(f"æ‰¾åˆ° {len(h5_files)} ä¸ª{data_type}æ–‡ä»¶")
        return h5_files
    
    def _build_index(self):
        """æ„å»ºæ ·æœ¬ç´¢å¼•"""
        logger.info("æ„å»ºæ ·æœ¬ç´¢å¼•...")
        
        for h5_path, year in tqdm(self.h5_files, desc="æ‰«æH5æ–‡ä»¶"):
            try:
                with h5py.File(h5_path, 'r') as f:
                    # è·å–æ•°æ®é›†ä¿¡æ¯
                    if h5_path not in self.dataset_info:
                        self.dataset_info[h5_path] = {
                            'year': year,
                            'total_bands': f.attrs.get('total_bands', 0),
                            'driver_names': f.attrs.get('driver_names', []),
                            'past_days': f.attrs.get('past_days', 0),
                            'future_days': f.attrs.get('future_days', 0),
                            'data_format': f.attrs.get('data_format', 'unknown')
                        }
                    
                    # æ‰«ææ‰€æœ‰æ•°æ®é›†
                    for dataset_key in f.keys():
                        # è§£ææ•°æ®é›†åç§°: YYYYMMDD_{past/future}_{firms_value}_row_col
                        metadata = self._parse_dataset_key(dataset_key)
                        if metadata is None:
                            continue
                        
                        # æ£€æŸ¥FIRMSå€¼è¿‡æ»¤æ¡ä»¶
                        if (self.firms_values is not None and 
                            metadata['firms_value'] not in self.firms_values):
                            continue
                        
                        # æ·»åŠ åˆ°ç´¢å¼•
                        self.sample_index.append((h5_path, dataset_key, metadata))
                        
            except Exception as e:
                logger.error(f"å¤„ç†æ–‡ä»¶ {h5_path} æ—¶å‡ºé”™: {str(e)}")
                continue
    
    def _parse_dataset_key(self, dataset_key):
        """
        è§£ææ•°æ®é›†é”®å
        æ”¯æŒä¸¤ç§æ ¼å¼:
        1. YYYYMMDD_{past/future}_{firms_value}_row_col
        2. YYYY_MM_DD_{past/future}_{firms_value}_row_col (å¸¦ä¸‹åˆ’çº¿çš„æ—¥æœŸæ ¼å¼)
        """
        try:
            # å°è¯•ç¬¬ä¸€ç§æ ¼å¼: YYYYMMDD_{past/future}_{firms_value}_row_col
            pattern1 = r'(\d{8})_(past|future)_(\d+(?:\.\d+)?)_(\d+)_(\d+)'
            match = re.match(pattern1, dataset_key)
            
            if match:
                date_str, time_type, firms_value_str, row_str, col_str = match.groups()
                return {
                    'date': datetime.strptime(date_str, '%Y%m%d'),
                    'date_int': int(date_str),
                    'time_type': time_type,
                    'firms_value': float(firms_value_str),
                    'row': int(row_str),
                    'col': int(col_str),
                    'pixel_coord': (int(row_str), int(col_str))
                }
            
            # å°è¯•ç¬¬äºŒç§æ ¼å¼: YYYY_MM_DD_{past/future}_{firms_value}_row_col
            pattern2 = r'(\d{4})_(\d{2})_(\d{2})_(past|future)_(\d+(?:\.\d+)?)_(\d+)_(\d+)'
            match = re.match(pattern2, dataset_key)
            
            if match:
                year_str, month_str, day_str, time_type, firms_value_str, row_str, col_str = match.groups()
                date_str = f"{year_str}{month_str}{day_str}"
            return {
                'date': datetime.strptime(date_str, '%Y%m%d'),
                    'date_int': int(date_str),
                'time_type': time_type,
                'firms_value': float(firms_value_str),
                'row': int(row_str),
                'col': int(col_str),
                'pixel_coord': (int(row_str), int(col_str))
            }
                
            return None
            
        except Exception as e:
            logger.debug(f"è§£ææ•°æ®é›†é”®åå¤±è´¥: {dataset_key}, é”™è¯¯: {str(e)}")
            return None
    
    def _is_valid_sample(self, sample_group):
        """éªŒè¯æ ·æœ¬æ•°æ®æ˜¯å¦æœ‰æ•ˆ"""
        try:
            # æ£€æŸ¥å¿…è¦çš„å±æ€§
            if not all(attr in sample_group.attrs for attr in ['year', 'driver']):
                return False
            
            # æ£€æŸ¥å¹´ä»½æ˜¯å¦ç¬¦åˆè¦æ±‚
            year = int(sample_group.attrs['year'])
            if self.years and year not in self.years:
                return False
            
            # æ£€æŸ¥æ•°æ®ç»´åº¦
            if 'data' not in sample_group:
                return False
                
            data = sample_group['data'][:]
            if len(data.shape) != 2:  # åº”è¯¥æ˜¯ (bands, time_steps)
                return False
                
            # æ ¹æ®æ ·æœ¬ç±»å‹æ£€æŸ¥æ—¶é—´æ­¥æ•°
            time_steps = data.shape[1]
            sample_id = sample_group.name.split('/')[-1]
            
            # è§£ææ•°æ®é›†åç§°æ ¼å¼: YYYYMMDD_{past/future}_{firms_value}_row_col
            parts = sample_id.split('_')
            if len(parts) < 4:
                return False
            
            data_type = parts[1]  # past æˆ– future
            
            # æ£€æŸ¥æ—¶é—´æ­¥æ•°
            if data_type == 'past' and time_steps != 365:
                return False
            elif data_type == 'future' and time_steps != 30:
                return False
            elif data_type not in ['past', 'future']:
                return False
            
            # å¦‚æœæŒ‡å®šäº†FIRMSå€¼è¿‡æ»¤ï¼Œæ£€æŸ¥FIRMSå€¼
            if self.firms_values:
                try:
                    firms_value = int(parts[2])
                    if firms_value not in self.firms_values:
                        return False
                except (ValueError, IndexError):
                    return False
                
            return True
            
        except Exception as e:
            logger.error(f"éªŒè¯æ ·æœ¬æ—¶å‡ºé”™: {str(e)}")
            return False
    
    def __len__(self):
        return len(self.sample_index)
    
    def __getitem__(self, idx):
        """
        è·å–å•ä¸ªæ ·æœ¬
        
        Returns:
            - past_data: (bands, past_time_steps) 
            - future_data: (bands, future_time_steps)
            - metadata (å¯é€‰): åŒ…å«æ—¥æœŸã€åæ ‡ç­‰ä¿¡æ¯
        """
        h5_path, dataset_key, metadata = self.sample_index[idx]
        
        try:
            with h5py.File(h5_path, 'r') as f:
                data = f[dataset_key][:]  # shape: (total_bands, time_steps)
                
                # ç¡®ä¿æ•°æ®æ˜¯2Dæ ¼å¼ (bands, time_steps)
                if data.ndim == 1:
                    data = data[np.newaxis, :]  # æ·»åŠ æ³¢æ®µç»´åº¦
                elif data.ndim > 2:
                    logger.warning(f"æ•°æ®ç»´åº¦å¼‚å¸¸: {data.shape}, æ•°æ®é›†: {dataset_key}")
                    data = data.reshape(data.shape[0], -1)  # å±•å¹³ä¸º2D
                
                # æ£€æŸ¥å¹¶å¤„ç†NaN/Infå€¼
                if np.isnan(data).any() or np.isinf(data).any():
                    nan_count = np.isnan(data).sum()
                    inf_count = np.isinf(data).sum()
                    # logger.debug(f"æ•°æ®é›† {dataset_key} åŒ…å« {nan_count} ä¸ªNaNå€¼å’Œ {inf_count} ä¸ªInfå€¼ï¼Œå·²æ›¿æ¢ä¸º0")
                    data = np.nan_to_num(data, nan=0.0, posinf=0.0, neginf=0.0)
                
                # è½¬æ¢ä¸ºtorch tensor
                data = torch.from_numpy(data).float()
                
                # æ ¹æ®æ•°æ®é›†ç±»å‹è¿”å›å¯¹åº”çš„æ•°æ®
                time_type = metadata['time_type']  # 'past' æˆ– 'future'
                
                if time_type == 'past':
                    # å¯¹äºpastæ•°æ®ï¼Œå¿…é¡»æ‰¾åˆ°å¯¹åº”çš„futureæ•°æ®
                    future_dataset_key = dataset_key.replace('_past_', '_future_')
                    if future_dataset_key not in f:
                        raise ValueError(f"Pastæ•°æ® {dataset_key} ç¼ºå°‘å¯¹åº”çš„Futureæ•°æ® {future_dataset_key}")
                    
                    future_data = f[future_dataset_key][:]
                    if future_data.ndim == 1:
                        future_data = future_data[np.newaxis, :]
                    elif future_data.ndim > 2:
                        future_data = future_data.reshape(future_data.shape[0], -1)
                    
                    # æ£€æŸ¥å¹¶å¤„ç†Futureæ•°æ®çš„NaN/Infå€¼
                    if np.isnan(future_data).any() or np.isinf(future_data).any():
                        nan_count = np.isnan(future_data).sum()
                        inf_count = np.isinf(future_data).sum()
                        logger.debug(f"Futureæ•°æ®é›† {future_dataset_key} åŒ…å« {nan_count} ä¸ªNaNå€¼å’Œ {inf_count} ä¸ªInfå€¼ï¼Œå·²æ›¿æ¢ä¸º0")
                        future_data = np.nan_to_num(future_data, nan=0.0, posinf=0.0, neginf=0.0)
                    
                    future_data = torch.from_numpy(future_data).float()
                    past_data = data
                    
                elif time_type == 'future':
                    # å¯¹äºfutureæ•°æ®ï¼Œå¿…é¡»æ‰¾åˆ°å¯¹åº”çš„pastæ•°æ®
                    past_dataset_key = dataset_key.replace('_future_', '_past_')
                    if past_dataset_key not in f:
                        raise ValueError(f"Futureæ•°æ® {dataset_key} ç¼ºå°‘å¯¹åº”çš„Pastæ•°æ® {past_dataset_key}")
                    
                    past_data = f[past_dataset_key][:]
                    if past_data.ndim == 1:
                        past_data = past_data[np.newaxis, :]
                    elif past_data.ndim > 2:
                        past_data = past_data.reshape(past_data.shape[0], -1)
                    
                    # æ£€æŸ¥å¹¶å¤„ç†Pastæ•°æ®çš„NaN/Infå€¼
                    if np.isnan(past_data).any() or np.isinf(past_data).any():
                        nan_count = np.isnan(past_data).sum()
                        inf_count = np.isinf(past_data).sum()
                        logger.debug(f"Pastæ•°æ®é›† {past_dataset_key} åŒ…å« {nan_count} ä¸ªNaNå€¼å’Œ {inf_count} ä¸ªInfå€¼ï¼Œå·²æ›¿æ¢ä¸º0")
                        past_data = np.nan_to_num(past_data, nan=0.0, posinf=0.0, neginf=0.0)
                    
                    past_data = torch.from_numpy(past_data).float()
                    future_data = data
                
                else:
                    raise ValueError(f"æœªçŸ¥çš„æ—¶é—´ç±»å‹: {time_type}")
                
                # éªŒè¯æ•°æ®å½¢çŠ¶çš„åˆç†æ€§ - ä»H5æ–‡ä»¶å±æ€§è·å–æœŸæœ›å€¼
                dataset_info = self.dataset_info.get(h5_path, {})
                expected_past_steps = dataset_info.get('past_days', 365)
                expected_future_steps = dataset_info.get('future_days', 30)
                
                if past_data.shape[1] != expected_past_steps:
                    raise ValueError(f"Pastæ•°æ®æ—¶é—´æ­¥æ•°é”™è¯¯: æœŸæœ›{expected_past_steps}, å®é™…{past_data.shape[1]}, æ•°æ®é›†: {dataset_key}")
                
                if future_data.shape[1] != expected_future_steps:
                    raise ValueError(f"Futureæ•°æ®æ—¶é—´æ­¥æ•°é”™è¯¯: æœŸæœ›{expected_future_steps}, å®é™…{future_data.shape[1]}, æ•°æ®é›†: {dataset_key}")
                
                if past_data.shape[0] != future_data.shape[0]:
                    raise ValueError(f"Pastå’ŒFutureæ•°æ®æ³¢æ®µæ•°ä¸åŒ¹é…: Past={past_data.shape[0]}, Future={future_data.shape[0]}, æ•°æ®é›†: {dataset_key}")
                
                # æœ€ç»ˆæ£€æŸ¥ï¼šç¡®ä¿è¿”å›çš„tensorä¸åŒ…å«NaN/Infå€¼
                if torch.isnan(past_data).any() or torch.isinf(past_data).any():
                    # logger.warning(f"Pastæ•°æ®ä»åŒ…å«NaN/Infå€¼ï¼Œå¼ºåˆ¶æ›¿æ¢ä¸º0: {dataset_key}")
                    past_data = torch.nan_to_num(past_data, nan=0.0, posinf=0.0, neginf=0.0)
                
                if torch.isnan(future_data).any() or torch.isinf(future_data).any():
                    # logger.warning(f"Futureæ•°æ®ä»åŒ…å«NaN/Infå€¼ï¼Œå¼ºåˆ¶æ›¿æ¢ä¸º0: {dataset_key}")
                    future_data = torch.nan_to_num(future_data, nan=0.0, posinf=0.0, neginf=0.0)
                
                if self.return_metadata:
                    # è¿”å›ç®€åŒ–çš„metadataæ ¼å¼: [æ—¥æœŸ, xåæ ‡, yåæ ‡]
                    simplified_metadata = [metadata['date_int'], metadata['row'], metadata['col']]
                    return past_data, future_data, simplified_metadata
                else:
                    return past_data, future_data
                        
        except Exception as e:
            logger.error(f"è¯»å–æ ·æœ¬å¤±è´¥: {dataset_key}, é”™è¯¯: {str(e)}")
            raise e  # é‡æ–°æŠ›å‡ºå¼‚å¸¸ï¼Œä¸è¦æ©ç›–é—®é¢˜
    
    def get_dataset_info(self):
        """è·å–æ•°æ®é›†ä¿¡æ¯"""
        return self.dataset_info
    
    def get_sample_by_criteria(self, year=None, firms_value=None, date_range=None):
        """
        æ ¹æ®æ¡ä»¶ç­›é€‰æ ·æœ¬
        
        Args:
            year: å¹´ä»½
            firms_value: FIRMSå€¼
            date_range: æ—¥æœŸèŒƒå›´ (start_date, end_date)
        
        Returns:
            ç¬¦åˆæ¡ä»¶çš„æ ·æœ¬ç´¢å¼•åˆ—è¡¨
        """
        matching_indices = []
        
        for idx, (h5_path, dataset_key, metadata) in enumerate(self.sample_index):
            # æ£€æŸ¥å¹´ä»½
            if year is not None and self.dataset_info[h5_path]['year'] != year:
                continue
            
            # æ£€æŸ¥FIRMSå€¼
            if firms_value is not None and metadata['firms_value'] != firms_value:
                continue
            
            # æ£€æŸ¥æ—¥æœŸèŒƒå›´
            if date_range is not None:
                start_date, end_date = date_range
                if not (start_date <= metadata['date'] <= end_date):
                    continue
            
            matching_indices.append(idx)
        
        return matching_indices
    
    def get_statistics(self):
        """è·å–æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            'total_samples': len(self.sample_index),
            'years': set(),
            'firms_values': set(),
            'time_types': set(),
            'files': len(self.h5_files)
        }
        
        for h5_path, dataset_key, metadata in self.sample_index:
            stats['years'].add(self.dataset_info[h5_path]['year'])
            stats['firms_values'].add(metadata['firms_value'])
            stats['time_types'].add(metadata['time_type'])
        
        # è½¬æ¢ä¸ºåˆ—è¡¨å¹¶æ’åº
        stats['years'] = sorted(list(stats['years']))
        stats['firms_values'] = sorted(list(stats['firms_values']))
        stats['time_types'] = sorted(list(stats['time_types']))
        
        return stats

    def _apply_sample_ratio_filtering(self, seed=None, epoch_rotation_strategy=True):
        """
        åº”ç”¨æ­£è´Ÿæ ·æœ¬æ¯”ä¾‹ç­›é€‰
        
        Args:
            seed: éšæœºç§å­ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨å›ºå®šç§å­42
            epoch_rotation_strategy: æ˜¯å¦ä½¿ç”¨è½®æ¢ç­–ç•¥ç¡®ä¿æ•°æ®è¦†ç›–
        """
        if seed is None:
            seed = 42
        
        if self.verbose_sampling:
            logger.info(f"å¼€å§‹åº”ç”¨æ­£è´Ÿæ ·æœ¬æ¯”ä¾‹ç­›é€‰... (éšæœºç§å­: {seed})")
        
        # åˆ†ç¦»æ­£æ ·æœ¬å’Œè´Ÿæ ·æœ¬çš„ç´¢å¼•
        positive_samples = []
        negative_samples = []
        
        for idx, (h5_path, dataset_key, metadata) in enumerate(self.sample_index):
            if metadata['firms_value'] > 0:
                positive_samples.append(idx)
            else:
                negative_samples.append(idx)
        
        positive_count = len(positive_samples)
        negative_count = len(negative_samples)
        
        if self.verbose_sampling:
            logger.info(f"åŸå§‹æ ·æœ¬ç»Ÿè®¡: æ­£æ ·æœ¬ {positive_count} ä¸ª, è´Ÿæ ·æœ¬ {negative_count} ä¸ª")
        
        # è®¡ç®—éœ€è¦ä¿ç•™çš„æ­£æ ·æœ¬æ•°
        retained_positive_count = int(positive_count * self.positive_ratio)
        
        # è®¡ç®—éœ€è¦ä¿ç•™çš„è´Ÿæ ·æœ¬æ•°
        retained_negative_count = int(retained_positive_count * self.pos_neg_ratio)
        
        # ç¡®ä¿ä¸è¶…è¿‡å¯ç”¨çš„è´Ÿæ ·æœ¬æ•°
        retained_negative_count = min(retained_negative_count, negative_count)
        
        if self.verbose_sampling:
            logger.info(f"è®¡åˆ’ä¿ç•™: æ­£æ ·æœ¬ {retained_positive_count} ä¸ª, è´Ÿæ ·æœ¬ {retained_negative_count} ä¸ª")
        
        # ä½¿ç”¨æŒ‡å®šçš„éšæœºç§å­è¿›è¡ŒæŠ½æ ·
        random.seed(seed)
        
        if epoch_rotation_strategy and hasattr(self, 'epoch_seed') and self.resample_each_epoch and self.epoch_seed is not None:
            # è½®æ¢ç­–ç•¥ï¼šç¡®ä¿ç»è¿‡è¶³å¤Ÿepochåèƒ½è§åˆ°æ‰€æœ‰æ•°æ®
            selected_positive_indices = self._get_rotated_samples(positive_samples, retained_positive_count, self.epoch_seed)
            selected_negative_indices = self._get_rotated_samples(negative_samples, retained_negative_count, self.epoch_seed)
        else:
            # ä¼ ç»ŸéšæœºæŠ½æ ·
            selected_positive_indices = random.sample(positive_samples, retained_positive_count) if retained_positive_count < len(positive_samples) else positive_samples
            selected_negative_indices = random.sample(negative_samples, retained_negative_count) if retained_negative_count < len(negative_samples) else negative_samples
        
        # åˆå¹¶é€‰ä¸­çš„æ ·æœ¬ç´¢å¼•
        selected_indices = selected_positive_indices + selected_negative_indices
        
        # é‡æ–°æ„å»ºæ ·æœ¬ç´¢å¼•
        new_sample_index = []
        for idx in selected_indices:
            new_sample_index.append(self.sample_index[idx])
        
        self.sample_index = new_sample_index
        
        if self.verbose_sampling:
            logger.info(f"æ ·æœ¬ç­›é€‰å®Œæˆ:")
            logger.info(f"  å®é™…ä¿ç•™æ­£æ ·æœ¬: {len(selected_positive_indices)} ä¸ª")
            logger.info(f"  å®é™…ä¿ç•™è´Ÿæ ·æœ¬: {len(selected_negative_indices)} ä¸ª")
            logger.info(f"  æ€»æ ·æœ¬æ•°: {len(self.sample_index)} ä¸ª")
        
        # é¿å…é™¤é›¶é”™è¯¯
        if len(selected_positive_indices) > 0:
            ratio = len(selected_negative_indices) / len(selected_positive_indices)
            logger.info(f"  æ­£è´Ÿæ ·æœ¬æ¯”ä¾‹: 1:{ratio:.2f}")
        else:
            logger.info(f"  æ­£è´Ÿæ ·æœ¬æ¯”ä¾‹: æ— æ­£æ ·æœ¬")
    
    def _get_rotated_samples(self, sample_pool, target_count, epoch_seed):
        """
        è½®æ¢æŠ½æ ·ç­–ç•¥ï¼šç¡®ä¿ç»è¿‡è¶³å¤Ÿepochåèƒ½è§åˆ°æ‰€æœ‰æ•°æ®
        
        Args:
            sample_pool: æ ·æœ¬æ± ï¼ˆæ­£æ ·æœ¬æˆ–è´Ÿæ ·æœ¬çš„ç´¢å¼•åˆ—è¡¨ï¼‰
            target_count: ç›®æ ‡æŠ½æ ·æ•°é‡
            epoch_seed: å½“å‰epochçš„ç§å­
        
        Returns:
            é€‰ä¸­çš„æ ·æœ¬ç´¢å¼•åˆ—è¡¨
        """
        total_samples = len(sample_pool)
        
        if target_count >= total_samples:
            # å¦‚æœç›®æ ‡æ•°é‡å¤§äºç­‰äºæ€»æ ·æœ¬æ•°ï¼Œè¿”å›æ‰€æœ‰æ ·æœ¬
            return sample_pool
        
        if self.positive_ratio >= 1.0:
            # å¦‚æœä½¿ç”¨å…¨éƒ¨æ•°æ®ï¼Œç›´æ¥è¿”å›æ‰€æœ‰æ ·æœ¬
            return sample_pool
        
        # è®¡ç®—éœ€è¦å¤šå°‘ä¸ªepochæ‰èƒ½è¦†ç›–æ‰€æœ‰æ•°æ®
        epochs_for_full_coverage = math.ceil(total_samples / target_count)
        
        # æ ¹æ®å½“å‰epochç¡®å®šèµ·å§‹ä½ç½®
        # ä½¿ç”¨epoch_seedè€Œä¸æ˜¯ç®€å•çš„epochç¼–å·ï¼Œä¿æŒä¸€å®šçš„éšæœºæ€§
        random.seed(epoch_seed)
        base_offset = random.randint(0, epochs_for_full_coverage - 1)
        
        # è®¡ç®—å½“å‰epochåº”è¯¥ä½¿ç”¨çš„æ ·æœ¬èŒƒå›´
        current_epoch_mod = (epoch_seed - 42) % epochs_for_full_coverage  # å‡å»åŸºç¡€ç§å­42
        start_idx = (current_epoch_mod * target_count + base_offset * target_count // epochs_for_full_coverage) % total_samples
        
        # é€‰æ‹©æ ·æœ¬ï¼Œä½¿ç”¨å¾ªç¯æ–¹å¼ç¡®ä¿è¦†ç›–
        selected_indices = []
        for i in range(target_count):
            idx = (start_idx + i) % total_samples
            selected_indices.append(sample_pool[idx])
        
        # ä¸ºäº†ä¿æŒä¸€å®šçš„éšæœºæ€§ï¼Œå¯¹é€‰ä¸­çš„æ ·æœ¬è¿›è¡Œè½»å¾®shuffle
        random.seed(epoch_seed)
        random.shuffle(selected_indices)
        
        return selected_indices

    def get_data_coverage_info(self):
        """
        è·å–æ•°æ®è¦†ç›–ä¿¡æ¯
        
        Returns:
            dict: åŒ…å«æ•°æ®è¦†ç›–ç»Ÿè®¡çš„å­—å…¸
        """
        if not self.resample_each_epoch or self.positive_ratio >= 1.0:
            return {
                'strategy': 'full_data' if self.positive_ratio >= 1.0 else 'fixed_subset',
                'coverage_epochs': 1,
                'coverage_ratio': self.positive_ratio
            }
        
        # åˆ†ç¦»æ­£æ ·æœ¬å’Œè´Ÿæ ·æœ¬
        positive_count = sum(1 for _, _, metadata in self.full_sample_index if metadata['firms_value'] > 0)
        negative_count = len(self.full_sample_index) - positive_count
        
        # è®¡ç®—è¦†ç›–æ‰€éœ€çš„epochæ•°
        retained_positive_count = int(positive_count * self.positive_ratio)
        retained_negative_count = int(retained_positive_count * self.pos_neg_ratio)
        retained_negative_count = min(retained_negative_count, negative_count)
        
        positive_coverage_epochs = math.ceil(positive_count / retained_positive_count) if retained_positive_count > 0 else 1
        negative_coverage_epochs = math.ceil(negative_count / retained_negative_count) if retained_negative_count > 0 else 1
        
        max_coverage_epochs = max(positive_coverage_epochs, negative_coverage_epochs)
        
        return {
            'strategy': 'rotated_sampling',
            'positive_coverage_epochs': positive_coverage_epochs,
            'negative_coverage_epochs': negative_coverage_epochs,
            'max_coverage_epochs': max_coverage_epochs,
            'total_positive_samples': positive_count,
            'total_negative_samples': negative_count,
            'samples_per_epoch_positive': retained_positive_count,
            'samples_per_epoch_negative': retained_negative_count,
            'coverage_description': f"ç»è¿‡ {max_coverage_epochs} ä¸ªepochåï¼Œæ¨¡å‹å°†è§åˆ°æ‰€æœ‰è®­ç»ƒæ•°æ®"
        }


class TimeSeriesDataLoader:
    """æ—¶é—´åºåˆ—æ•°æ®åŠ è½½å™¨çš„ä¾¿æ·åŒ…è£…ç±»"""
    
    def __init__(self, h5_dir, positive_ratio=1.0, pos_neg_ratio=1.0, resample_each_epoch=False, verbose_sampling=True, **dataset_kwargs):
        """
        åˆå§‹åŒ–æ•°æ®åŠ è½½å™¨
        
        Args:
            h5_dir: H5æ–‡ä»¶ç›®å½•
            positive_ratio: æ­£æ ·æœ¬ä½¿ç”¨æ¯”ä¾‹ï¼Œæ§åˆ¶é€‰å–çš„æ­£æ ·æœ¬æ•°å æ€»æ­£æ ·æœ¬æ•°çš„æ¯”ä¾‹ (0.0-1.0)
            pos_neg_ratio: æ­£è´Ÿæ ·æœ¬æ¯”ä¾‹ï¼Œå³è´Ÿæ ·æœ¬æ•° = æ­£æ ·æœ¬æ•° Ã— pos_neg_ratio
            resample_each_epoch: æ˜¯å¦åœ¨æ¯ä¸ªepoché‡æ–°è¿›è¡Œæ ·æœ¬æŠ½æ ·
            verbose_sampling: æ˜¯å¦æ‰“å°è¯¦ç»†çš„æ ·æœ¬ç»Ÿè®¡ä¿¡æ¯
            **dataset_kwargs: ä¼ é€’ç»™TimeSeriesPixelDatasetçš„å…¶ä»–å‚æ•°
        """
        self.resample_each_epoch = resample_each_epoch
        self.verbose_sampling = verbose_sampling
        self.dataset = TimeSeriesPixelDataset(
            h5_dir, 
            positive_ratio=positive_ratio, 
            pos_neg_ratio=pos_neg_ratio, 
            resample_each_epoch=resample_each_epoch,
            verbose_sampling=verbose_sampling,
            **dataset_kwargs
        )
    
    def resample_for_epoch(self, epoch):
        """
        ä¸ºæ–°çš„epoché‡æ–°è¿›è¡Œæ ·æœ¬æŠ½æ ·
        
        Args:
            epoch: å½“å‰epochç¼–å·ï¼ˆä»0å¼€å§‹ï¼‰
        """
        if self.resample_each_epoch:
            # ä½¿ç”¨é¢„å®šä¹‰çš„ç§å­åºåˆ—ï¼Œæé«˜å¯é‡å¤æ€§
            predefined_seeds = [42, 123, 456, 789, 321, 654, 987, 147, 258, 369, 
                              741, 852, 963, 159, 267, 378, 489, 591, 612, 723]
            seed = predefined_seeds[epoch % len(predefined_seeds)]
            self.dataset.resample_for_epoch(seed)
            
            # åªåœ¨verboseæ¨¡å¼ä¸‹æ‰“å°æ ·æœ¬ç»Ÿè®¡ä¿¡æ¯
            if self.verbose_sampling:
                stats = self.dataset.get_current_sample_stats()
                logger.info(f"Epoch {epoch} æ ·æœ¬ç»Ÿè®¡: æ€»è®¡={stats['total_samples']}, "
                           f"æ­£æ ·æœ¬={stats['positive_samples']}, è´Ÿæ ·æœ¬={stats['negative_samples']}, "
                           f"æ­£è´Ÿæ¯”ä¾‹=1:{stats['pos_neg_ratio']:.2f}")
    
    def get_sample_stats(self):
        """è·å–å½“å‰æ ·æœ¬ç»Ÿè®¡ä¿¡æ¯"""
        return self.dataset.get_current_sample_stats()
    
    def get_data_coverage_info(self):
        """è·å–æ•°æ®è¦†ç›–ä¿¡æ¯"""
        return self.dataset.get_data_coverage_info()
    
    def print_data_coverage_info(self):
        """æ‰“å°æ•°æ®è¦†ç›–ä¿¡æ¯"""
        coverage_info = self.get_data_coverage_info()
        
        print(f"\nğŸ“Š æ•°æ®è¦†ç›–ç­–ç•¥åˆ†æ:")
        print(f"   ç­–ç•¥ç±»å‹: {coverage_info['strategy']}")
        
        if coverage_info['strategy'] == 'rotated_sampling':
            print(f"   æ­£æ ·æœ¬æ€»æ•°: {coverage_info['total_positive_samples']:,}")
            print(f"   è´Ÿæ ·æœ¬æ€»æ•°: {coverage_info['total_negative_samples']:,}")
            print(f"   æ¯epochæ­£æ ·æœ¬: {coverage_info['samples_per_epoch_positive']:,}")
            print(f"   æ¯epochè´Ÿæ ·æœ¬: {coverage_info['samples_per_epoch_negative']:,}")
            print(f"   æ­£æ ·æœ¬å®Œå…¨è¦†ç›–éœ€è¦: {coverage_info['positive_coverage_epochs']} epochs")
            print(f"   è´Ÿæ ·æœ¬å®Œå…¨è¦†ç›–éœ€è¦: {coverage_info['negative_coverage_epochs']} epochs")
            print(f"   ğŸ¯ {coverage_info['coverage_description']}")
        elif coverage_info['strategy'] == 'full_data':
            print(f"   âœ… ä½¿ç”¨å…¨éƒ¨è®­ç»ƒæ•°æ®ï¼Œæ— éœ€è½®æ¢")
        else:
            print(f"   âš ï¸  ä½¿ç”¨å›ºå®šå­é›† ({coverage_info['coverage_ratio']:.1%})ï¼Œå»ºè®®å¯ç”¨åŠ¨æ€æŠ½æ ·")
    
    def create_dataloader(self, batch_size=32, shuffle=True, num_workers=4, worker_init_fn=None, **dataloader_kwargs):
        """
        åˆ›å»ºPyTorch DataLoader
        
        Args:
            batch_size: æ‰¹æ¬¡å¤§å°
            shuffle: æ˜¯å¦æ‰“ä¹±æ•°æ®
            num_workers: å·¥ä½œè¿›ç¨‹æ•°
            worker_init_fn: workeråˆå§‹åŒ–å‡½æ•°ï¼Œç¡®ä¿å¤šè¿›ç¨‹å¯é‡å¤æ€§
            **dataloader_kwargs: ä¼ é€’ç»™DataLoaderçš„å…¶ä»–å‚æ•°
        
        Returns:
            torch.utils.data.DataLoader
        """
        # ä½¿ç”¨è‡ªå®šä¹‰çš„collateå‡½æ•°æ¥å¤„ç†å°ºå¯¸ä¸ä¸€è‡´çš„é—®é¢˜
        return DataLoader(
            self.dataset,
            batch_size=batch_size,
            shuffle=shuffle,
            num_workers=num_workers,
            collate_fn=self.dataset.custom_collate_fn,
            worker_init_fn=worker_init_fn,
            **dataloader_kwargs
        )
    
    def get_year_based_split(self, train_years, val_years, test_years, test_full_years=None):
        """
        åŸºäºå¹´ä»½åˆ›å»ºè®­ç»ƒã€éªŒè¯å’Œæµ‹è¯•æ•°æ®é›†åˆ†å‰²
        
        Args:
            train_years: è®­ç»ƒå¹´ä»½åˆ—è¡¨ï¼ˆä½¿ç”¨æŠ½æ ·æ•°æ®ï¼‰
            val_years: éªŒè¯å¹´ä»½åˆ—è¡¨ï¼ˆä½¿ç”¨æŠ½æ ·æ•°æ®ï¼‰
            test_years: æµ‹è¯•å¹´ä»½åˆ—è¡¨ï¼ˆä½¿ç”¨æŠ½æ ·æ•°æ®ï¼‰
            test_full_years: å®Œæ•´æ•°æ®æµ‹è¯•å¹´ä»½åˆ—è¡¨ï¼ˆä½¿ç”¨å®Œæ•´æ•°æ®ï¼Œå¯é€‰ï¼‰
        
        Returns:
            å¦‚æœtest_full_yearsä¸ºNone: (train_indices, val_indices, test_indices)
            å¦‚æœtest_full_yearsä¸ä¸ºNone: (train_indices, val_indices, test_indices, test_full_indices)
        """
        train_indices = []
        val_indices = []
        test_indices = []
        
        for idx, (h5_path, dataset_key, metadata) in enumerate(self.dataset.sample_index):
            year = self.dataset.dataset_info[h5_path]['year']
            
            if year in train_years:
                train_indices.append(idx)
            elif year in val_years:
                val_indices.append(idx)
            elif year in test_years:
                test_indices.append(idx)
        
        if self.verbose_sampling:
            data_type = "å®Œæ•´æ•°æ®" if self.dataset.use_full_data else "æŠ½æ ·æ•°æ®"
            logger.info(f"å¹´ä»½åˆ’åˆ†ç»“æœ ({data_type}):")
            logger.info(f"  è®­ç»ƒé›†: {len(train_indices)} æ ·æœ¬ (å¹´ä»½: {train_years})")
            logger.info(f"  éªŒè¯é›†: {len(val_indices)} æ ·æœ¬ (å¹´ä»½: {val_years})")
            logger.info(f"  æµ‹è¯•é›†: {len(test_indices)} æ ·æœ¬ (å¹´ä»½: {test_years})")
        
        # å¦‚æœæŒ‡å®šäº†å®Œæ•´æ•°æ®æµ‹è¯•å¹´ä»½ï¼Œåˆ›å»ºå®Œæ•´æ•°æ®æµ‹è¯•é›†
        if test_full_years is not None:
            # åˆ›å»ºå®Œæ•´æ•°æ®åŠ è½½å™¨
            full_dataset = TimeSeriesPixelDataset(
                h5_dir=self.dataset.h5_dir,
                years=test_full_years,
                firms_values=self.dataset.firms_values,
                return_metadata=self.dataset.return_metadata,
                use_full_data=True
            )
            
            # è·å–å®Œæ•´æ•°æ®çš„æ‰€æœ‰ç´¢å¼•
            test_full_indices = list(range(len(full_dataset)))
            
            logger.info(f"å®Œæ•´æ•°æ®æµ‹è¯•é›†: {len(test_full_indices)} æ ·æœ¬ (å¹´ä»½: {test_full_years})")
            
            return train_indices, val_indices, test_indices, test_full_indices, full_dataset
        
        return train_indices, val_indices, test_indices


class FullDatasetLoader:
    """
    ä¸“é—¨ç”¨äºå®Œæ•´æ•°æ®é›†æµ‹è¯•çš„æ•°æ®åŠ è½½å™¨
    ä¸è¿›è¡Œä»»ä½•é‡‡æ ·ï¼ŒåŠ è½½æ‰€æœ‰å¯ç”¨æ•°æ®
    """
    
    def __init__(self, h5_dir, years=None, return_metadata=True):
        """
        åˆå§‹åŒ–å®Œæ•´æ•°æ®é›†åŠ è½½å™¨
        
        Args:
            h5_dir: H5æ–‡ä»¶ç›®å½•
            years: è¦åŠ è½½çš„å¹´ä»½åˆ—è¡¨ï¼ŒNoneè¡¨ç¤ºåŠ è½½æ‰€æœ‰å¹´ä»½
            return_metadata: æ˜¯å¦è¿”å›å…ƒæ•°æ®ï¼ˆæ—¥æœŸã€åæ ‡ã€FIRMSå€¼ç­‰ï¼‰
        """
        self.h5_dir = h5_dir
        self.years = years
        self.return_metadata = return_metadata
        
        # åˆ›å»ºåŸºç¡€æ•°æ®é›†ï¼Œä¸è¿›è¡Œä»»ä½•é‡‡æ ·
        self.dataset = TimeSeriesPixelDataset(
            h5_dir=h5_dir,
            years=years,
            firms_values=None,  # åŠ è½½æ‰€æœ‰FIRMSå€¼
            return_metadata=return_metadata,
            use_full_data=True,  # ä½¿ç”¨å®Œæ•´æ•°æ®æ–‡ä»¶
            positive_ratio=1.0,  # ä½¿ç”¨æ‰€æœ‰æ­£æ ·æœ¬
            pos_neg_ratio=999999  # ä½¿ç”¨æ‰€æœ‰è´Ÿæ ·æœ¬ï¼ˆå®é™…ä¸Šä¸é™åˆ¶ï¼‰
        )
        
        logger.info(f"å®Œæ•´æ•°æ®é›†åŠ è½½å™¨åˆå§‹åŒ–å®Œæˆï¼Œå…± {len(self.dataset)} ä¸ªæ ·æœ¬")
        
        # è·å–æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯
        stats = self.dataset.get_statistics()
        logger.info(f"æ•°æ®é›†ç»Ÿè®¡: {stats}")
    
    def create_dataloader(self, batch_size=32, shuffle=False, num_workers=4, worker_init_fn=None, **dataloader_kwargs):
        """
        åˆ›å»ºPyTorch DataLoader
        
        Args:
            batch_size: æ‰¹æ¬¡å¤§å°
            shuffle: æ˜¯å¦æ‰“ä¹±æ•°æ®ï¼ˆå®Œæ•´æ•°æ®é›†æµ‹è¯•é€šå¸¸ä¸æ‰“ä¹±ï¼‰
            num_workers: å·¥ä½œè¿›ç¨‹æ•°
            worker_init_fn: workeråˆå§‹åŒ–å‡½æ•°ï¼Œç¡®ä¿å¤šè¿›ç¨‹å¯é‡å¤æ€§
            **dataloader_kwargs: ä¼ é€’ç»™DataLoaderçš„å…¶ä»–å‚æ•°
        
        Returns:
            torch.utils.data.DataLoader
        """
        from torch.utils.data import DataLoader
        
        return DataLoader(
            self.dataset,
            batch_size=batch_size,
            shuffle=shuffle,
            num_workers=num_workers,
            collate_fn=self.dataset.custom_collate_fn,
            worker_init_fn=worker_init_fn,
            **dataloader_kwargs
        )
    
    def __len__(self):
        return len(self.dataset)
    
    def get_statistics(self):
        """è·å–æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯"""
        return self.dataset.get_statistics()


if __name__ == '__main__':
    
    h5_dir = '/mnt/raid/zhengsen/wildfire_dataset/self_built_materials/pixel_samples_merged'
    
    # 1. åˆ›å»ºæŠ½æ ·æ•°æ®åŠ è½½å™¨
    data_loader = TimeSeriesDataLoader(h5_dir=h5_dir)
    
    # 2. åŸºäºå¹´ä»½åˆ’åˆ†æ•°æ®é›†
    train_years = [2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 
                   2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020]
    val_years = [2021, 2022]
    test_years = [2023, 2024]
    test_full_years = []  # ä½¿ç”¨å®Œæ•´æ•°æ®è¿›è¡Œæœ€ç»ˆæµ‹è¯•
    
    result = data_loader.get_year_based_split(
        train_years, val_years, test_years, test_full_years
    )
    
    if len(result) == 5:
        train_indices, val_indices, test_indices, test_full_indices, full_dataset = result
    else:
        train_indices, val_indices, test_indices = result
        test_full_indices = []
        full_dataset = None
    
    # 4. åˆ›å»ºPyTorch DataLoader
    from torch.utils.data import Subset
    
    # è®­ç»ƒé›†
    train_dataset = Subset(data_loader.dataset, train_indices)
    train_dataloader = data_loader.create_dataloader(
        batch_size=32,
        shuffle=True,
        num_workers=4
    )
    
    # éªŒè¯é›†
    val_dataset = Subset(data_loader.dataset, val_indices)
    val_dataloader = DataLoader(
        val_dataset,
        batch_size=32,
        shuffle=False,
        num_workers=4,
        collate_fn=data_loader.dataset.custom_collate_fn
    )
    
    # æµ‹è¯•é›†
    test_dataset = Subset(data_loader.dataset, test_indices)
    test_dataloader = DataLoader(
        test_dataset,
        batch_size=32,
        shuffle=False,
        num_workers=4,
        collate_fn=data_loader.dataset.custom_collate_fn
    )
    
    if full_dataset is not None:
        test_full_dataset = Subset(full_dataset, test_full_indices)
        test_full_dataloader = DataLoader(
            test_full_dataset,
            batch_size=32,
            shuffle=False,
            num_workers=4,
            collate_fn=full_dataset.custom_collate_fn
        )
        
        
