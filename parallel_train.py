#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¹¶è¡Œè®­ç»ƒè„šæœ¬ - åœ¨å¤šä¸ªGPUä¸ŠåŒæ—¶è®­ç»ƒä¸åŒæ¨¡å‹
æ”¯æŒåœ¨2ä¸ªGPUä¸Šå¹¶è¡Œè¿è¡Œ4ä¸ªè®­ç»ƒä»»åŠ¡
"""

import subprocess
import threading
import time
import os
import sys
from datetime import datetime
import json

# é…ç½®å‚æ•°
PARALLEL_CONFIG = {
    'gpu_devices': [0, 1],           # ä½¿ç”¨çš„GPUè®¾å¤‡
    'tasks_per_gpu': 2,              # æ¯ä¸ªGPUä¸Šè¿è¡Œçš„ä»»åŠ¡æ•°
    'train_script': 'train_all_models_combined.py',  # è®­ç»ƒè„šæœ¬åç§°
    'log_dir': 'parallel_logs',      # æ—¥å¿—ç›®å½•
    'wait_interval': 5,              # æ£€æŸ¥ä»»åŠ¡çŠ¶æ€çš„é—´éš”ï¼ˆç§’ï¼‰
}

# è·å–æ‰€æœ‰å¯ç”¨æ¨¡å‹
def get_available_models():
    """è·å–æ‰€æœ‰å¯ç”¨çš„æ¨¡å‹åˆ—è¡¨"""
    try:
        # ä»train_all_models_combined.pyä¸­è·å–æ¨¡å‹åˆ—è¡¨
        import importlib.util
        spec = importlib.util.spec_from_file_location("train_module", PARALLEL_CONFIG['train_script'])
        train_module = importlib.util.module_from_spec(spec)
        spec.loader.exec_module(train_module)
        
        standard_models = train_module.MODEL_LIST_STANDARD
        models_10x = train_module.MODEL_LIST_10X
        
        return standard_models, models_10x
    except Exception as e:
        print(f"âš ï¸ æ— æ³•è·å–æ¨¡å‹åˆ—è¡¨: {e}")
        # å¤‡ç”¨æ¨¡å‹åˆ—è¡¨
        standard_models = ['Autoformer', 'Autoformer_M', 'iTransformer', 's_mamba']
        models_10x = ['Autoformer', 'Autoformer_M', 'iTransformer', 's_mamba']
        return standard_models, models_10x

class ParallelTrainer:
    """å¹¶è¡Œè®­ç»ƒç®¡ç†å™¨"""
    
    def __init__(self):
        self.running_tasks = []
        self.completed_tasks = []
        self.failed_tasks = []
        self.start_time = None
        
        # åˆ›å»ºæ—¥å¿—ç›®å½•
        os.makedirs(PARALLEL_CONFIG['log_dir'], exist_ok=True)
        
    def create_single_model_script(self, model_name, model_type, gpu_id):
        """ä¸ºå•ä¸ªæ¨¡å‹åˆ›å»ºè®­ç»ƒè„šæœ¬"""
        script_content = f'''#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å•æ¨¡å‹è®­ç»ƒè„šæœ¬ - {model_name} ({model_type})
è‡ªåŠ¨ç”Ÿæˆçš„å¹¶è¡Œè®­ç»ƒè„šæœ¬
"""

import os
import sys
import torch

# è®¾ç½®CUDAè®¾å¤‡
os.environ['CUDA_VISIBLE_DEVICES'] = '{gpu_id}'

# å¯¼å…¥è®­ç»ƒæ¨¡å—
from train_all_models_combined import *

def train_single_model_only():
    """åªè®­ç»ƒæŒ‡å®šçš„å•ä¸ªæ¨¡å‹"""
    print(f"ğŸš€ å¼€å§‹è®­ç»ƒå•ä¸ªæ¨¡å‹: {{model_name}} ({{model_type}}) on GPU {{gpu_id}}")
    
    # åˆå§‹åŒ–
    set_seed(TRAINING_CONFIG['seed'])
    device = torch.device('cuda:0')  # ç”±äºè®¾ç½®äº†CUDA_VISIBLE_DEVICESï¼Œè¿™é‡Œæ€»æ˜¯0
    print(f"ğŸ–¥ï¸  ä½¿ç”¨è®¾å¤‡: GPU {{gpu_id}} (æ˜ å°„ä¸ºcuda:0)")
    
    # å‡†å¤‡æ•°æ®
    train_dataset, val_dataset, test_dataset, data_loader_obj = prepare_data_loaders()
    
    # åˆå§‹åŒ–FIRMSå½’ä¸€åŒ–å™¨
    print("ğŸ”§ åˆå§‹åŒ–FIRMSå½’ä¸€åŒ–å™¨...")
    firms_normalizer = FIRMSNormalizer(
        method='log1p_minmax',
        firms_min=DATA_CONFIG['firms_min'],
        firms_max=DATA_CONFIG['firms_max']
    )
    
    # ä¸ºå½’ä¸€åŒ–æ‹Ÿåˆåˆ›å»ºä¸´æ—¶æ•°æ®åŠ è½½å™¨
    temp_loader = DataLoader(
        train_dataset, batch_size=512, shuffle=False, 
        num_workers=2, collate_fn=data_loader_obj.dataset.custom_collate_fn
    )
    firms_normalizer.fit(temp_loader)
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    config_key = '{model_type}'
    train_config = TRAINING_CONFIG[config_key]
    
    train_loader = DataLoader(
        train_dataset, batch_size=train_config['batch_size'], shuffle=True, 
        num_workers=2, collate_fn=data_loader_obj.dataset.custom_collate_fn, worker_init_fn=worker_init_fn
    )
    val_loader = DataLoader(
        val_dataset, batch_size=train_config['batch_size'], shuffle=False,
        num_workers=2, collate_fn=data_loader_obj.dataset.custom_collate_fn, worker_init_fn=worker_init_fn
    )
    test_loader = DataLoader(
        test_dataset, batch_size=train_config['batch_size'], shuffle=False,
        num_workers=2, collate_fn=data_loader_obj.dataset.custom_collate_fn, worker_init_fn=worker_init_fn
    )
    
    # è®­ç»ƒæ¨¡å‹
    try:
        result = train_single_model(
            '{model_name}', device, train_loader, val_loader, test_loader, firms_normalizer, '{model_type}'
        )
        
        if result is not None:
            print(f"âœ… {{model_name}} ({{model_type}}) è®­ç»ƒå®Œæˆ")
            
            # æµ‹è¯•æ‰€æœ‰ä¿å­˜çš„æ¨¡å‹
            test_results = []
            for metric_name, metric_info in result.items():
                if metric_info['path'] is not None:
                    print(f"ğŸ§ª æµ‹è¯• {{metric_name}} æ¨¡å‹...")
                    test_result = test_model('{model_name}', metric_info['path'], device, test_loader, firms_normalizer, '{model_type}')
                    if test_result:
                        test_results.append((test_result, metric_name))
            
            # ä¿å­˜ç»“æœ
            results_file = f"{{PARALLEL_CONFIG['log_dir']}}/{model_name}_{model_type}_results.json"
            with open(results_file, 'w') as f:
                json.dump({{
                    'model_name': '{model_name}',
                    'model_type': '{model_type}',
                    'gpu_id': {gpu_id},
                    'training_metrics': result,
                    'test_results': test_results,
                    'status': 'completed'
                }}, f, indent=2, default=str)
            
            print(f"ğŸ“Š ç»“æœå·²ä¿å­˜åˆ°: {{results_file}}")
        else:
            print(f"âŒ {{model_name}} ({{model_type}}) è®­ç»ƒå¤±è´¥")
            
    except Exception as e:
        print(f"âŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {{e}}")
        # ä¿å­˜é”™è¯¯ä¿¡æ¯
        error_file = f"{{PARALLEL_CONFIG['log_dir']}}/{model_name}_{model_type}_error.json"
        with open(error_file, 'w') as f:
            json.dump({{
                'model_name': '{model_name}',
                'model_type': '{model_type}',
                'gpu_id': {gpu_id},
                'error': str(e),
                'status': 'failed'
            }}, f, indent=2)
        raise

if __name__ == "__main__":
    train_single_model_only()
'''
        
        # ä¿å­˜è„šæœ¬æ–‡ä»¶
        script_filename = f"{PARALLEL_CONFIG['log_dir']}/train_{model_name}_{model_type}_gpu{gpu_id}.py"
        with open(script_filename, 'w') as f:
            f.write(script_content)
        
        return script_filename
    
    def run_task(self, model_name, model_type, gpu_id, task_id):
        """è¿è¡Œå•ä¸ªè®­ç»ƒä»»åŠ¡"""
        print(f"ğŸš€ å¯åŠ¨ä»»åŠ¡ {task_id}: {model_name} ({model_type}) on GPU {gpu_id}")
        
        # åˆ›å»ºå•æ¨¡å‹è®­ç»ƒè„šæœ¬
        script_path = self.create_single_model_script(model_name, model_type, gpu_id)
        
        # è®¾ç½®æ—¥å¿—æ–‡ä»¶
        log_file = f"{PARALLEL_CONFIG['log_dir']}/train_{model_name}_{model_type}_gpu{gpu_id}.log"
        
        # è¿è¡Œè®­ç»ƒè„šæœ¬
        try:
            with open(log_file, 'w') as f:
                process = subprocess.Popen(
                    [sys.executable, script_path],
                    stdout=f,
                    stderr=subprocess.STDOUT,
                    env=dict(os.environ, CUDA_VISIBLE_DEVICES=str(gpu_id))
                )
                
                # ç­‰å¾…ä»»åŠ¡å®Œæˆ
                return_code = process.wait()
                
                if return_code == 0:
                    print(f"âœ… ä»»åŠ¡ {task_id} å®Œæˆ: {model_name} ({model_type})")
                    self.completed_tasks.append((task_id, model_name, model_type, gpu_id))
                else:
                    print(f"âŒ ä»»åŠ¡ {task_id} å¤±è´¥: {model_name} ({model_type}) - è¿”å›ç : {return_code}")
                    self.failed_tasks.append((task_id, model_name, model_type, gpu_id, return_code))
                    
        except Exception as e:
            print(f"âŒ ä»»åŠ¡ {task_id} å¼‚å¸¸: {model_name} ({model_type}) - é”™è¯¯: {e}")
            self.failed_tasks.append((task_id, model_name, model_type, gpu_id, str(e)))
        
        finally:
            # ä»è¿è¡Œåˆ—è¡¨ä¸­ç§»é™¤
            self.running_tasks = [(tid, mn, mt, gid) for tid, mn, mt, gid in self.running_tasks 
                                 if not (tid == task_id and mn == model_name and mt == model_type)]
    
    def create_task_queue(self, models_standard, models_10x):
        """åˆ›å»ºä»»åŠ¡é˜Ÿåˆ—"""
        tasks = []
        task_id = 1
        
        # æ·»åŠ æ ‡å‡†æ¨¡å‹ä»»åŠ¡
        for model in models_standard:
            tasks.append((task_id, model, 'standard'))
            task_id += 1
            
        # æ·»åŠ 10xæ¨¡å‹ä»»åŠ¡
        for model in models_10x:
            tasks.append((task_id, model, '10x'))
            task_id += 1
            
        return tasks
    
    def run_parallel_training(self, models_standard, models_10x):
        """è¿è¡Œå¹¶è¡Œè®­ç»ƒ"""
        self.start_time = datetime.now()
        print(f"ğŸ”¥ å¼€å§‹å¹¶è¡Œè®­ç»ƒ - {self.start_time.strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"ğŸ“‹ é…ç½®: {len(PARALLEL_CONFIG['gpu_devices'])} GPU, æ¯GPU {PARALLEL_CONFIG['tasks_per_gpu']} ä»»åŠ¡")
        print(f"ğŸ“Š æ ‡å‡†æ¨¡å‹: {len(models_standard)} ä¸ª")
        print(f"ğŸ“Š 10xæ¨¡å‹: {len(models_10x)} ä¸ª")
        
        # åˆ›å»ºä»»åŠ¡é˜Ÿåˆ—
        task_queue = self.create_task_queue(models_standard, models_10x)
        total_tasks = len(task_queue)
        print(f"ğŸ“‹ æ€»ä»»åŠ¡æ•°: {total_tasks}")
        
        # è®¡ç®—æœ€å¤§å¹¶è¡Œä»»åŠ¡æ•°
        max_parallel = len(PARALLEL_CONFIG['gpu_devices']) * PARALLEL_CONFIG['tasks_per_gpu']
        
        task_index = 0
        active_threads = []
        
        while task_index < total_tasks or active_threads:
            # å¯åŠ¨æ–°ä»»åŠ¡ï¼ˆå¦‚æœæœ‰ç©ºé—²æ§½ä½ï¼‰
            while len(active_threads) < max_parallel and task_index < total_tasks:
                task_id, model_name, model_type = task_queue[task_index]
                
                # åˆ†é…GPU
                gpu_id = PARALLEL_CONFIG['gpu_devices'][len(active_threads) % len(PARALLEL_CONFIG['gpu_devices'])]
                
                # åˆ›å»ºå¹¶å¯åŠ¨çº¿ç¨‹
                thread = threading.Thread(
                    target=self.run_task,
                    args=(model_name, model_type, gpu_id, task_id)
                )
                thread.start()
                active_threads.append(thread)
                self.running_tasks.append((task_id, model_name, model_type, gpu_id))
                
                task_index += 1
                time.sleep(2)  # é¿å…åŒæ—¶å¯åŠ¨è¿‡å¤šä»»åŠ¡
            
            # æ£€æŸ¥å®Œæˆçš„çº¿ç¨‹
            active_threads = [t for t in active_threads if t.is_alive()]
            
            # æ˜¾ç¤ºè¿›åº¦
            completed = len(self.completed_tasks)
            failed = len(self.failed_tasks)
            running = len(active_threads)
            
            print(f"ğŸ“Š è¿›åº¦: å®Œæˆ={completed}, å¤±è´¥={failed}, è¿è¡Œä¸­={running}, å‰©ä½™={total_tasks-completed-failed-running}")
            
            time.sleep(PARALLEL_CONFIG['wait_interval'])
        
        # ç­‰å¾…æ‰€æœ‰çº¿ç¨‹å®Œæˆ
        for thread in active_threads:
            thread.join()
        
        # è¾“å‡ºæœ€ç»ˆç»“æœ
        self.print_final_results()
    
    def print_final_results(self):
        """æ‰“å°æœ€ç»ˆç»“æœ"""
        end_time = datetime.now()
        duration = end_time - self.start_time
        
        print(f"\n{'='*80}")
        print(f"ğŸ‰ å¹¶è¡Œè®­ç»ƒå®Œæˆ! - {end_time.strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"â±ï¸  æ€»ç”¨æ—¶: {duration}")
        print(f"{'='*80}")
        
        print(f"\nğŸ“Š è®­ç»ƒç»“æœç»Ÿè®¡:")
        print(f"âœ… æˆåŠŸå®Œæˆ: {len(self.completed_tasks)} ä¸ªä»»åŠ¡")
        print(f"âŒ å¤±è´¥ä»»åŠ¡: {len(self.failed_tasks)} ä¸ªä»»åŠ¡")
        
        if self.completed_tasks:
            print(f"\nâœ… æˆåŠŸå®Œæˆçš„ä»»åŠ¡:")
            for task_id, model_name, model_type, gpu_id in self.completed_tasks:
                print(f"  - ä»»åŠ¡{task_id}: {model_name} ({model_type}) on GPU {gpu_id}")
        
        if self.failed_tasks:
            print(f"\nâŒ å¤±è´¥çš„ä»»åŠ¡:")
            for task_id, model_name, model_type, gpu_id, error in self.failed_tasks:
                print(f"  - ä»»åŠ¡{task_id}: {model_name} ({model_type}) on GPU {gpu_id} - {error}")
        
        print(f"\nğŸ“ è¯¦ç»†æ—¥å¿—å’Œç»“æœæ–‡ä»¶ä¿å­˜åœ¨: {PARALLEL_CONFIG['log_dir']}/")
        print(f"ğŸ“Š å¯ä»¥æŸ¥çœ‹å„ä¸ªæ¨¡å‹çš„è®­ç»ƒæ—¥å¿—å’Œç»“æœæ–‡ä»¶")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ”¥ é‡ç«é¢„æµ‹æ¨¡å‹å¹¶è¡Œè®­ç»ƒç³»ç»Ÿ")
    print("="*50)
    
    # æ£€æŸ¥è®­ç»ƒè„šæœ¬æ˜¯å¦å­˜åœ¨
    if not os.path.exists(PARALLEL_CONFIG['train_script']):
        print(f"âŒ è®­ç»ƒè„šæœ¬ä¸å­˜åœ¨: {PARALLEL_CONFIG['train_script']}")
        return
    
    # æ£€æŸ¥GPUå¯ç”¨æ€§
    try:
        import torch
        if not torch.cuda.is_available():
            print("âŒ CUDAä¸å¯ç”¨ï¼Œæ— æ³•è¿›è¡ŒGPUè®­ç»ƒ")
            return
        
        available_gpus = torch.cuda.device_count()
        print(f"ğŸ’¾ å¯ç”¨GPUæ•°é‡: {available_gpus}")
        
        for gpu_id in PARALLEL_CONFIG['gpu_devices']:
            if gpu_id >= available_gpus:
                print(f"âŒ GPU {gpu_id} ä¸å­˜åœ¨ï¼Œå¯ç”¨GPU: 0-{available_gpus-1}")
                return
            
            gpu_name = torch.cuda.get_device_name(gpu_id)
            gpu_memory = torch.cuda.get_device_properties(gpu_id).total_memory / 1024**3
            print(f"  GPU {gpu_id}: {gpu_name} ({gpu_memory:.1f} GB)")
            
    except ImportError:
        print("âŒ PyTorchæœªå®‰è£…ï¼Œæ— æ³•æ£€æŸ¥GPUçŠ¶æ€")
        return
    
    # è·å–æ¨¡å‹åˆ—è¡¨
    print(f"\nğŸ“‹ è·å–å¯è®­ç»ƒæ¨¡å‹åˆ—è¡¨...")
    models_standard, models_10x = get_available_models()
    
    if not models_standard and not models_10x:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°å¯è®­ç»ƒçš„æ¨¡å‹")
        return
    
    print(f"ğŸ“Š æ ‡å‡†æ¨¡å‹ ({len(models_standard)}): {', '.join(models_standard)}")
    print(f"ğŸ“Š 10xæ¨¡å‹ ({len(models_10x)}): {', '.join(models_10x)}")
    
    # ç”¨æˆ·ç¡®è®¤
    total_tasks = len(models_standard) + len(models_10x)
    max_parallel = len(PARALLEL_CONFIG['gpu_devices']) * PARALLEL_CONFIG['tasks_per_gpu']
    
    print(f"\nğŸ”§ å¹¶è¡Œè®­ç»ƒé…ç½®:")
    print(f"  - æ€»ä»»åŠ¡æ•°: {total_tasks}")
    print(f"  - æœ€å¤§å¹¶è¡Œæ•°: {max_parallel}")
    print(f"  - ä½¿ç”¨GPU: {PARALLEL_CONFIG['gpu_devices']}")
    print(f"  - æ¯GPUä»»åŠ¡æ•°: {PARALLEL_CONFIG['tasks_per_gpu']}")
    
    response = input(f"\næ˜¯å¦å¼€å§‹å¹¶è¡Œè®­ç»ƒ? [y/N]: ").strip().lower()
    if response not in ['y', 'yes']:
        print("âŒ ç”¨æˆ·å–æ¶ˆè®­ç»ƒ")
        return
    
    # å¼€å§‹å¹¶è¡Œè®­ç»ƒ
    trainer = ParallelTrainer()
    trainer.run_parallel_training(models_standard, models_10x)

if __name__ == "__main__":
    main() 